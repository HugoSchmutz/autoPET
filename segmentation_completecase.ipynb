{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on Tue Aug 22 11:42:51 2023\n",
      "TorchIO version: 0.18.90\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "import os \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchio as tio\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from unet import UNet\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Config\n",
    "seed = 42  # for reproducibility\n",
    "training_split_ratio = 0.9  # use 90% of samples for training, 10% for testing\n",
    "num_epochs = 5\n",
    "\n",
    "# If the following values are False, the models will be downloaded and not computed\n",
    "compute_histograms = False\n",
    "train_whole_images = False \n",
    "train_patches = False\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "plt.rcParams['figure.figsize'] = 30, 10\n",
    "\n",
    "print('Last run on', time.ctime())\n",
    "print('TorchIO version:', tio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE:  tensor(1.4712, grad_fn=<DivBackward0>)\n",
      "Dice:  tensor(0.8927, grad_fn=<MeanBackward0>) tensor(0.5001, grad_fn=<MeanBackward0>)\n",
      "DiceCE:  tensor(2.3638, grad_fn=<AddBackward0>) tensor(1.4032, grad_fn=<AddBackward0>)\n",
      "MSE:  tensor(0.4423, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import monai\n",
    "\n",
    "loss = monai.losses.MaskedDiceLoss(to_onehot_y=False,softmax=True,include_background=False,batch=True)\n",
    "loss_dice_ce = monai.losses.DiceCELoss(to_onehot_y=False,softmax=True,include_background=False,batch=True)\n",
    "loss_dice= monai.losses.DiceLoss(to_onehot_y=False,softmax=True,include_background=False,batch=True)\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "m = nn.Softmax(dim=1)\n",
    "input = torch.randn(12, 2, 128,128, 32, requires_grad=True)\n",
    "proba = m(input)\n",
    "\n",
    "mask=  (proba>threshold).long() + (proba<1-threshold).long() \n",
    "\n",
    "target = torch.empty(12, 2, 128,128, 32).random_(2)\n",
    "\n",
    "ce = -(torch.sum(((torch.log(proba)*target) + (torch.log(1-proba)*(1-target)))*mask))/mask.sum()\n",
    "dice = loss(input, target, mask=mask[:,:1])\n",
    "mse = (((proba-target)**2)*mask).sum()/mask.sum()\n",
    "\n",
    "\n",
    "print('CE: ', ce)\n",
    "print('Dice: ', dice, loss_dice(input, target))\n",
    "\n",
    "print('DiceCE: ', dice+ce, loss_dice_ce(input, target))\n",
    "\n",
    "\n",
    "print('MSE: ', mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_label = torch.randn(12, 2, 128,128, 32, requires_grad=True)\n",
    "input_unlabel = torch.randn(12, 2, 128,128, 32, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.clamp(torch.randn_like(input_unlabel)* 0.1, -0.2, 0.2) # Bruit gaussian pas plus gros que 0.2\n",
    "ema_inputs = input_unlabel + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = input_label * 2\n",
    "with torch.no_grad():\n",
    "    ema_output = ema_inputs*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 8\n",
    "volume_batch_r = input_unlabel.repeat(2, 1, 1, 1, 1)\n",
    "stride = volume_batch_r.shape[0] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.zeros([stride * T, 2, 128, 128, 32]).cuda()\n",
    "for i in range(T//2):\n",
    "    ema_inputs = volume_batch_r + torch.clamp(torch.randn_like(volume_batch_r) * 0.1, -0.2, 0.2)\n",
    "    with torch.no_grad():\n",
    "        preds[2 * stride * i:2 * stride * (i + 1)] = ema_inputs*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = F.softmax(preds, dim=1)\n",
    "preds = preds.reshape(T, stride, 2, 128, 128, 32)\n",
    "preds = torch.mean(preds, dim=0)  \n",
    "\n",
    "uncertainty = -1.0*torch.sum(preds*torch.log(preds + 1e-6), dim=1, keepdim=True) #(batch, 1, 112,112,80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0.6499, 0.6815, 0.5827,  ..., 0.6922, 0.6502, 0.6054],\n",
       "           [0.6211, 0.5829, 0.6091,  ..., 0.6366, 0.6520, 0.5951],\n",
       "           [0.6435, 0.6879, 0.6667,  ..., 0.6394, 0.6223, 0.5823],\n",
       "           ...,\n",
       "           [0.5835, 0.5880, 0.6218,  ..., 0.6025, 0.6127, 0.5955],\n",
       "           [0.6376, 0.6862, 0.6176,  ..., 0.6051, 0.6115, 0.5971],\n",
       "           [0.6057, 0.5913, 0.5889,  ..., 0.6385, 0.5895, 0.5871]],\n",
       "\n",
       "          [[0.6874, 0.6602, 0.5958,  ..., 0.6037, 0.6035, 0.6403],\n",
       "           [0.5828, 0.6829, 0.6768,  ..., 0.6013, 0.6910, 0.5859],\n",
       "           [0.5824, 0.6053, 0.6895,  ..., 0.5927, 0.6931, 0.6021],\n",
       "           ...,\n",
       "           [0.5929, 0.6029, 0.6325,  ..., 0.6104, 0.6289, 0.6467],\n",
       "           [0.5915, 0.6677, 0.5933,  ..., 0.5881, 0.6237, 0.6155],\n",
       "           [0.6080, 0.6425, 0.6068,  ..., 0.6919, 0.6713, 0.5970]],\n",
       "\n",
       "          [[0.6140, 0.5840, 0.6587,  ..., 0.6153, 0.6900, 0.6396],\n",
       "           [0.6306, 0.5860, 0.6878,  ..., 0.6296, 0.5848, 0.6070],\n",
       "           [0.6120, 0.6391, 0.5951,  ..., 0.6192, 0.6492, 0.5869],\n",
       "           ...,\n",
       "           [0.6541, 0.6426, 0.6866,  ..., 0.5881, 0.5929, 0.6575],\n",
       "           [0.6899, 0.5847, 0.6022,  ..., 0.5892, 0.6359, 0.6352],\n",
       "           [0.6500, 0.5823, 0.6889,  ..., 0.5919, 0.6634, 0.6876]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.5842, 0.5844, 0.6392,  ..., 0.6799, 0.6852, 0.6293],\n",
       "           [0.6621, 0.6509, 0.5876,  ..., 0.5996, 0.6309, 0.6870],\n",
       "           [0.6613, 0.5835, 0.6569,  ..., 0.6355, 0.6126, 0.6062],\n",
       "           ...,\n",
       "           [0.6136, 0.5858, 0.5876,  ..., 0.6928, 0.5983, 0.5982],\n",
       "           [0.6642, 0.6928, 0.6189,  ..., 0.5862, 0.6466, 0.6854],\n",
       "           [0.6402, 0.5830, 0.6190,  ..., 0.6499, 0.6063, 0.6030]],\n",
       "\n",
       "          [[0.6922, 0.6757, 0.6158,  ..., 0.5989, 0.6846, 0.5824],\n",
       "           [0.6424, 0.5970, 0.6891,  ..., 0.6098, 0.6846, 0.6603],\n",
       "           [0.6195, 0.5836, 0.6121,  ..., 0.6928, 0.6180, 0.6619],\n",
       "           ...,\n",
       "           [0.5834, 0.6724, 0.6931,  ..., 0.5882, 0.6353, 0.5912],\n",
       "           [0.6870, 0.6027, 0.6780,  ..., 0.6019, 0.5933, 0.5830],\n",
       "           [0.6930, 0.6649, 0.6854,  ..., 0.6762, 0.5879, 0.5909]],\n",
       "\n",
       "          [[0.6007, 0.6917, 0.6931,  ..., 0.6284, 0.6412, 0.6828],\n",
       "           [0.5825, 0.6811, 0.6153,  ..., 0.5924, 0.5843, 0.5825],\n",
       "           [0.5872, 0.6339, 0.6465,  ..., 0.5982, 0.6110, 0.6536],\n",
       "           ...,\n",
       "           [0.5898, 0.6197, 0.5844,  ..., 0.6909, 0.6050, 0.5975],\n",
       "           [0.6430, 0.6875, 0.6929,  ..., 0.6216, 0.6603, 0.6067],\n",
       "           [0.6895, 0.6526, 0.5941,  ..., 0.6931, 0.5870, 0.6889]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.5890, 0.6394, 0.6553,  ..., 0.6619, 0.6409, 0.5880],\n",
       "           [0.6400, 0.6728, 0.6757,  ..., 0.6042, 0.6893, 0.5965],\n",
       "           [0.6431, 0.5956, 0.6337,  ..., 0.6142, 0.5944, 0.6704],\n",
       "           ...,\n",
       "           [0.6421, 0.5836, 0.6275,  ..., 0.6930, 0.5843, 0.5938],\n",
       "           [0.6861, 0.6895, 0.5836,  ..., 0.6758, 0.6717, 0.5898],\n",
       "           [0.5840, 0.6300, 0.6264,  ..., 0.6128, 0.6404, 0.6667]],\n",
       "\n",
       "          [[0.6499, 0.5951, 0.5833,  ..., 0.6907, 0.5937, 0.6528],\n",
       "           [0.5864, 0.5887, 0.6083,  ..., 0.6501, 0.5984, 0.5957],\n",
       "           [0.6467, 0.6931, 0.5825,  ..., 0.6674, 0.6895, 0.6714],\n",
       "           ...,\n",
       "           [0.6778, 0.5908, 0.6788,  ..., 0.6415, 0.6142, 0.5970],\n",
       "           [0.6139, 0.5875, 0.6551,  ..., 0.6852, 0.6078, 0.6291],\n",
       "           [0.6287, 0.5853, 0.6898,  ..., 0.5988, 0.6433, 0.6666]],\n",
       "\n",
       "          [[0.6754, 0.6085, 0.6608,  ..., 0.6873, 0.6027, 0.5897],\n",
       "           [0.5876, 0.6211, 0.6602,  ..., 0.6741, 0.6112, 0.5936],\n",
       "           [0.6003, 0.6929, 0.6508,  ..., 0.6640, 0.5950, 0.6549],\n",
       "           ...,\n",
       "           [0.6920, 0.5963, 0.6736,  ..., 0.5878, 0.6284, 0.5833],\n",
       "           [0.5970, 0.6037, 0.6894,  ..., 0.5829, 0.6069, 0.6810],\n",
       "           [0.6055, 0.5947, 0.6498,  ..., 0.6282, 0.5842, 0.6702]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.6835, 0.6644, 0.6080,  ..., 0.6012, 0.6505, 0.6890],\n",
       "           [0.6676, 0.6157, 0.6444,  ..., 0.6717, 0.6514, 0.6176],\n",
       "           [0.5840, 0.5965, 0.5865,  ..., 0.6371, 0.6373, 0.6773],\n",
       "           ...,\n",
       "           [0.6931, 0.5828, 0.6158,  ..., 0.5877, 0.6632, 0.6929],\n",
       "           [0.6217, 0.6670, 0.6331,  ..., 0.6606, 0.5875, 0.6102],\n",
       "           [0.6913, 0.6930, 0.6493,  ..., 0.6260, 0.5907, 0.6429]],\n",
       "\n",
       "          [[0.6930, 0.6607, 0.6718,  ..., 0.6497, 0.5933, 0.6521],\n",
       "           [0.5914, 0.6281, 0.5829,  ..., 0.6315, 0.6830, 0.6930],\n",
       "           [0.6542, 0.6337, 0.5864,  ..., 0.6827, 0.6929, 0.6925],\n",
       "           ...,\n",
       "           [0.6647, 0.6624, 0.6523,  ..., 0.6427, 0.5981, 0.6513],\n",
       "           [0.5867, 0.6797, 0.6879,  ..., 0.5987, 0.6226, 0.6670],\n",
       "           [0.6704, 0.6928, 0.5935,  ..., 0.6893, 0.6499, 0.5843]],\n",
       "\n",
       "          [[0.6929, 0.6107, 0.6929,  ..., 0.5828, 0.6450, 0.6021],\n",
       "           [0.5916, 0.6908, 0.6650,  ..., 0.5986, 0.6664, 0.6845],\n",
       "           [0.6845, 0.6835, 0.6162,  ..., 0.5891, 0.5870, 0.6388],\n",
       "           ...,\n",
       "           [0.6155, 0.5879, 0.6928,  ..., 0.6777, 0.6335, 0.6026],\n",
       "           [0.6144, 0.6868, 0.6866,  ..., 0.6908, 0.5937, 0.6634],\n",
       "           [0.6108, 0.5932, 0.6438,  ..., 0.5877, 0.5904, 0.6863]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.6039, 0.6627, 0.6603,  ..., 0.5848, 0.5942, 0.6685],\n",
       "           [0.6016, 0.6930, 0.6035,  ..., 0.5951, 0.6148, 0.6667],\n",
       "           [0.6027, 0.5825, 0.5904,  ..., 0.6931, 0.6919, 0.6576],\n",
       "           ...,\n",
       "           [0.6420, 0.6045, 0.6429,  ..., 0.6486, 0.6919, 0.6843],\n",
       "           [0.6081, 0.6101, 0.6900,  ..., 0.6565, 0.6684, 0.6554],\n",
       "           [0.5923, 0.6755, 0.5967,  ..., 0.6911, 0.6341, 0.6917]],\n",
       "\n",
       "          [[0.5841, 0.5868, 0.6719,  ..., 0.5825, 0.6551, 0.6885],\n",
       "           [0.6013, 0.5854, 0.5924,  ..., 0.6467, 0.6102, 0.6295],\n",
       "           [0.6865, 0.6732, 0.6000,  ..., 0.6914, 0.5973, 0.6241],\n",
       "           ...,\n",
       "           [0.6306, 0.6656, 0.6018,  ..., 0.6626, 0.6475, 0.5926],\n",
       "           [0.6212, 0.6931, 0.5869,  ..., 0.6447, 0.6554, 0.6706],\n",
       "           [0.6906, 0.6497, 0.5826,  ..., 0.6904, 0.6414, 0.5833]],\n",
       "\n",
       "          [[0.6079, 0.6845, 0.6427,  ..., 0.6784, 0.6568, 0.6926],\n",
       "           [0.5866, 0.5993, 0.5987,  ..., 0.6519, 0.6891, 0.6743],\n",
       "           [0.5856, 0.6469, 0.6069,  ..., 0.6101, 0.6372, 0.6807],\n",
       "           ...,\n",
       "           [0.5826, 0.6918, 0.6172,  ..., 0.6931, 0.5920, 0.6040],\n",
       "           [0.6291, 0.6354, 0.6817,  ..., 0.6581, 0.6663, 0.6087],\n",
       "           [0.6514, 0.6578, 0.6316,  ..., 0.5872, 0.6167, 0.6363]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.6768, 0.6557, 0.5922,  ..., 0.6865, 0.6038, 0.6901],\n",
       "           [0.5874, 0.6214, 0.6569,  ..., 0.6621, 0.6846, 0.5970],\n",
       "           [0.6531, 0.6010, 0.6193,  ..., 0.6080, 0.5976, 0.5829],\n",
       "           ...,\n",
       "           [0.6861, 0.5950, 0.6011,  ..., 0.6920, 0.6286, 0.6884],\n",
       "           [0.6036, 0.5915, 0.6112,  ..., 0.5839, 0.5985, 0.6256],\n",
       "           [0.6316, 0.6024, 0.6298,  ..., 0.6512, 0.6869, 0.6900]],\n",
       "\n",
       "          [[0.6398, 0.6920, 0.6002,  ..., 0.6662, 0.6868, 0.6054],\n",
       "           [0.6760, 0.6685, 0.6532,  ..., 0.5947, 0.6701, 0.6827],\n",
       "           [0.6015, 0.5971, 0.6587,  ..., 0.5991, 0.6420, 0.5824],\n",
       "           ...,\n",
       "           [0.6088, 0.6074, 0.5858,  ..., 0.6487, 0.6920, 0.6779],\n",
       "           [0.6101, 0.6780, 0.6919,  ..., 0.6517, 0.6346, 0.5866],\n",
       "           [0.5907, 0.5827, 0.5876,  ..., 0.5954, 0.6086, 0.6094]],\n",
       "\n",
       "          [[0.5888, 0.6872, 0.6856,  ..., 0.6617, 0.6100, 0.6537],\n",
       "           [0.6916, 0.6644, 0.6537,  ..., 0.6478, 0.6234, 0.6843],\n",
       "           [0.6929, 0.6927, 0.5889,  ..., 0.6717, 0.6620, 0.6807],\n",
       "           ...,\n",
       "           [0.6866, 0.6216, 0.5857,  ..., 0.6615, 0.6460, 0.6264],\n",
       "           [0.6190, 0.6442, 0.6762,  ..., 0.6508, 0.6194, 0.6164],\n",
       "           [0.6625, 0.6924, 0.5925,  ..., 0.6228, 0.6905, 0.5832]]]],\n",
       "\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.5829, 0.6837, 0.6931,  ..., 0.6190, 0.6736, 0.6785],\n",
       "           [0.6281, 0.6651, 0.5867,  ..., 0.6537, 0.6361, 0.5956],\n",
       "           [0.6877, 0.5941, 0.6132,  ..., 0.6127, 0.6742, 0.5928],\n",
       "           ...,\n",
       "           [0.6929, 0.6878, 0.6220,  ..., 0.6909, 0.6797, 0.5878],\n",
       "           [0.6050, 0.6280, 0.5985,  ..., 0.6922, 0.6505, 0.6166],\n",
       "           [0.6143, 0.6460, 0.6713,  ..., 0.6829, 0.6068, 0.5893]],\n",
       "\n",
       "          [[0.5857, 0.5862, 0.6848,  ..., 0.6722, 0.6445, 0.6319],\n",
       "           [0.5861, 0.5887, 0.5877,  ..., 0.6301, 0.6200, 0.5966],\n",
       "           [0.5864, 0.5829, 0.5881,  ..., 0.6220, 0.6456, 0.6789],\n",
       "           ...,\n",
       "           [0.6165, 0.6121, 0.5957,  ..., 0.6914, 0.6550, 0.6891],\n",
       "           [0.6677, 0.6347, 0.5852,  ..., 0.6395, 0.6328, 0.6181],\n",
       "           [0.6564, 0.6774, 0.5843,  ..., 0.6870, 0.6769, 0.6894]],\n",
       "\n",
       "          [[0.5839, 0.6645, 0.6381,  ..., 0.6564, 0.5923, 0.6887],\n",
       "           [0.5973, 0.6166, 0.6072,  ..., 0.5853, 0.6006, 0.5947],\n",
       "           [0.5835, 0.6560, 0.6591,  ..., 0.5835, 0.6330, 0.6924],\n",
       "           ...,\n",
       "           [0.5878, 0.6756, 0.6774,  ..., 0.5908, 0.6930, 0.6377],\n",
       "           [0.6317, 0.6453, 0.6920,  ..., 0.6235, 0.5942, 0.6931],\n",
       "           [0.6647, 0.6468, 0.6660,  ..., 0.6912, 0.5859, 0.5955]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.6132, 0.6928, 0.5902,  ..., 0.6511, 0.5923, 0.6812],\n",
       "           [0.5855, 0.6743, 0.6361,  ..., 0.6470, 0.6453, 0.6545],\n",
       "           [0.6058, 0.6558, 0.6525,  ..., 0.6931, 0.6922, 0.5833],\n",
       "           ...,\n",
       "           [0.6473, 0.6534, 0.6651,  ..., 0.6927, 0.6931, 0.6206],\n",
       "           [0.5879, 0.6752, 0.5942,  ..., 0.6394, 0.6602, 0.5936],\n",
       "           [0.6381, 0.6253, 0.6089,  ..., 0.5833, 0.5873, 0.6724]],\n",
       "\n",
       "          [[0.5987, 0.6197, 0.6128,  ..., 0.6863, 0.6867, 0.6931],\n",
       "           [0.5853, 0.6522, 0.6679,  ..., 0.6926, 0.5830, 0.6013],\n",
       "           [0.6136, 0.6182, 0.6077,  ..., 0.6273, 0.6363, 0.6114],\n",
       "           ...,\n",
       "           [0.6422, 0.5993, 0.6523,  ..., 0.6687, 0.6843, 0.5901],\n",
       "           [0.6771, 0.6661, 0.6830,  ..., 0.6930, 0.6923, 0.6140],\n",
       "           [0.6826, 0.6703, 0.5934,  ..., 0.5879, 0.5906, 0.5928]],\n",
       "\n",
       "          [[0.5898, 0.6902, 0.6115,  ..., 0.6319, 0.6307, 0.5867],\n",
       "           [0.6929, 0.6910, 0.6455,  ..., 0.5889, 0.6931, 0.6688],\n",
       "           [0.5971, 0.6183, 0.6556,  ..., 0.6867, 0.5852, 0.6470],\n",
       "           ...,\n",
       "           [0.6916, 0.5848, 0.6931,  ..., 0.5845, 0.6352, 0.6326],\n",
       "           [0.5985, 0.6126, 0.6342,  ..., 0.5885, 0.6628, 0.6694],\n",
       "           [0.6616, 0.5874, 0.6902,  ..., 0.6914, 0.5824, 0.6429]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.5849, 0.6824, 0.6398,  ..., 0.6925, 0.5851, 0.6193],\n",
       "           [0.6486, 0.6538, 0.6370,  ..., 0.5974, 0.5863, 0.5874],\n",
       "           [0.6644, 0.6273, 0.6897,  ..., 0.6931, 0.6543, 0.6923],\n",
       "           ...,\n",
       "           [0.5937, 0.5910, 0.5931,  ..., 0.5971, 0.6166, 0.6118],\n",
       "           [0.6279, 0.6186, 0.6133,  ..., 0.6931, 0.6806, 0.6222],\n",
       "           [0.5908, 0.6592, 0.5969,  ..., 0.6187, 0.5842, 0.6287]],\n",
       "\n",
       "          [[0.6590, 0.6526, 0.6748,  ..., 0.6496, 0.6021, 0.6438],\n",
       "           [0.6215, 0.6263, 0.5846,  ..., 0.6374, 0.6169, 0.6922],\n",
       "           [0.6508, 0.6902, 0.5964,  ..., 0.6911, 0.5923, 0.6918],\n",
       "           ...,\n",
       "           [0.6745, 0.5923, 0.6928,  ..., 0.6427, 0.6784, 0.5849],\n",
       "           [0.6259, 0.6849, 0.6028,  ..., 0.6384, 0.6668, 0.5961],\n",
       "           [0.6909, 0.6863, 0.6065,  ..., 0.6846, 0.6912, 0.5850]],\n",
       "\n",
       "          [[0.6042, 0.6863, 0.6927,  ..., 0.6074, 0.6581, 0.6274],\n",
       "           [0.5917, 0.6770, 0.5928,  ..., 0.5930, 0.6590, 0.6769],\n",
       "           [0.6879, 0.6040, 0.6930,  ..., 0.5832, 0.5929, 0.6698],\n",
       "           ...,\n",
       "           [0.5860, 0.6788, 0.6102,  ..., 0.6906, 0.6753, 0.6898],\n",
       "           [0.6824, 0.6556, 0.6705,  ..., 0.6693, 0.6064, 0.5991],\n",
       "           [0.6931, 0.6062, 0.6151,  ..., 0.6450, 0.5824, 0.5943]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.6556, 0.5952, 0.6365,  ..., 0.6844, 0.6450, 0.6272],\n",
       "           [0.6318, 0.6071, 0.6326,  ..., 0.5876, 0.5824, 0.6638],\n",
       "           [0.5935, 0.6402, 0.6638,  ..., 0.5952, 0.6104, 0.5866],\n",
       "           ...,\n",
       "           [0.6497, 0.5930, 0.6266,  ..., 0.6771, 0.6919, 0.6688],\n",
       "           [0.6890, 0.6931, 0.6023,  ..., 0.5992, 0.6381, 0.5849],\n",
       "           [0.6040, 0.6803, 0.6530,  ..., 0.5919, 0.5824, 0.5833]],\n",
       "\n",
       "          [[0.6931, 0.6901, 0.6338,  ..., 0.6299, 0.6521, 0.5961],\n",
       "           [0.6111, 0.6535, 0.5951,  ..., 0.5921, 0.6063, 0.5830],\n",
       "           [0.6176, 0.6301, 0.6811,  ..., 0.5972, 0.6444, 0.6907],\n",
       "           ...,\n",
       "           [0.6447, 0.5853, 0.6019,  ..., 0.5823, 0.5927, 0.6265],\n",
       "           [0.5833, 0.6931, 0.6416,  ..., 0.6205, 0.6087, 0.6322],\n",
       "           [0.5922, 0.6825, 0.6066,  ..., 0.6495, 0.6146, 0.6868]],\n",
       "\n",
       "          [[0.6922, 0.5828, 0.6931,  ..., 0.6103, 0.6916, 0.6715],\n",
       "           [0.5906, 0.6153, 0.6785,  ..., 0.5835, 0.6131, 0.6888],\n",
       "           [0.6924, 0.6731, 0.5866,  ..., 0.5825, 0.5851, 0.6004],\n",
       "           ...,\n",
       "           [0.5938, 0.6510, 0.6747,  ..., 0.6111, 0.5869, 0.6621],\n",
       "           [0.6008, 0.5826, 0.6018,  ..., 0.6852, 0.5951, 0.5828],\n",
       "           [0.5841, 0.6186, 0.5947,  ..., 0.6650, 0.5967, 0.5857]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.6274, 0.6475, 0.6419,  ..., 0.6171, 0.6832, 0.6931],\n",
       "           [0.5932, 0.6661, 0.6158,  ..., 0.6037, 0.5916, 0.6068],\n",
       "           [0.5941, 0.5830, 0.6043,  ..., 0.6131, 0.6920, 0.6663],\n",
       "           ...,\n",
       "           [0.6825, 0.6196, 0.5828,  ..., 0.5827, 0.5850, 0.6252],\n",
       "           [0.6931, 0.5926, 0.6341,  ..., 0.5853, 0.5835, 0.6694],\n",
       "           [0.6380, 0.6597, 0.6910,  ..., 0.6141, 0.6458, 0.6450]],\n",
       "\n",
       "          [[0.6861, 0.6069, 0.5875,  ..., 0.5886, 0.6140, 0.5981],\n",
       "           [0.5977, 0.6163, 0.6090,  ..., 0.6180, 0.6061, 0.6648],\n",
       "           [0.5940, 0.6324, 0.5955,  ..., 0.6807, 0.5839, 0.6402],\n",
       "           ...,\n",
       "           [0.5912, 0.6921, 0.6807,  ..., 0.6230, 0.6713, 0.5916],\n",
       "           [0.6924, 0.5863, 0.6267,  ..., 0.6440, 0.6282, 0.6889],\n",
       "           [0.6804, 0.5826, 0.6101,  ..., 0.5852, 0.5990, 0.5876]],\n",
       "\n",
       "          [[0.6096, 0.5872, 0.5971,  ..., 0.6134, 0.6671, 0.6554],\n",
       "           [0.6737, 0.6224, 0.6197,  ..., 0.6612, 0.5852, 0.5885],\n",
       "           [0.6902, 0.6093, 0.6055,  ..., 0.6701, 0.5924, 0.6660],\n",
       "           ...,\n",
       "           [0.6060, 0.6927, 0.6691,  ..., 0.6231, 0.5963, 0.6213],\n",
       "           [0.6918, 0.6428, 0.5834,  ..., 0.6162, 0.6356, 0.6278],\n",
       "           [0.6486, 0.5920, 0.5879,  ..., 0.6620, 0.6926, 0.5958]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.5887, 0.6036, 0.6746,  ..., 0.5899, 0.6842, 0.5983],\n",
       "           [0.6458, 0.6684, 0.6304,  ..., 0.6302, 0.6551, 0.5961],\n",
       "           [0.6906, 0.6075, 0.6097,  ..., 0.6178, 0.6931, 0.5886],\n",
       "           ...,\n",
       "           [0.6852, 0.6045, 0.5826,  ..., 0.6920, 0.6409, 0.6755],\n",
       "           [0.6315, 0.6635, 0.6456,  ..., 0.6915, 0.5825, 0.5962],\n",
       "           [0.6560, 0.6930, 0.5932,  ..., 0.5846, 0.6036, 0.5855]],\n",
       "\n",
       "          [[0.6597, 0.6924, 0.5874,  ..., 0.6423, 0.5880, 0.6931],\n",
       "           [0.5952, 0.6059, 0.5992,  ..., 0.6462, 0.5832, 0.5925],\n",
       "           [0.6691, 0.5860, 0.5854,  ..., 0.6023, 0.6311, 0.5863],\n",
       "           ...,\n",
       "           [0.6118, 0.6013, 0.6402,  ..., 0.6918, 0.5833, 0.6511],\n",
       "           [0.6163, 0.5826, 0.6881,  ..., 0.6716, 0.6138, 0.6930],\n",
       "           [0.6931, 0.6292, 0.6931,  ..., 0.5874, 0.5824, 0.5828]],\n",
       "\n",
       "          [[0.6220, 0.6911, 0.6292,  ..., 0.6265, 0.6887, 0.6008],\n",
       "           [0.5841, 0.6924, 0.5948,  ..., 0.6917, 0.5854, 0.6713],\n",
       "           [0.6863, 0.6430, 0.6923,  ..., 0.6881, 0.6882, 0.6589],\n",
       "           ...,\n",
       "           [0.5913, 0.6015, 0.6676,  ..., 0.6555, 0.6841, 0.6892],\n",
       "           [0.6796, 0.6902, 0.6323,  ..., 0.6265, 0.6659, 0.6903],\n",
       "           [0.6146, 0.6019, 0.6894,  ..., 0.6686, 0.6566, 0.6483]]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consistency_weight = 0.1 * ramps.sigmoid_rampup(100//150, 40)\n",
    "\n",
    "\n",
    "consistency_dist = consistency_criterion(outputs[labeled_bs:], ema_output) #(batch, 2, 112,112,80)\n",
    "threshold = (0.75+0.25*ramps.sigmoid_rampup(iter_num, max_iterations))*np.log(2)\n",
    "mask = (uncertainty<threshold).float()\n",
    "consistency_dist = torch.sum(mask*consistency_dist)/(2*torch.sum(mask)+1e-16)\n",
    "consistency_loss = consistency_weight * consistency_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_236047/2378503869.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiceLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtarget_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiceLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'B' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from monai.losses import DiceLoss\n",
    "input = torch.rand(B, C, H, W)\n",
    "target_idx = torch.randint(low=0, high=C - 1, size=(B, H, W)).long()\n",
    "target = one_hot(target_idx[:, None, ...], num_classes=C)\n",
    "self = DiceLoss(reduction='none')\n",
    "loss = self(input, target)\n",
    "assert np.broadcast_shapes(loss.shape, input.shape) == input.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic trick to reload library in jupyter: import utils as nash\n",
    "from importlib import reload\n",
    "reload(nash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/FDG-PET-CT-Lesions_nifti/'\n",
    "\n",
    "patients = list(np.load(os.path.join(data_path+'positive_patients.npy')))\n",
    "patients_train, patients_test = train_test_split(patients, test_size = 0.1, random_state=0)\n",
    "\n",
    "all_paths_train= []\n",
    "all_paths_test = []\n",
    "for patient in patients_train:\n",
    "    examens = os.listdir(os.path.join(data_path,patient))\n",
    "    for exam in examens:\n",
    "        all_paths_train.append(os.path.join(os.path.join(data_path,patient), exam))\n",
    "        \n",
    "for patient in patients_test:\n",
    "    examens = os.listdir(os.path.join(data_path,patient))\n",
    "    for exam in examens:\n",
    "        all_paths_test.append(os.path.join(os.path.join(data_path,patient), exam))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 489 subjects\n"
     ]
    }
   ],
   "source": [
    "subjects_train = []\n",
    "for path in all_paths_train:\n",
    "    subject = tio.Subject(\n",
    "        ct=tio.ScalarImage(os.path.join(path, 'CTres.nii.gz') ),\n",
    "        pet=tio.ScalarImage(os.path.join(path, 'SUV.nii.gz') ),\n",
    "        segmentation=tio.LabelMap(os.path.join(path, 'SEG.nii.gz') ),\n",
    "    )\n",
    "    subjects_train.append(subject)\n",
    "dataset_train = tio.SubjectsDataset(subjects_train)\n",
    "print('Dataset size:', len(dataset_train), 'subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 54 subjects\n"
     ]
    }
   ],
   "source": [
    "subjects_test = []\n",
    "for path in all_paths_test:\n",
    "    subject = tio.Subject(\n",
    "        ct=tio.ScalarImage(os.path.join(path, 'CTres.nii.gz') ),\n",
    "        pet=tio.ScalarImage(os.path.join(path, 'SUV.nii.gz') ),\n",
    "        segmentation=tio.LabelMap(os.path.join(path, 'SEG.nii.gz') ),\n",
    "    )\n",
    "    subjects_test.append(subject)\n",
    "dataset_test = tio.SubjectsDataset(subjects_test)\n",
    "print('Dataset size:', len(dataset_test), 'subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labelled set: 48 subjects \t Training unlabelled set: 441 subjects\n",
      "Validation set: 54 subjects\n"
     ]
    }
   ],
   "source": [
    "training_transform = tio.Compose([\n",
    "    tio.ToCanonical(),\n",
    "    tio.OneHot(num_classes=2)])\n",
    "\n",
    "validation_transform = tio.Compose([\n",
    "    tio.ToCanonical(),\n",
    "    tio.Resample(4),\n",
    "    tio.OneHot(num_classes=2)\n",
    "    ])\n",
    "\n",
    "labelled_split_ratio = 0.1\n",
    "n_train = len(dataset_train)\n",
    "nl = int(labelled_split_ratio * n_train)\n",
    "nu = n_train - nl\n",
    "\n",
    "num_split_subjects = nl, nu\n",
    "labelled_subjects, unlabelled_subjects = torch.utils.data.random_split(subjects_train, num_split_subjects)\n",
    "\n",
    "training_labelled_set = tio.SubjectsDataset(\n",
    "    labelled_subjects, transform=training_transform)\n",
    "\n",
    "training_unlabelled_set = tio.SubjectsDataset(\n",
    "    unlabelled_subjects, transform=training_transform)\n",
    "\n",
    "\n",
    "validation_set = tio.SubjectsDataset(\n",
    "    subjects_test, transform=validation_transform)\n",
    "\n",
    "print('Training labelled set:', len(training_labelled_set), 'subjects \\t Training unlabelled set:', len(training_unlabelled_set), 'subjects')\n",
    "print('Validation set:', len(validation_set), 'subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch_size = 1\n",
    "validation_batch_size = 2 * training_batch_size\n",
    "\n",
    "patch_size = (128,128,30)\n",
    "samples_per_volume = 1\n",
    "max_queue_length = 1\n",
    "sampler = tio.data.UniformSampler(patch_size)\n",
    "\n",
    "patches_training_labelled_set = tio.Queue(\n",
    "    subjects_dataset=training_labelled_set,\n",
    "    max_length=max_queue_length,\n",
    "    samples_per_volume=samples_per_volume,\n",
    "    sampler=sampler,\n",
    "    num_workers=num_workers,\n",
    "    shuffle_subjects=True,\n",
    "    shuffle_patches=True,\n",
    ")\n",
    "\n",
    "patches_training_unlabelled_set = tio.Queue(\n",
    "    subjects_dataset=training_unlabelled_set,\n",
    "    max_length=max_queue_length,\n",
    "    samples_per_volume=samples_per_volume,\n",
    "    sampler=sampler,\n",
    "    num_workers=num_workers,\n",
    "    shuffle_subjects=True,\n",
    "    shuffle_patches=True,\n",
    ")\n",
    "\n",
    "patches_validation_set = tio.Queue(\n",
    "    subjects_dataset=validation_set,\n",
    "    max_length=max_queue_length,\n",
    "    samples_per_volume=samples_per_volume,\n",
    "    sampler=sampler,\n",
    "    num_workers=num_workers,\n",
    "    shuffle_subjects=False,\n",
    "    shuffle_patches=False,\n",
    ")\n",
    "\n",
    "training_labelled_loader_patches = torch.utils.data.DataLoader(\n",
    "    patches_training_labelled_set, batch_size=training_batch_size)\n",
    "\n",
    "training_unlabelled_loader_patches = torch.utils.data.DataLoader(\n",
    "    patches_training_unlabelled_set, batch_size=training_batch_size)\n",
    "\n",
    "validation_loader_patches = torch.utils.data.DataLoader(\n",
    "    patches_validation_set, batch_size=validation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch = next(iter(training_labelled_loader_patches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'one_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_296619/2578239744.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segmentation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'one_batch' is not defined"
     ]
    }
   ],
   "source": [
    "one_batch['segmentation']['data'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import prepare_batch, get_dice_score, get_dice_loss, get_model_and_optimizer, run_epoch, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(enum.Enum):\n",
    "    TRAIN = 'Training'\n",
    "    VALIDATE = 'Validation'\n",
    "\n",
    "CHANNELS_DIMENSION = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "unet = monai.networks.nets.UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=2,\n",
    "    out_channels=2,\n",
    "    channels=(4, 8, 16),\n",
    "    strides=(1, 1),\n",
    ")\n",
    "unet = unet.to(device)\n",
    "\n",
    "learning_rate=1e-2\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
    "loss_function = monai.losses.DiceCELoss(softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018088102340698242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 455,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf8852bfcb84d17b5e93612b2a92c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.339525818824768\n",
      "0.8386653661727905\n",
      "0.6340301632881165\n",
      "0.5822932720184326\n",
      "0.5214874148368835\n",
      "0.5083507895469666\n",
      "0.5064225196838379\n",
      "0.5090855360031128\n",
      "0.50334233045578\n",
      "0.5215180516242981\n",
      "0.5010581612586975\n",
      "0.5000287890434265\n",
      "0.5000192523002625\n",
      "0.5914199352264404\n",
      "0.5000523328781128\n",
      "0.49980300664901733\n",
      "0.500038206577301\n",
      "0.5000171661376953\n",
      "0.49922457337379456\n",
      "0.4952628016471863\n",
      "0.49142563343048096\n",
      "0.4682731330394745\n",
      "0.4998035132884979\n",
      "0.49993032217025757\n",
      "0.3426441252231598\n",
      "0.4995730221271515\n",
      "0.4985935389995575\n",
      "0.49533960223197937\n",
      "0.25100409984588623\n",
      "0.03877878189086914\n",
      "0.011460274457931519\n",
      "0.01946359872817993\n",
      "0.0007806122303009033\n",
      "0.0005221366882324219\n",
      "0.0001512467861175537\n",
      "1.233816146850586e-05\n",
      "5.602836608886719e-06\n",
      "2.0772218704223633e-05\n",
      "0.0\n",
      "5.960464477539063e-08\n",
      "3.2782554626464844e-07\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5039191246032715\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.1045666933059692\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0360136032104492\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "10.723427772521973\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.3185423612594604\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.9636882543563843\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "8.331445693969727\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.5456047058105469\n",
      "0.0\n",
      "84.53878784179688\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "89.6466293334961\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.904725193977356\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "26.899808883666992\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.47677329182624817\n",
      "0.0\n",
      "0.7613092660903931\n",
      "0.0\n",
      "0.0\n",
      "0.4977451264858246\n",
      "0.4998355507850647\n",
      "0.32992619276046753\n",
      "0.5017443299293518\n",
      "5.279759883880615\n",
      "0.8623708486557007\n",
      "5.742029190063477\n",
      "4.304589748382568\n",
      "1.7624526023864746\n",
      "16.47402572631836\n",
      "0.5042043328285217\n",
      "0.5633251070976257\n",
      "0.5079419612884521\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "4.682466983795166\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "for batch_idx, batch in enumerate(tqdm(training_labelled_loader_patches)):\n",
    "    inputs, targets = prepare_batch(batch, device)\n",
    "    logits = unet(inputs)\n",
    "    batch_loss = loss_function(logits, targets)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    print(batch_loss.item())\n",
    "    epoch_losses.append(batch_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02100682258605957,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 455,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af9797f94e84dc6acaf47a23dccd1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.6248313784599304\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.7487013339996338\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "54.669124603271484\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "8.617166519165039\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "9.288957595825195\n",
      "0.0\n",
      "1.0173509120941162\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "34.76984786987305\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.0428013801574707\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "9.69432258605957\n",
      "0.0\n",
      "0.0\n",
      "19.74350929260254\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "86.4998550415039\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.1308643817901611\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5002224445343018\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.4999990165233612\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.38589394092559814\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.036817699670791626\n",
      "0.0\n",
      "0.0\n",
      "0.5063086748123169\n",
      "0.501122236251831\n",
      "0.0\n",
      "0.5265758633613586\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5008740425109863\n",
      "0.0\n",
      "0.0\n",
      "0.5016178488731384\n",
      "31.267986297607422\n",
      "1.8647372722625732\n",
      "0.0\n",
      "0.5009209513664246\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.4981011748313904\n",
      "0.5009559988975525\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5959213972091675\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5000317096710205\n",
      "0.0\n",
      "0.0\n",
      "1.126568078994751\n",
      "0.501762330532074\n",
      "0.0\n",
      "0.0\n",
      "0.8098269104957581\n",
      "0.0\n",
      "0.0\n",
      "13.922237396240234\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5097173452377319\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5107617974281311\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "4.094592094421387\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5012751817703247\n",
      "1.3536425828933716\n",
      "0.0\n",
      "0.4996713101863861\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5477495193481445\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5224961042404175\n",
      "0.0\n",
      "0.5038085579872131\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5030964612960815\n",
      "0.6803056001663208\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.7881393432617188e-07\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5133772492408752\n",
      "0.0\n",
      "0.0\n",
      "0.5002917051315308\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "for batch_idx, batch in enumerate(tqdm(training_labelled_loader_patches)):\n",
    "    inputs, targets = prepare_batch(batch, device)\n",
    "    logits = unet(inputs)\n",
    "    batch_loss = loss_function(logits, targets)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    print(batch_loss.item())\n",
    "    epoch_losses.append(batch_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efcb02c38b0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 369
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/hschmutz/Documents/autoPEt/utils.py'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_losses_val = []\n",
    "for batch_idx, batch in enumerate(tqdm(training_loader_patches)):\n",
    "    inputs, targets = prepare_batch(one_batch, device)\n",
    "    logits = unet(inputs)\n",
    "    batch_losses = get_dice_loss(probabilities, targets)\n",
    "    batch_loss = batch_losses.mean()\n",
    "    epoch_losses_val.append(batch_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
