{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last run on Thu Sep 14 10:16:09 2023\n",
      "TorchIO version: 0.18.90\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "from pathlib import Path\n",
    "import os \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchio as tio\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from unet import UNet\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "from tqdm.auto import tqdm\n",
    "# Config\n",
    "seed = 42  # for reproducibility\n",
    "training_split_ratio = 0.9  # use 90% of samples for training, 10% for testing\n",
    "num_epochs = 5\n",
    "\n",
    "# If the following values are False, the models will be downloaded and not computed\n",
    "compute_histograms = False\n",
    "train_whole_images = False \n",
    "train_patches = False\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "plt.rcParams['figure.figsize'] = 30, 10\n",
    "\n",
    "print('Last run on', time.ctime())\n",
    "print('TorchIO version:', tio.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4711, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.sum(loss_Ce_torch(output,target)*mask[:,0], dim = (1,2,3))/torch.sum(mask[:,0], dim = (1,2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4711, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(loss_Ce_torch(output,target)*mask[:,0])/torch.sum(mask[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0666, 3.0653, 3.0184, 3.0274, 3.0400, 3.0221, 3.0364, 3.0505, 3.0350,\n",
       "        3.0497, 3.0512, 3.0561], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(loss_MSE(output,target)[:,0]*mask[:,0],  dim = (1,2,3))/torch.sum(mask[:,0], dim = (1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(334592.2500, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(torch.sum(loss_MSE(proba,target)[:,1]*mask[:,0],  dim = (1,2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(756884)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(mask[:,0], dim = (1,2,3)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.0140, 3.0377, 3.0620, 3.0652, 3.0518, 3.0730, 3.0527, 3.0323, 3.0530,\n",
       "        3.0567, 3.0325, 3.0205], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(loss_MSE(output,target)[:,1]*mask[:,0],  dim = (1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2305419.5000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(loss_MSE(output,target)[:,1]*mask[:,0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4421, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(torch.sum(loss_MSE(proba,target)[:,1]*mask[:,0], dim = (1,2,3))/torch.sum(mask[:,0], dim = (1,2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4421, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_loss import MaskedMSELoss\n",
    "loss = MaskedMSELoss(to_onehot_y=False,softmax=True,include_background=False,batch=True, reduction='mean')\n",
    "loss(output, target, mask[:,:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(334592.2500, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(loss_MSE(proba,target)[:,1]*mask[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8919, 0.8922, 0.8936, 0.8932, 0.8919, 0.8938, 0.8927, 0.8922, 0.8934,\n",
       "        0.8928, 0.8913, 0.8929], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = monai.losses.DiceLoss(to_onehot_y=False,softmax=True,include_background=False,batch=False, reduction='none')\n",
    "loss(output*mask, target*mask).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE full:  tensor(0.9032, grad_fn=<AddBackward0>) tensor(0.9032, grad_fn=<DivBackward1>) tensor(0.9032, grad_fn=<MeanBackward0>) tensor(0.9032, grad_fn=<MeanBackward0>)\n",
      "CE masked:  tensor(1.4712, grad_fn=<DivBackward0>) tensor(1.4711, grad_fn=<DivBackward0>) tensor(1.4711, grad_fn=<MeanBackward0>)\n",
      "Dice:  tensor(0.8927, grad_fn=<MeanBackward0>) tensor(0.8927, grad_fn=<MeanBackward0>) tensor(0.5001, grad_fn=<MeanBackward0>)\n",
      "MSE:  tensor(0.3185, grad_fn=<MeanBackward0>) tensor(0.3186, grad_fn=<MeanBackward0>)\n",
      "torch.Size([12, 1, 128, 128, 32])\n",
      "MSE masked: tensor(0.4423, grad_fn=<DivBackward0>) tensor(0.4421, grad_fn=<DivBackward0>) tensor(0.4421, grad_fn=<MeanBackward0>)\n",
      "DiceCE masked:  tensor(2.3638, grad_fn=<AddBackward0>) tensor(2.3637, grad_fn=<AddBackward0>) tensor(0.1770, grad_fn=<AddBackward0>)\n",
      "tensor(2.3637, grad_fn=<AddBackward0>)\n",
      "tensor(0.5909, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import monai\n",
    "from utils_loss import MSELoss, MaskedCELoss, CELoss, MaskedMSELoss, MaskedDiceCELoss\n",
    "\n",
    "\n",
    "\n",
    "threshold = 0.9\n",
    "\n",
    "m = nn.Softmax(dim=1)\n",
    "output = torch.randn(12, 2, 128,128, 32, requires_grad=True)\n",
    "proba = m(output)\n",
    "target = torch.empty(12, 2, 128,128, 32).random_(2).float()\n",
    "\n",
    "mask=  (proba>threshold).long() + (proba<1-threshold).long() \n",
    "\n",
    "\n",
    "#CE\n",
    "\n",
    "CE_homemade = CELoss(to_onehot_y=False,softmax=True,include_background=False,batch=True, reduction='mean')\n",
    "CE_masked_homemade = MaskedCELoss(to_onehot_y=False,softmax=True,include_background=False,batch=True, reduction='mean')\n",
    "\n",
    "loss_CE = nn.CrossEntropyLoss()\n",
    "ce = -(torch.sum(((torch.log(proba)*target) + (torch.log(1-proba)*(1-target)))*mask))/mask[:,:1].sum()/2\n",
    "loss_dice_ce = monai.losses.DiceCELoss(to_onehot_y=False,softmax=True,include_background=False,batch=True, lambda_dice=0.0)\n",
    "\n",
    "loss_Ce_torch = nn.CrossEntropyLoss(reduction= 'none')\n",
    "\n",
    "print('CE full: ', loss_dice_ce(output,target), loss_CE(output, target), CE_homemade(output,target), torch.mean(loss_Ce_torch(output,target)))\n",
    "print('CE masked: ', ce, torch.sum(loss_Ce_torch(output,target)*mask[:,0])/torch.sum(mask[:,0]), CE_masked_homemade(output, target, mask[:,:1]))\n",
    "\n",
    "#Dice\n",
    "loss = monai.losses.MaskedDiceLoss(to_onehot_y=False,softmax=True,include_background=False,batch=True)\n",
    "loss_dice= monai.losses.DiceLoss(to_onehot_y=False,softmax=True,include_background=False,batch=True)\n",
    "dice = loss(output, target, mask=mask[:,:1])\n",
    "print('Dice: ', dice, loss_dice(output*mask[:,:1], target*mask[:,:1]),loss_dice(output, target))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#MSE\n",
    "loss_MSE = nn.MSELoss(reduction='none')\n",
    "mse_sol = torch.sum(loss_MSE(proba,target)[:,1]*mask[:,0])/torch.sum(mask[:,0])\n",
    "\n",
    "mse = (((proba-target)**2)*mask).sum()/mask.sum()\n",
    "mse_torch = torch.mean(loss_MSE(proba, target))\n",
    "test_mse = ((proba*mask[:,:1]-target*mask[:,:1])**2).mean()\n",
    "\n",
    "\n",
    "MSE_homemade = MSELoss(to_onehot_y=False,softmax=True,include_background=False,batch=True, reduction='mean')\n",
    "MSE_masked_homemade = MaskedMSELoss(to_onehot_y=False,softmax=True,include_background=False,batch=True, reduction='mean')\n",
    "\n",
    "\n",
    "\n",
    "print('MSE: ', MSE_homemade(output, target), mse_torch)\n",
    "print('MSE masked:', mse, mse_sol, MSE_masked_homemade(output, target, mask[:,:1]))\n",
    "\n",
    "\n",
    "\n",
    "#Dice CE\n",
    "\n",
    "dice_ce_sol = dice + torch.sum(loss_Ce_torch(output,target)*mask[:,0])/torch.sum(mask[:,0])\n",
    "\n",
    "\n",
    "dice_ce_sol = dice + torch.sum(loss_Ce_torch(output,target)*mask[:,0])/torch.sum(mask[:,0])\n",
    "print('DiceCE masked: ', dice+ce, dice_ce_sol, loss_dice_ce(output*mask[:,:1], target*mask[:,:1]))\n",
    "\n",
    "\n",
    "print(CE_masked_homemade(output, target, mask[:,:1]) + loss(output, target, mask=mask[:,:1]))\n",
    "\n",
    "\n",
    "dice_ce_masked_loss_homemade = MaskedDiceCELoss(to_onehot_y=False,softmax=True,include_background=False,batch=True, reduction='mean') \n",
    "print(dice_ce_masked_loss_homemade(output, target, mask[:,:1])/2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8919, 0.8922, 0.8936, 0.8932, 0.8919, 0.8938, 0.8927, 0.8922, 0.8934,\n",
       "        0.8928, 0.8913, 0.8929], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[4.5484e-01, 7.2902e-02, 1.6939e-01,  ..., 1.3408e-01,\n",
       "           6.0902e-02, 7.9730e-02],\n",
       "          [5.2162e-01, 1.7572e-01, 3.6363e-01,  ..., 7.2710e-02,\n",
       "           2.6667e-01, 4.3206e-02],\n",
       "          [6.1186e-01, 8.6304e-02, 3.4057e-01,  ..., 1.6040e-03,\n",
       "           1.8240e-01, 1.8206e-02],\n",
       "          ...,\n",
       "          [9.7656e-01, 6.8346e-01, 9.0309e-01,  ..., 1.0698e-01,\n",
       "           4.1379e-02, 2.6800e-01],\n",
       "          [2.2726e-02, 1.2626e-01, 4.1470e-02,  ..., 6.6933e-01,\n",
       "           7.5827e-01, 7.9447e-01],\n",
       "          [5.7774e-01, 6.9398e-02, 2.1112e-01,  ..., 2.0729e-01,\n",
       "           2.2225e-01, 5.9088e-01]],\n",
       "\n",
       "         [[8.9230e-01, 5.5419e-01, 5.6956e-01,  ..., 2.1151e-02,\n",
       "           7.2820e-01, 3.1483e-02],\n",
       "          [4.3786e-01, 1.4126e-02, 1.9246e-02,  ..., 4.6269e-01,\n",
       "           2.8028e-01, 5.0885e-01],\n",
       "          [1.2577e-01, 5.3444e-01, 5.6999e-01,  ..., 9.5054e-02,\n",
       "           1.4517e-01, 1.9141e-01],\n",
       "          ...,\n",
       "          [4.1527e-01, 5.7232e-01, 7.0103e-02,  ..., 7.3046e-01,\n",
       "           5.9347e-02, 7.0097e-01],\n",
       "          [8.6849e-01, 8.3395e-01, 6.9391e-01,  ..., 4.6128e-01,\n",
       "           2.5413e-02, 6.9218e-02],\n",
       "          [7.8987e-01, 5.6346e-02, 1.6556e-01,  ..., 3.9440e-01,\n",
       "           6.8580e-01, 3.1124e-03]],\n",
       "\n",
       "         [[7.9410e-02, 3.4970e-01, 2.1034e-01,  ..., 4.6382e-01,\n",
       "           6.0885e-01, 6.3475e-01],\n",
       "          [1.6185e-01, 2.0432e-01, 3.6084e-01,  ..., 3.0040e-01,\n",
       "           4.1498e-01, 1.0548e-03],\n",
       "          [2.2209e-01, 7.7140e-03, 2.8672e-01,  ..., 6.1103e-01,\n",
       "           8.5863e-02, 3.8769e-01],\n",
       "          ...,\n",
       "          [1.2092e-02, 6.5185e-01, 2.0116e-02,  ..., 6.4880e-01,\n",
       "           7.6253e-02, 1.7683e-01],\n",
       "          [1.6913e-01, 1.2493e-02, 7.4263e-01,  ..., 4.8863e-02,\n",
       "           2.0976e-01, 1.1359e-01],\n",
       "          [9.3575e-01, 4.0801e-01, 5.3515e-02,  ..., 2.8673e-01,\n",
       "           3.0879e-01, 3.9696e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[4.2089e-02, 2.6542e-01, 2.0113e-01,  ..., 7.2037e-01,\n",
       "           1.5456e-01, 5.2007e-02],\n",
       "          [4.1235e-01, 7.1331e-01, 9.6627e-02,  ..., 8.7522e-01,\n",
       "           8.3113e-02, 2.4317e-02],\n",
       "          [1.5658e-01, 4.9296e-03, 2.7136e-02,  ..., 5.8302e-01,\n",
       "           5.9739e-01, 3.5262e-01],\n",
       "          ...,\n",
       "          [3.8748e-01, 8.9560e-01, 9.3078e-01,  ..., 3.4005e-01,\n",
       "           4.0978e-01, 3.6732e-01],\n",
       "          [1.1736e-01, 2.2518e-03, 3.6718e-01,  ..., 5.5236e-01,\n",
       "           3.6301e-02, 6.3887e-01],\n",
       "          [3.1040e-02, 8.9678e-01, 6.3735e-01,  ..., 9.1342e-01,\n",
       "           5.2516e-01, 7.1661e-01]],\n",
       "\n",
       "         [[8.3044e-01, 4.6402e-03, 8.0949e-01,  ..., 7.9869e-01,\n",
       "           4.6317e-01, 3.8542e-01],\n",
       "          [1.1852e-03, 1.1362e-01, 5.9293e-01,  ..., 9.2595e-02,\n",
       "           4.8904e-02, 7.6272e-02],\n",
       "          [5.9262e-02, 4.0224e-01, 2.1442e-01,  ..., 1.3183e-01,\n",
       "           3.4335e-01, 7.1355e-01],\n",
       "          ...,\n",
       "          [3.9282e-01, 5.1559e-01, 6.6962e-02,  ..., 4.0670e-01,\n",
       "           6.4708e-02, 7.0089e-02],\n",
       "          [8.8883e-02, 1.5320e-01, 3.5485e-01,  ..., 2.3995e-01,\n",
       "           1.1593e-01, 4.2215e-01],\n",
       "          [6.3351e-01, 5.3800e-01, 2.2016e-01,  ..., 1.2229e-02,\n",
       "           2.4912e-01, 4.7272e-01]],\n",
       "\n",
       "         [[4.0981e-02, 8.1369e-02, 3.0671e-02,  ..., 5.0864e-01,\n",
       "           3.3193e-01, 4.2836e-02],\n",
       "          [2.7596e-01, 1.0252e-01, 1.7280e-01,  ..., 3.8830e-02,\n",
       "           5.8613e-03, 6.5143e-02],\n",
       "          [3.0931e-01, 6.1861e-01, 3.4462e-01,  ..., 1.6234e-01,\n",
       "           5.8816e-01, 2.1996e-02],\n",
       "          ...,\n",
       "          [6.3313e-04, 6.9380e-02, 8.6069e-02,  ..., 2.5421e-01,\n",
       "           6.5191e-01, 1.3433e-01],\n",
       "          [6.2408e-02, 2.9267e-01, 3.4229e-01,  ..., 2.3178e-03,\n",
       "           2.9121e-01, 7.6728e-01],\n",
       "          [3.9498e-01, 3.0973e-01, 4.2792e-01,  ..., 6.4349e-01,\n",
       "           6.7194e-01, 1.8325e-01]]],\n",
       "\n",
       "\n",
       "        [[[2.4566e-01, 3.4602e-02, 9.3179e-01,  ..., 3.4681e-01,\n",
       "           2.8064e-01, 3.2695e-02],\n",
       "          [3.4516e-01, 5.4609e-01, 3.8808e-02,  ..., 7.1248e-01,\n",
       "           3.5745e-01, 2.6539e-01],\n",
       "          [3.9211e-03, 1.0852e-02, 7.1135e-01,  ..., 7.7430e-01,\n",
       "           4.8369e-01, 9.9969e-03],\n",
       "          ...,\n",
       "          [4.7415e-03, 8.7601e-02, 6.2572e-01,  ..., 6.6606e-03,\n",
       "           6.0513e-01, 5.6812e-01],\n",
       "          [3.7594e-01, 7.5024e-01, 4.5790e-01,  ..., 9.3233e-02,\n",
       "           5.4645e-01, 2.0115e-01],\n",
       "          [4.2651e-01, 4.0049e-01, 1.6791e-01,  ..., 8.1026e-02,\n",
       "           2.8522e-01, 5.7837e-01]],\n",
       "\n",
       "         [[1.6211e-01, 5.5909e-01, 2.8996e-01,  ..., 6.3716e-02,\n",
       "           8.7878e-02, 4.5865e-01],\n",
       "          [5.1445e-01, 7.3331e-01, 3.2348e-01,  ..., 3.0677e-01,\n",
       "           4.9874e-02, 1.9163e-01],\n",
       "          [1.2505e-02, 6.3853e-01, 4.4681e-01,  ..., 1.5541e-01,\n",
       "           5.7185e-01, 6.5454e-01],\n",
       "          ...,\n",
       "          [3.5752e-01, 2.0498e-01, 1.6206e-01,  ..., 3.4763e-01,\n",
       "           9.0622e-01, 1.5430e-01],\n",
       "          [7.6776e-01, 2.6013e-02, 1.5414e-01,  ..., 7.0585e-03,\n",
       "           1.8365e-01, 7.3272e-02],\n",
       "          [1.1015e-01, 3.0994e-03, 2.2343e-01,  ..., 1.2648e-01,\n",
       "           3.5806e-01, 8.9166e-02]],\n",
       "\n",
       "         [[8.2271e-01, 6.0895e-01, 8.6275e-01,  ..., 4.3313e-01,\n",
       "           2.7345e-01, 6.7923e-01],\n",
       "          [1.9377e-01, 1.8148e-01, 6.6734e-01,  ..., 4.8272e-02,\n",
       "           5.0168e-01, 7.8973e-01],\n",
       "          [4.3530e-02, 2.3240e-01, 2.8733e-02,  ..., 1.2322e-01,\n",
       "           3.4891e-02, 5.8623e-01],\n",
       "          ...,\n",
       "          [2.2936e-02, 8.9042e-01, 2.1701e-01,  ..., 2.8281e-01,\n",
       "           1.3829e-02, 3.2237e-01],\n",
       "          [8.3007e-01, 2.4971e-01, 5.0396e-02,  ..., 3.0229e-01,\n",
       "           1.5458e-02, 1.1405e-02],\n",
       "          [3.8215e-01, 5.5653e-01, 5.0394e-01,  ..., 1.6075e-02,\n",
       "           8.5537e-01, 2.3164e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.2218e-01, 4.0899e-01, 6.2990e-02,  ..., 6.2794e-01,\n",
       "           6.7955e-02, 4.3102e-01],\n",
       "          [7.6807e-02, 2.1961e-01, 5.2811e-02,  ..., 6.8291e-01,\n",
       "           4.9370e-02, 8.7472e-02],\n",
       "          [3.9509e-01, 1.2278e-01, 2.4568e-02,  ..., 1.6466e-01,\n",
       "           7.7259e-02, 4.1938e-01],\n",
       "          ...,\n",
       "          [3.2255e-02, 5.9792e-01, 4.1749e-03,  ..., 6.1482e-01,\n",
       "           6.2392e-01, 3.9622e-02],\n",
       "          [6.2753e-01, 5.3975e-01, 4.6433e-01,  ..., 8.6346e-01,\n",
       "           9.9873e-03, 7.9310e-01],\n",
       "          [1.1973e-02, 9.4585e-01, 4.2974e-02,  ..., 4.3389e-01,\n",
       "           2.5769e-01, 7.6084e-01]],\n",
       "\n",
       "         [[3.0175e-01, 1.4026e-01, 2.2614e-01,  ..., 1.0861e-01,\n",
       "           8.7834e-01, 7.1350e-01],\n",
       "          [9.2827e-01, 5.6060e-02, 7.3990e-03,  ..., 4.1414e-01,\n",
       "           5.2744e-01, 5.0421e-01],\n",
       "          [3.7029e-01, 3.5804e-01, 4.0428e-02,  ..., 8.4504e-02,\n",
       "           6.8643e-01, 8.2255e-01],\n",
       "          ...,\n",
       "          [2.5028e-01, 8.0004e-03, 1.7318e-01,  ..., 4.6129e-01,\n",
       "           6.0763e-01, 8.0029e-02],\n",
       "          [2.2157e-02, 2.1738e-01, 2.0544e-01,  ..., 2.7143e-01,\n",
       "           1.6605e-02, 3.3053e-01],\n",
       "          [1.1360e-01, 2.6606e-02, 8.2496e-02,  ..., 8.6044e-03,\n",
       "           5.7974e-01, 5.8915e-01]],\n",
       "\n",
       "         [[3.1695e-01, 8.9081e-02, 7.4646e-02,  ..., 9.0413e-01,\n",
       "           2.2832e-01, 3.6952e-01],\n",
       "          [2.7070e-01, 5.5760e-01, 2.0966e-02,  ..., 1.0320e-01,\n",
       "           4.7946e-01, 6.7359e-01],\n",
       "          [4.4101e-01, 7.1075e-01, 5.9212e-01,  ..., 4.3376e-01,\n",
       "           1.1718e-01, 9.7065e-02],\n",
       "          ...,\n",
       "          [2.0651e-01, 8.0693e-01, 7.5080e-01,  ..., 1.3969e-01,\n",
       "           1.7364e-01, 7.0035e-01],\n",
       "          [1.5543e-01, 5.3086e-01, 6.4601e-01,  ..., 1.0052e-01,\n",
       "           9.8780e-02, 1.4305e-01],\n",
       "          [1.2374e-01, 4.1998e-03, 1.5449e-01,  ..., 1.8539e-01,\n",
       "           1.3411e-02, 6.9068e-01]]],\n",
       "\n",
       "\n",
       "        [[[1.3225e-01, 7.2695e-01, 9.0339e-02,  ..., 5.5240e-02,\n",
       "           4.5460e-01, 8.4026e-01],\n",
       "          [2.2801e-01, 7.0297e-03, 1.5711e-01,  ..., 1.9852e-01,\n",
       "           5.4670e-02, 2.8819e-02],\n",
       "          [7.3796e-01, 6.2378e-01, 4.3126e-01,  ..., 2.6251e-01,\n",
       "           4.1012e-01, 1.0013e-01],\n",
       "          ...,\n",
       "          [5.8797e-02, 6.1922e-01, 2.4510e-01,  ..., 8.4138e-01,\n",
       "           8.4107e-01, 4.0401e-02],\n",
       "          [1.9088e-01, 4.4280e-02, 1.7064e-02,  ..., 3.0835e-01,\n",
       "           2.6668e-01, 5.9526e-02],\n",
       "          [3.3204e-02, 3.1042e-01, 2.1179e-01,  ..., 1.7502e-01,\n",
       "           5.4531e-01, 1.9879e-01]],\n",
       "\n",
       "         [[3.0704e-02, 1.7598e-02, 4.9295e-01,  ..., 7.3282e-02,\n",
       "           3.2170e-03, 1.9881e-01],\n",
       "          [5.0189e-01, 6.2753e-01, 8.0992e-01,  ..., 3.5095e-01,\n",
       "           7.6076e-01, 2.0280e-01],\n",
       "          [5.3300e-01, 5.0126e-01, 4.6721e-01,  ..., 3.8143e-01,\n",
       "           3.8067e-01, 7.3810e-02],\n",
       "          ...,\n",
       "          [2.9772e-01, 7.8507e-01, 5.8188e-01,  ..., 1.9596e-01,\n",
       "           3.0148e-01, 1.8671e-01],\n",
       "          [1.2132e-01, 8.0756e-02, 8.1774e-01,  ..., 3.1130e-01,\n",
       "           2.3878e-01, 2.5018e-01],\n",
       "          [2.2602e-01, 3.9723e-01, 1.9823e-01,  ..., 1.1339e-02,\n",
       "           7.4070e-01, 3.0525e-01]],\n",
       "\n",
       "         [[2.5049e-01, 1.7817e-01, 2.6151e-02,  ..., 3.0535e-01,\n",
       "           5.3005e-01, 2.3662e-01],\n",
       "          [3.8446e-01, 4.1408e-01, 3.7273e-01,  ..., 1.0764e-01,\n",
       "           5.3435e-02, 1.3427e-01],\n",
       "          [6.0982e-01, 3.8826e-01, 1.8402e-01,  ..., 3.5233e-01,\n",
       "           1.6072e-01, 1.2265e-01],\n",
       "          ...,\n",
       "          [6.0245e-01, 8.0286e-01, 6.9764e-01,  ..., 3.9296e-01,\n",
       "           6.7421e-01, 3.6526e-01],\n",
       "          [3.0740e-02, 3.8328e-01, 4.9549e-02,  ..., 6.5309e-03,\n",
       "           4.5161e-01, 5.4527e-01],\n",
       "          [1.6802e-01, 7.1829e-02, 5.6593e-01,  ..., 2.3739e-01,\n",
       "           7.0185e-01, 1.9840e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.4751e-01, 1.4073e-01, 4.1274e-01,  ..., 4.1364e-01,\n",
       "           3.8194e-01, 2.4935e-01],\n",
       "          [7.0144e-01, 1.0348e-03, 5.3505e-01,  ..., 5.7967e-01,\n",
       "           6.1326e-01, 1.0396e-01],\n",
       "          [3.5129e-02, 1.4721e-01, 5.3160e-01,  ..., 3.6855e-01,\n",
       "           3.9841e-01, 2.4817e-01],\n",
       "          ...,\n",
       "          [4.2607e-02, 2.7540e-01, 1.0131e-01,  ..., 1.4412e-01,\n",
       "           5.4033e-01, 1.4828e-01],\n",
       "          [7.0649e-01, 5.4263e-01, 3.7481e-01,  ..., 4.9305e-01,\n",
       "           3.6495e-01, 4.6823e-02],\n",
       "          [2.0799e-01, 8.6369e-01, 6.5033e-01,  ..., 2.3021e-03,\n",
       "           5.9346e-01, 4.0757e-01]],\n",
       "\n",
       "         [[5.4205e-03, 1.4151e-01, 5.1362e-02,  ..., 1.9725e-03,\n",
       "           3.0290e-03, 7.6623e-01],\n",
       "          [9.4229e-02, 2.2035e-01, 2.7581e-01,  ..., 3.8761e-03,\n",
       "           6.3436e-01, 4.1600e-01],\n",
       "          [1.5925e-01, 5.4047e-02, 9.8689e-02,  ..., 4.7298e-01,\n",
       "           4.6737e-02, 3.3966e-01],\n",
       "          ...,\n",
       "          [2.5027e-01, 2.1405e-01, 2.0633e-01,  ..., 1.5117e-02,\n",
       "           5.6752e-01, 4.7278e-01],\n",
       "          [6.6898e-01, 6.6250e-01, 4.2788e-01,  ..., 1.5898e-01,\n",
       "           2.1549e-01, 8.5479e-01],\n",
       "          [3.6220e-02, 5.7095e-01, 3.7402e-01,  ..., 3.7703e-01,\n",
       "           2.5877e-01, 4.3386e-02]],\n",
       "\n",
       "         [[2.7014e-01, 1.2929e-01, 1.2158e-01,  ..., 6.9763e-02,\n",
       "           6.3328e-01, 5.6762e-01],\n",
       "          [1.8831e-01, 1.6947e-01, 4.3384e-02,  ..., 4.8022e-01,\n",
       "           2.0053e-01, 8.1945e-01],\n",
       "          [6.8341e-01, 4.4339e-01, 3.2254e-01,  ..., 1.4158e-02,\n",
       "           8.9561e-01, 3.4410e-01],\n",
       "          ...,\n",
       "          [6.3319e-01, 1.6556e-01, 5.0425e-01,  ..., 6.3059e-01,\n",
       "           9.8522e-02, 2.8878e-01],\n",
       "          [1.0645e-01, 5.5280e-02, 5.6036e-01,  ..., 8.0020e-01,\n",
       "           3.1936e-01, 4.8638e-01],\n",
       "          [1.2040e-02, 5.3643e-02, 3.5012e-01,  ..., 3.5751e-01,\n",
       "           2.1841e-01, 1.4112e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[7.0264e-01, 4.6198e-01, 2.8601e-01,  ..., 9.3954e-02,\n",
       "           7.9541e-02, 2.7552e-02],\n",
       "          [9.2014e-01, 8.8181e-03, 1.5720e-01,  ..., 1.7939e-01,\n",
       "           1.7191e-01, 8.6748e-02],\n",
       "          [6.1761e-03, 7.4662e-02, 4.3752e-01,  ..., 1.9922e-01,\n",
       "           4.0034e-01, 4.0377e-01],\n",
       "          ...,\n",
       "          [4.9161e-02, 1.1133e-01, 7.5274e-01,  ..., 6.5214e-01,\n",
       "           6.4438e-03, 7.5468e-01],\n",
       "          [7.4037e-01, 1.5566e-02, 6.8517e-01,  ..., 5.1027e-01,\n",
       "           9.2353e-01, 2.7296e-02],\n",
       "          [8.6352e-01, 3.1780e-01, 9.6829e-01,  ..., 1.7368e-01,\n",
       "           3.6483e-01, 4.8533e-01]],\n",
       "\n",
       "         [[6.7072e-02, 3.4584e-01, 5.8944e-01,  ..., 4.2593e-01,\n",
       "           4.6787e-01, 5.0473e-01],\n",
       "          [9.5320e-01, 1.6004e-01, 2.0421e-01,  ..., 1.0444e-01,\n",
       "           7.4103e-01, 2.8710e-01],\n",
       "          [1.9494e-01, 3.5943e-01, 2.8139e-02,  ..., 7.0360e-02,\n",
       "           1.1028e-02, 5.9949e-02],\n",
       "          ...,\n",
       "          [4.7658e-01, 2.1991e-01, 1.7323e-01,  ..., 6.5144e-01,\n",
       "           2.0391e-02, 4.3178e-02],\n",
       "          [5.7824e-01, 6.9779e-01, 1.2356e-01,  ..., 2.6102e-01,\n",
       "           3.2497e-01, 3.0632e-02],\n",
       "          [2.3092e-02, 1.4307e-03, 2.8021e-02,  ..., 1.1730e-01,\n",
       "           1.2766e-01, 6.3326e-02]],\n",
       "\n",
       "         [[5.2083e-03, 7.7101e-02, 5.1975e-01,  ..., 1.9345e-02,\n",
       "           5.1907e-01, 9.2743e-02],\n",
       "          [5.1821e-02, 8.7208e-01, 5.6432e-01,  ..., 1.4432e-01,\n",
       "           8.3314e-02, 2.9318e-02],\n",
       "          [6.2654e-02, 1.2141e-01, 2.3123e-02,  ..., 4.0303e-01,\n",
       "           1.4051e-01, 4.3842e-01],\n",
       "          ...,\n",
       "          [5.3692e-02, 8.0552e-01, 4.3066e-02,  ..., 7.6776e-01,\n",
       "           6.9648e-02, 6.9760e-01],\n",
       "          [1.1348e-01, 9.5824e-01, 2.2101e-01,  ..., 5.7558e-01,\n",
       "           1.1133e-02, 1.9387e-01],\n",
       "          [7.2970e-01, 6.9196e-01, 7.7691e-01,  ..., 2.4394e-01,\n",
       "           2.4235e-01, 1.7209e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.3871e-01, 6.5485e-01, 8.8285e-01,  ..., 6.0626e-02,\n",
       "           4.4738e-03, 4.9813e-02],\n",
       "          [8.6300e-02, 3.1585e-02, 1.3744e-01,  ..., 7.7248e-01,\n",
       "           1.3598e-02, 6.0261e-01],\n",
       "          [4.0260e-01, 1.1990e-01, 1.3057e-01,  ..., 9.0153e-01,\n",
       "           4.7972e-01, 7.4435e-01],\n",
       "          ...,\n",
       "          [5.8724e-01, 9.5559e-02, 3.2031e-02,  ..., 1.0456e-02,\n",
       "           7.4188e-01, 9.3455e-02],\n",
       "          [6.7671e-01, 6.8662e-02, 1.8087e-01,  ..., 1.1881e-01,\n",
       "           1.2198e-01, 1.9736e-01],\n",
       "          [3.4705e-01, 2.0413e-01, 9.1512e-01,  ..., 2.6488e-01,\n",
       "           1.5224e-01, 6.8074e-01]],\n",
       "\n",
       "         [[8.4465e-02, 2.6789e-02, 4.5384e-01,  ..., 5.5324e-01,\n",
       "           3.7314e-02, 3.3804e-01],\n",
       "          [1.4104e-01, 1.2734e-02, 7.1668e-02,  ..., 5.6947e-01,\n",
       "           1.0394e-02, 7.4600e-01],\n",
       "          [2.1527e-01, 3.8142e-01, 7.0954e-02,  ..., 1.9394e-01,\n",
       "           1.8989e-01, 2.4124e-02],\n",
       "          ...,\n",
       "          [2.8607e-02, 1.1536e-02, 7.7710e-01,  ..., 7.1955e-01,\n",
       "           2.2077e-01, 1.1932e-01],\n",
       "          [8.8510e-02, 2.2042e-03, 8.2826e-01,  ..., 4.0167e-01,\n",
       "           1.8455e-01, 9.2960e-02],\n",
       "          [5.7502e-01, 6.6523e-01, 6.5253e-01,  ..., 3.5788e-01,\n",
       "           2.7521e-01, 1.4043e-02]],\n",
       "\n",
       "         [[3.4875e-01, 4.8126e-01, 1.1016e-02,  ..., 4.1054e-01,\n",
       "           2.3862e-01, 5.2115e-01],\n",
       "          [1.6040e-01, 7.3268e-02, 3.3898e-02,  ..., 8.5472e-01,\n",
       "           5.0483e-02, 2.3740e-03],\n",
       "          [7.5587e-02, 1.1194e-01, 9.4734e-02,  ..., 1.8239e-01,\n",
       "           1.8360e-01, 3.0657e-01],\n",
       "          ...,\n",
       "          [3.3273e-02, 4.7560e-01, 1.1760e-01,  ..., 4.7875e-01,\n",
       "           2.1916e-01, 3.9296e-02],\n",
       "          [8.5463e-02, 6.1173e-01, 3.4699e-01,  ..., 4.3032e-01,\n",
       "           1.0972e-02, 2.2092e-02],\n",
       "          [4.6728e-01, 5.5119e-01, 1.0665e-02,  ..., 3.1330e-01,\n",
       "           5.8759e-02, 8.3980e-01]]],\n",
       "\n",
       "\n",
       "        [[[1.9999e-01, 2.2107e-01, 1.8335e-01,  ..., 1.9935e-01,\n",
       "           6.9167e-03, 9.5339e-02],\n",
       "          [4.8297e-01, 1.5714e-01, 1.8253e-01,  ..., 2.3055e-01,\n",
       "           1.1330e-02, 6.7302e-01],\n",
       "          [7.5203e-02, 8.8473e-02, 2.6920e-01,  ..., 6.4569e-01,\n",
       "           4.4704e-01, 5.7253e-02],\n",
       "          ...,\n",
       "          [3.5834e-01, 4.3537e-01, 2.9950e-02,  ..., 8.0360e-01,\n",
       "           7.5409e-01, 5.4748e-01],\n",
       "          [4.7919e-02, 6.6093e-01, 2.2423e-01,  ..., 8.2325e-02,\n",
       "           6.7870e-02, 4.5716e-01],\n",
       "          [3.8587e-01, 3.6208e-05, 8.9757e-01,  ..., 7.1888e-01,\n",
       "           5.3997e-01, 2.0391e-03]],\n",
       "\n",
       "         [[2.7610e-01, 4.9832e-02, 7.9483e-02,  ..., 1.2004e-01,\n",
       "           6.1383e-01, 5.4366e-01],\n",
       "          [4.1866e-01, 8.1519e-02, 5.6307e-01,  ..., 8.4878e-02,\n",
       "           2.8633e-01, 5.6284e-01],\n",
       "          [4.9689e-02, 4.2727e-02, 7.4868e-01,  ..., 7.2677e-03,\n",
       "           5.3699e-01, 9.7656e-03],\n",
       "          ...,\n",
       "          [4.1576e-02, 4.8268e-01, 4.1849e-01,  ..., 3.8846e-01,\n",
       "           2.4139e-01, 2.6482e-01],\n",
       "          [9.5761e-02, 7.8790e-03, 6.0929e-01,  ..., 2.2872e-01,\n",
       "           6.5949e-01, 3.5101e-02],\n",
       "          [3.5723e-01, 6.7289e-01, 1.4993e-01,  ..., 2.3110e-01,\n",
       "           2.8245e-02, 2.1791e-01]],\n",
       "\n",
       "         [[2.0196e-02, 2.6987e-01, 1.6983e-01,  ..., 3.0668e-01,\n",
       "           1.2097e-02, 7.8095e-01],\n",
       "          [4.5703e-01, 6.0776e-01, 7.4936e-01,  ..., 8.3971e-01,\n",
       "           2.6852e-01, 3.5205e-02],\n",
       "          [1.1298e-02, 1.5861e-03, 4.8066e-02,  ..., 1.6537e-01,\n",
       "           6.2139e-01, 2.9814e-02],\n",
       "          ...,\n",
       "          [3.9379e-01, 2.5328e-02, 1.8788e-01,  ..., 5.2901e-01,\n",
       "           1.8446e-01, 4.0482e-04],\n",
       "          [7.2720e-01, 6.3321e-02, 5.9084e-01,  ..., 4.3334e-01,\n",
       "           3.3386e-03, 6.9809e-01],\n",
       "          [1.0654e-01, 5.3935e-01, 8.0085e-01,  ..., 2.6348e-01,\n",
       "           8.4158e-01, 1.2258e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[7.3592e-01, 6.1087e-02, 5.7425e-01,  ..., 1.9059e-01,\n",
       "           4.1814e-02, 2.3361e-02],\n",
       "          [1.6817e-02, 1.8999e-01, 1.4748e-01,  ..., 9.0480e-01,\n",
       "           7.3211e-01, 5.6585e-01],\n",
       "          [3.5821e-01, 5.3420e-01, 3.9816e-01,  ..., 7.0970e-01,\n",
       "           5.9501e-01, 7.0479e-02],\n",
       "          ...,\n",
       "          [2.3043e-01, 7.6287e-02, 1.1570e-01,  ..., 4.2420e-01,\n",
       "           1.3324e-01, 2.5098e-01],\n",
       "          [2.8037e-02, 1.3214e-02, 2.9988e-01,  ..., 9.3237e-01,\n",
       "           9.2349e-01, 1.5982e-01],\n",
       "          [6.5305e-03, 6.7483e-01, 2.7158e-01,  ..., 8.5957e-01,\n",
       "           2.0941e-02, 3.8325e-02]],\n",
       "\n",
       "         [[4.0698e-02, 2.8269e-01, 2.9752e-01,  ..., 4.1162e-01,\n",
       "           3.5275e-02, 7.7304e-01],\n",
       "          [1.5040e-01, 1.8657e-02, 2.5405e-01,  ..., 9.3983e-02,\n",
       "           2.1755e-03, 1.3896e-01],\n",
       "          [1.2533e-01, 2.3820e-01, 9.4674e-02,  ..., 3.1420e-01,\n",
       "           8.8798e-01, 4.0520e-01],\n",
       "          ...,\n",
       "          [2.4604e-01, 6.3446e-02, 5.1454e-02,  ..., 2.2493e-01,\n",
       "           1.4817e-01, 2.1417e-01],\n",
       "          [1.1231e-01, 7.0972e-01, 2.3452e-01,  ..., 2.8051e-01,\n",
       "           6.0745e-02, 1.4477e-02],\n",
       "          [1.1682e-01, 3.9073e-01, 7.0402e-01,  ..., 3.1306e-01,\n",
       "           2.3872e-01, 8.5665e-02]],\n",
       "\n",
       "         [[7.5248e-01, 3.9664e-01, 2.8494e-01,  ..., 8.8878e-01,\n",
       "           2.7872e-01, 2.8775e-01],\n",
       "          [3.1296e-01, 1.0956e-01, 2.3032e-01,  ..., 5.3575e-01,\n",
       "           2.8225e-01, 2.3346e-01],\n",
       "          [8.6421e-01, 1.2070e-01, 4.9118e-01,  ..., 4.1599e-01,\n",
       "           9.5000e-02, 7.9163e-01],\n",
       "          ...,\n",
       "          [3.0867e-01, 4.5798e-02, 6.7423e-02,  ..., 5.7554e-02,\n",
       "           1.3650e-01, 9.4124e-01],\n",
       "          [4.9132e-02, 5.7208e-01, 1.1483e-01,  ..., 3.1750e-01,\n",
       "           4.7018e-01, 8.3238e-01],\n",
       "          [1.0372e-01, 4.3475e-01, 2.4547e-01,  ..., 5.5498e-02,\n",
       "           7.7502e-01, 1.5298e-01]]],\n",
       "\n",
       "\n",
       "        [[[8.2467e-01, 2.0219e-03, 3.4108e-01,  ..., 2.6194e-03,\n",
       "           6.1014e-02, 3.7593e-01],\n",
       "          [5.5038e-01, 2.2409e-01, 5.5399e-02,  ..., 2.6680e-01,\n",
       "           2.2586e-01, 3.6048e-01],\n",
       "          [7.1978e-02, 4.8057e-01, 3.2278e-02,  ..., 2.7967e-03,\n",
       "           1.0036e-01, 7.2942e-01],\n",
       "          ...,\n",
       "          [3.6003e-01, 6.4918e-01, 3.9923e-01,  ..., 8.6115e-01,\n",
       "           3.3904e-01, 3.7846e-01],\n",
       "          [1.1387e-01, 7.4419e-02, 9.5049e-01,  ..., 6.2415e-01,\n",
       "           3.7260e-01, 4.7299e-01],\n",
       "          [8.2013e-03, 4.3588e-01, 3.9185e-01,  ..., 3.7854e-01,\n",
       "           6.5073e-01, 7.2892e-01]],\n",
       "\n",
       "         [[8.6347e-03, 2.8519e-01, 2.2969e-02,  ..., 3.9912e-01,\n",
       "           1.8711e-01, 9.1892e-01],\n",
       "          [3.4766e-01, 1.1589e-02, 5.1395e-01,  ..., 4.7827e-01,\n",
       "           1.9085e-01, 9.1691e-03],\n",
       "          [2.4169e-02, 2.0764e-02, 4.3077e-01,  ..., 2.2868e-01,\n",
       "           7.5316e-02, 1.5342e-01],\n",
       "          ...,\n",
       "          [8.1693e-01, 6.0904e-01, 3.1675e-02,  ..., 2.6339e-02,\n",
       "           3.0865e-01, 2.8445e-01],\n",
       "          [3.9298e-01, 3.9475e-01, 4.5916e-01,  ..., 2.0246e-02,\n",
       "           6.0633e-03, 3.3623e-01],\n",
       "          [6.1699e-01, 5.5135e-02, 2.6312e-01,  ..., 9.5660e-02,\n",
       "           5.8342e-01, 9.6026e-01]],\n",
       "\n",
       "         [[3.0367e-01, 3.9608e-02, 9.0866e-01,  ..., 5.4302e-01,\n",
       "           1.7504e-01, 7.6390e-02],\n",
       "          [9.4440e-02, 1.1065e-01, 6.5089e-03,  ..., 6.5165e-02,\n",
       "           9.6985e-02, 1.5178e-01],\n",
       "          [2.5168e-01, 2.9045e-01, 9.6358e-02,  ..., 1.0718e-01,\n",
       "           3.7763e-01, 4.3379e-03],\n",
       "          ...,\n",
       "          [6.5696e-01, 7.9769e-01, 2.8782e-01,  ..., 4.7091e-03,\n",
       "           5.9364e-01, 1.6209e-03],\n",
       "          [3.5024e-01, 6.4420e-01, 1.6731e-01,  ..., 3.9104e-02,\n",
       "           8.2396e-02, 2.6065e-01],\n",
       "          [3.0669e-02, 3.5036e-01, 5.1434e-01,  ..., 3.8699e-03,\n",
       "           6.8960e-02, 1.7480e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.7991e-01, 2.0699e-01, 4.1809e-01,  ..., 1.1156e-03,\n",
       "           2.0553e-01, 1.1087e-01],\n",
       "          [5.0868e-01, 2.8767e-01, 5.0692e-01,  ..., 4.2583e-01,\n",
       "           7.5119e-01, 2.0702e-01],\n",
       "          [1.4162e-01, 6.0299e-02, 1.9205e-01,  ..., 8.4208e-01,\n",
       "           2.0911e-02, 2.9115e-01],\n",
       "          ...,\n",
       "          [4.6382e-01, 8.6383e-03, 7.2274e-02,  ..., 1.3102e-02,\n",
       "           1.2440e-01, 6.7531e-03],\n",
       "          [1.8410e-02, 8.2208e-01, 5.9541e-01,  ..., 3.7916e-02,\n",
       "           8.5170e-03, 2.2078e-01],\n",
       "          [5.4380e-01, 6.8185e-04, 4.7934e-01,  ..., 1.2220e-01,\n",
       "           2.3848e-01, 6.3289e-01]],\n",
       "\n",
       "         [[2.0036e-01, 2.0566e-01, 1.7367e-02,  ..., 3.8110e-02,\n",
       "           2.3531e-03, 1.4621e-01],\n",
       "          [1.7608e-01, 1.1710e-01, 9.4195e-01,  ..., 8.6090e-01,\n",
       "           1.8992e-02, 1.0229e-01],\n",
       "          [8.3047e-01, 1.8577e-02, 5.9364e-01,  ..., 3.2467e-02,\n",
       "           1.2733e-01, 1.8861e-01],\n",
       "          ...,\n",
       "          [5.7123e-01, 8.6884e-01, 2.8690e-01,  ..., 7.4983e-02,\n",
       "           8.5089e-01, 1.1182e-01],\n",
       "          [5.5138e-01, 2.4275e-01, 7.0536e-01,  ..., 2.3906e-01,\n",
       "           8.6004e-01, 5.4874e-01],\n",
       "          [3.4972e-01, 1.6128e-01, 6.1052e-01,  ..., 4.8385e-01,\n",
       "           3.9060e-02, 4.1444e-01]],\n",
       "\n",
       "         [[1.8645e-01, 4.1050e-02, 1.9096e-01,  ..., 1.4774e-02,\n",
       "           5.6658e-03, 7.4066e-01],\n",
       "          [5.7094e-01, 8.9168e-03, 8.1404e-01,  ..., 1.8250e-03,\n",
       "           2.3951e-01, 8.2558e-01],\n",
       "          [6.2823e-01, 8.6610e-02, 7.4049e-01,  ..., 9.0106e-03,\n",
       "           7.2864e-01, 6.1431e-01],\n",
       "          ...,\n",
       "          [4.9277e-02, 2.8345e-02, 8.8339e-01,  ..., 3.8484e-01,\n",
       "           1.8938e-02, 1.9223e-02],\n",
       "          [1.0645e-01, 6.0961e-02, 3.0775e-01,  ..., 2.5245e-01,\n",
       "           3.6170e-01, 3.7190e-01],\n",
       "          [1.2043e-01, 2.6292e-01, 8.3935e-01,  ..., 1.9488e-01,\n",
       "           1.9989e-01, 1.7346e-02]]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_MSE(proba,target)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[4.5484e-01, 7.2902e-02, 3.4626e-01,  ..., 1.3408e-01,\n",
       "           5.6733e-01, 7.9730e-02],\n",
       "          [7.7156e-02, 3.3734e-01, 1.5759e-01,  ..., 5.3341e-01,\n",
       "           2.6667e-01, 4.3206e-02],\n",
       "          [6.1186e-01, 8.6304e-02, 3.4057e-01,  ..., 1.6040e-03,\n",
       "           1.8240e-01, 7.4835e-01],\n",
       "          ...,\n",
       "          [9.7656e-01, 6.8346e-01, 2.4691e-03,  ..., 1.0698e-01,\n",
       "           6.3454e-01, 2.6800e-01],\n",
       "          [7.2122e-01, 1.2626e-01, 6.3419e-01,  ..., 6.6933e-01,\n",
       "           7.5827e-01, 1.1809e-02],\n",
       "          [5.7555e-02, 5.4253e-01, 2.1112e-01,  ..., 2.9670e-01,\n",
       "           2.7939e-01, 5.9088e-01]],\n",
       "\n",
       "         [[8.9230e-01, 5.5419e-01, 6.0175e-02,  ..., 7.3028e-01,\n",
       "           2.1508e-02, 3.1483e-02],\n",
       "          [1.1444e-01, 7.7642e-01, 7.4179e-01,  ..., 4.6269e-01,\n",
       "           2.8028e-01, 5.0885e-01],\n",
       "          [4.1650e-01, 7.2331e-02, 6.0036e-02,  ..., 9.5054e-02,\n",
       "           3.8314e-01, 3.1640e-01],\n",
       "          ...,\n",
       "          [1.2644e-01, 5.9285e-02, 5.4056e-01,  ..., 7.3046e-01,\n",
       "           5.7212e-01, 2.6490e-02],\n",
       "          [4.6336e-03, 7.5329e-03, 6.9391e-01,  ..., 1.0293e-01,\n",
       "           2.5413e-02, 5.4303e-01],\n",
       "          [7.8987e-01, 5.8160e-01, 1.6556e-01,  ..., 3.9440e-01,\n",
       "           2.9539e-02, 3.1124e-03]],\n",
       "\n",
       "         [[5.1581e-01, 1.6699e-01, 2.1034e-01,  ..., 1.0173e-01,\n",
       "           4.8274e-02, 6.3475e-01],\n",
       "          [3.5724e-01, 3.0028e-01, 1.5944e-01,  ..., 3.0040e-01,\n",
       "           1.2660e-01, 9.3610e-01],\n",
       "          [2.7956e-01, 7.7140e-03, 2.8672e-01,  ..., 4.7662e-02,\n",
       "           4.9981e-01, 3.8769e-01],\n",
       "          ...,\n",
       "          [1.2092e-02, 6.5185e-01, 2.0116e-02,  ..., 6.4880e-01,\n",
       "           7.6253e-02, 1.7683e-01],\n",
       "          [1.6913e-01, 7.8895e-01, 7.4263e-01,  ..., 4.8863e-02,\n",
       "           2.9376e-01, 1.1359e-01],\n",
       "          [9.3575e-01, 1.3049e-01, 5.3515e-02,  ..., 2.8673e-01,\n",
       "           3.0879e-01, 3.9696e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[6.3178e-01, 2.3504e-01, 3.0417e-01,  ..., 7.2037e-01,\n",
       "           3.6828e-01, 5.2007e-02],\n",
       "          [1.2806e-01, 7.1331e-01, 4.7493e-01,  ..., 4.1558e-03,\n",
       "           5.0653e-01, 2.4317e-02],\n",
       "          [1.5658e-01, 4.9296e-03, 2.7136e-02,  ..., 5.8302e-01,\n",
       "           5.9739e-01, 3.5262e-01],\n",
       "          ...,\n",
       "          [1.4252e-01, 2.8773e-03, 9.3078e-01,  ..., 1.7377e-01,\n",
       "           1.2950e-01, 1.5518e-01],\n",
       "          [4.3221e-01, 2.2518e-03, 3.6718e-01,  ..., 6.5941e-02,\n",
       "           6.5524e-01, 6.3887e-01],\n",
       "          [3.1040e-02, 2.8106e-03, 4.0666e-02,  ..., 1.9597e-03,\n",
       "           7.5802e-02, 7.1661e-01]],\n",
       "\n",
       "         [[8.3044e-01, 4.6402e-03, 8.0949e-01,  ..., 7.9869e-01,\n",
       "           1.0204e-01, 3.8542e-01],\n",
       "          [9.3233e-01, 4.3947e-01, 5.9293e-01,  ..., 4.8401e-01,\n",
       "           6.0662e-01, 5.2392e-01],\n",
       "          [5.9262e-02, 1.3379e-01, 2.8831e-01,  ..., 4.0566e-01,\n",
       "           3.4335e-01, 7.1355e-01],\n",
       "          ...,\n",
       "          [3.9282e-01, 7.9499e-02, 6.6962e-02,  ..., 4.0670e-01,\n",
       "           5.5595e-01, 7.0089e-02],\n",
       "          [4.9262e-01, 1.5320e-01, 3.5485e-01,  ..., 2.3995e-01,\n",
       "           4.3496e-01, 4.2215e-01],\n",
       "          [6.3351e-01, 7.1030e-02, 2.2016e-01,  ..., 1.2229e-02,\n",
       "           2.5088e-01, 4.7272e-01]],\n",
       "\n",
       "         [[6.3610e-01, 5.1087e-01, 3.0671e-02,  ..., 5.0864e-01,\n",
       "           3.3193e-01, 4.2836e-02],\n",
       "          [2.7596e-01, 4.6215e-01, 3.4141e-01,  ..., 6.4472e-01,\n",
       "           8.5274e-01, 6.5143e-02],\n",
       "          [1.9700e-01, 6.1861e-01, 1.7054e-01,  ..., 3.5652e-01,\n",
       "           5.4329e-02, 2.1996e-02],\n",
       "          ...,\n",
       "          [6.3313e-04, 5.4258e-01, 4.9932e-01,  ..., 2.4582e-01,\n",
       "           6.5191e-01, 4.0130e-01],\n",
       "          [5.6278e-01, 2.9267e-01, 1.7218e-01,  ..., 9.0603e-01,\n",
       "           2.9121e-01, 7.6728e-01],\n",
       "          [1.3803e-01, 3.0973e-01, 1.1961e-01,  ..., 3.9132e-02,\n",
       "           3.2500e-02, 1.8325e-01]]],\n",
       "\n",
       "\n",
       "        [[[2.5438e-01, 6.6257e-01, 1.2047e-03,  ..., 3.4681e-01,\n",
       "           2.8064e-01, 3.2695e-02],\n",
       "          [1.7016e-01, 5.4609e-01, 6.4481e-01,  ..., 2.4309e-02,\n",
       "           3.5745e-01, 2.6539e-01],\n",
       "          [8.7868e-01, 8.0250e-01, 7.1135e-01,  ..., 7.7430e-01,\n",
       "           9.2732e-02, 9.9969e-03],\n",
       "          ...,\n",
       "          [8.6703e-01, 8.7601e-02, 4.3671e-02,  ..., 6.6606e-03,\n",
       "           6.0513e-01, 5.6812e-01],\n",
       "          [1.4966e-01, 7.5024e-01, 4.5790e-01,  ..., 4.8255e-01,\n",
       "           6.8004e-02, 3.0415e-01],\n",
       "          [4.2651e-01, 1.3481e-01, 3.4838e-01,  ..., 5.1172e-01,\n",
       "           2.1710e-01, 5.7837e-01]],\n",
       "\n",
       "         [[1.6211e-01, 6.3642e-02, 2.8996e-01,  ..., 5.5887e-01,\n",
       "           8.7878e-02, 4.5865e-01],\n",
       "          [7.9947e-02, 7.3331e-01, 3.2348e-01,  ..., 1.9903e-01,\n",
       "           6.0322e-01, 1.9163e-01],\n",
       "          [1.2505e-02, 4.0369e-02, 1.0993e-01,  ..., 3.6697e-01,\n",
       "           5.7185e-01, 6.5454e-01],\n",
       "          ...,\n",
       "          [1.6166e-01, 2.0498e-01, 1.6206e-01,  ..., 1.6842e-01,\n",
       "           9.0622e-01, 3.6868e-01],\n",
       "          [1.5321e-02, 7.0344e-01, 1.5414e-01,  ..., 7.0585e-03,\n",
       "           1.8365e-01, 7.3272e-02],\n",
       "          [1.1015e-01, 3.0994e-03, 2.2343e-01,  ..., 1.2648e-01,\n",
       "           3.5806e-01, 4.9195e-01]],\n",
       "\n",
       "         [[8.6429e-03, 4.8245e-02, 8.6275e-01,  ..., 1.1688e-01,\n",
       "           2.2760e-01, 3.0921e-02],\n",
       "          [1.9377e-01, 1.8148e-01, 3.3523e-02,  ..., 4.8272e-02,\n",
       "           8.5094e-02, 1.2394e-02],\n",
       "          [6.2625e-01, 2.6824e-01, 6.8972e-01,  ..., 1.2322e-01,\n",
       "           3.4891e-02, 5.8623e-01],\n",
       "          ...,\n",
       "          [7.2004e-01, 8.9042e-01, 2.1701e-01,  ..., 2.8281e-01,\n",
       "           1.3829e-02, 3.2237e-01],\n",
       "          [7.9069e-03, 2.4971e-01, 6.0142e-01,  ..., 2.0267e-01,\n",
       "           7.6680e-01, 7.9781e-01],\n",
       "          [1.4579e-01, 6.4512e-02, 8.4166e-02,  ..., 7.6250e-01,\n",
       "           5.6459e-03, 2.6906e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.2218e-01, 4.0899e-01, 6.2990e-02,  ..., 4.3088e-02,\n",
       "           6.7955e-02, 4.3102e-01],\n",
       "          [7.6807e-02, 2.8236e-01, 5.2811e-02,  ..., 3.0142e-02,\n",
       "           6.0498e-01, 8.7472e-02],\n",
       "          [3.9509e-01, 1.2278e-01, 2.4568e-02,  ..., 1.6466e-01,\n",
       "           7.7259e-02, 4.1938e-01],\n",
       "          ...,\n",
       "          [3.2255e-02, 5.1414e-02, 8.7495e-01,  ..., 4.6611e-02,\n",
       "           6.2392e-01, 6.4152e-01],\n",
       "          [6.2753e-01, 5.3975e-01, 1.0149e-01,  ..., 5.0093e-03,\n",
       "           8.1011e-01, 7.9310e-01],\n",
       "          [1.1973e-02, 9.4585e-01, 6.2837e-01,  ..., 4.3389e-01,\n",
       "           2.4243e-01, 7.6084e-01]],\n",
       "\n",
       "         [[2.0312e-01, 1.4026e-01, 2.7506e-01,  ..., 1.0861e-01,\n",
       "           8.7834e-01, 7.1350e-01],\n",
       "          [9.2827e-01, 5.6060e-02, 8.3536e-01,  ..., 4.1414e-01,\n",
       "           5.2744e-01, 8.4053e-02],\n",
       "          [1.5326e-01, 3.5804e-01, 6.3829e-01,  ..., 5.0311e-01,\n",
       "           2.9409e-02, 8.2255e-01],\n",
       "          ...,\n",
       "          [2.5028e-01, 8.0004e-03, 1.7318e-01,  ..., 1.0292e-01,\n",
       "           6.0763e-01, 8.0029e-02],\n",
       "          [2.2157e-02, 2.8489e-01, 2.9893e-01,  ..., 2.2945e-01,\n",
       "           1.6605e-02, 1.8069e-01],\n",
       "          [4.3951e-01, 7.0038e-01, 8.2496e-02,  ..., 8.6044e-03,\n",
       "           5.6926e-02, 5.4027e-02]],\n",
       "\n",
       "         [[1.9098e-01, 8.9081e-02, 5.2822e-01,  ..., 2.4148e-03,\n",
       "           2.2832e-01, 3.6952e-01],\n",
       "          [2.3013e-01, 5.5760e-01, 2.0966e-02,  ..., 1.0320e-01,\n",
       "           4.7946e-01, 6.7359e-01],\n",
       "          [4.4101e-01, 7.1075e-01, 5.9212e-01,  ..., 1.1655e-01,\n",
       "           4.3254e-01, 4.7396e-01],\n",
       "          ...,\n",
       "          [2.0651e-01, 1.0345e-02, 7.5080e-01,  ..., 3.9218e-01,\n",
       "           1.7364e-01, 2.6612e-02],\n",
       "          [3.6694e-01, 5.3086e-01, 3.8514e-02,  ..., 1.0052e-01,\n",
       "           4.7019e-01, 3.8661e-01],\n",
       "          [1.2374e-01, 8.7459e-01, 3.6839e-01,  ..., 1.8539e-01,\n",
       "           1.3411e-02, 6.9068e-01]]],\n",
       "\n",
       "\n",
       "        [[[4.0492e-01, 2.1723e-02, 9.0339e-02,  ..., 5.5240e-02,\n",
       "           1.0612e-01, 8.4026e-01],\n",
       "          [2.2801e-01, 8.3934e-01, 3.6437e-01,  ..., 3.0741e-01,\n",
       "           5.8704e-01, 6.8930e-01],\n",
       "          [1.9869e-02, 6.2378e-01, 1.1785e-01,  ..., 2.3780e-01,\n",
       "           1.2931e-01, 4.6727e-01],\n",
       "          ...,\n",
       "          [5.8797e-02, 6.1922e-01, 2.5495e-01,  ..., 8.4138e-01,\n",
       "           6.8724e-03, 4.0401e-02],\n",
       "          [1.9088e-01, 6.2342e-01, 1.7064e-02,  ..., 3.0835e-01,\n",
       "           2.6668e-01, 5.9526e-02],\n",
       "          [6.6877e-01, 3.1042e-01, 2.9137e-01,  ..., 1.7502e-01,\n",
       "           5.4531e-01, 3.0708e-01]],\n",
       "\n",
       "         [[6.8025e-01, 7.5228e-01, 4.9295e-01,  ..., 7.3282e-02,\n",
       "           3.2170e-03, 1.9881e-01],\n",
       "          [8.5008e-02, 4.3193e-02, 8.0992e-01,  ..., 1.6613e-01,\n",
       "           7.6076e-01, 2.0280e-01],\n",
       "          [7.2865e-02, 8.5267e-02, 1.0015e-01,  ..., 3.8143e-01,\n",
       "           3.8067e-01, 5.3045e-01],\n",
       "          ...,\n",
       "          [2.0644e-01, 1.2986e-02, 5.6258e-02,  ..., 3.1061e-01,\n",
       "           2.0334e-01, 1.8671e-01],\n",
       "          [4.2469e-01, 8.0756e-02, 8.1774e-01,  ..., 1.9541e-01,\n",
       "           2.3878e-01, 2.4982e-01],\n",
       "          [2.2602e-01, 3.9723e-01, 3.0777e-01,  ..., 1.1339e-02,\n",
       "           1.9421e-02, 3.0525e-01]],\n",
       "\n",
       "         [[2.5049e-01, 3.3397e-01, 7.0273e-01,  ..., 2.0018e-01,\n",
       "           7.3960e-02, 2.3662e-01],\n",
       "          [1.4436e-01, 4.1408e-01, 1.5170e-01,  ..., 1.0764e-01,\n",
       "           5.3435e-02, 4.0141e-01],\n",
       "          [4.8000e-02, 1.4205e-01, 1.8402e-01,  ..., 3.5233e-01,\n",
       "           1.6072e-01, 4.2223e-01],\n",
       "          ...,\n",
       "          [5.0096e-02, 8.0286e-01, 6.9764e-01,  ..., 3.9296e-01,\n",
       "           3.2004e-02, 1.5653e-01],\n",
       "          [6.8008e-01, 3.8328e-01, 6.0436e-01,  ..., 6.5309e-03,\n",
       "           1.0757e-01, 6.8421e-02],\n",
       "          [3.4821e-01, 7.1829e-02, 5.6593e-01,  ..., 2.3739e-01,\n",
       "           2.6320e-02, 1.9840e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[6.7632e-02, 3.9044e-01, 4.1274e-01,  ..., 1.2734e-01,\n",
       "           3.8194e-01, 2.4935e-01],\n",
       "          [7.0144e-01, 9.3670e-01, 7.2108e-02,  ..., 5.7967e-01,\n",
       "           4.7041e-02, 4.5910e-01],\n",
       "          [6.6027e-01, 3.7985e-01, 5.3160e-01,  ..., 3.6855e-01,\n",
       "           1.3602e-01, 2.4817e-01],\n",
       "          ...,\n",
       "          [4.2607e-02, 2.7540e-01, 1.0131e-01,  ..., 3.8486e-01,\n",
       "           7.0187e-02, 3.7814e-01],\n",
       "          [7.0649e-01, 5.4263e-01, 1.5037e-01,  ..., 8.8700e-02,\n",
       "           1.5673e-01, 4.6823e-02],\n",
       "          [2.0799e-01, 8.6369e-01, 3.7470e-02,  ..., 9.0634e-01,\n",
       "           5.9346e-01, 1.3074e-01]],\n",
       "\n",
       "         [[5.4205e-03, 3.8916e-01, 5.1362e-02,  ..., 9.1315e-01,\n",
       "           3.0290e-03, 1.5539e-02],\n",
       "          [9.4229e-02, 2.2035e-01, 2.2546e-01,  ..., 8.7936e-01,\n",
       "           6.3436e-01, 4.1600e-01],\n",
       "          [3.6113e-01, 5.8909e-01, 4.7039e-01,  ..., 4.7298e-01,\n",
       "           6.1436e-01, 1.7405e-01],\n",
       "          ...,\n",
       "          [2.4973e-01, 2.1405e-01, 2.9785e-01,  ..., 1.5117e-02,\n",
       "           5.6752e-01, 9.7602e-02],\n",
       "          [3.3157e-02, 3.4618e-02, 4.2788e-01,  ..., 3.6154e-01,\n",
       "           2.8707e-01, 8.5479e-01],\n",
       "          [6.5559e-01, 5.7095e-01, 3.7402e-01,  ..., 1.4898e-01,\n",
       "           2.5877e-01, 6.2680e-01]],\n",
       "\n",
       "         [[2.7014e-01, 4.1015e-01, 1.2158e-01,  ..., 6.9763e-02,\n",
       "           4.1702e-02, 6.0808e-02],\n",
       "          [1.8831e-01, 3.4614e-01, 4.3384e-02,  ..., 9.4262e-02,\n",
       "           3.0492e-01, 8.1945e-01],\n",
       "          [3.0037e-02, 4.4339e-01, 1.8669e-01,  ..., 1.4158e-02,\n",
       "           2.8766e-03, 3.4410e-01],\n",
       "          ...,\n",
       "          [6.3319e-01, 3.5178e-01, 5.0425e-01,  ..., 6.3059e-01,\n",
       "           4.7076e-01, 2.8878e-01],\n",
       "          [1.0645e-01, 5.8505e-01, 5.6036e-01,  ..., 8.0020e-01,\n",
       "           1.8912e-01, 4.8638e-01],\n",
       "          [7.9259e-01, 5.9042e-01, 3.5012e-01,  ..., 1.6167e-01,\n",
       "           2.8372e-01, 1.4112e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[2.6167e-02, 4.6198e-01, 2.8601e-01,  ..., 4.8092e-01,\n",
       "           7.9541e-02, 6.9558e-01],\n",
       "          [9.2014e-01, 8.8181e-03, 3.6423e-01,  ..., 3.3230e-01,\n",
       "           3.4267e-01, 8.6748e-02],\n",
       "          [6.1761e-03, 5.2818e-01, 1.1462e-01,  ..., 1.9922e-01,\n",
       "           4.0034e-01, 1.3291e-01],\n",
       "          ...,\n",
       "          [6.0571e-01, 4.4401e-01, 7.5274e-01,  ..., 6.5214e-01,\n",
       "           8.4590e-01, 7.5468e-01],\n",
       "          [7.4037e-01, 7.6604e-01, 2.9670e-02,  ..., 8.1606e-02,\n",
       "           9.2353e-01, 6.9686e-01],\n",
       "          [5.0045e-03, 3.1780e-01, 2.5549e-04,  ..., 3.4019e-01,\n",
       "           1.5681e-01, 9.2019e-02]],\n",
       "\n",
       "         [[6.7072e-02, 1.6968e-01, 5.8944e-01,  ..., 4.2593e-01,\n",
       "           9.9847e-02, 8.3844e-02],\n",
       "          [9.5320e-01, 1.6004e-01, 3.0042e-01,  ..., 1.0444e-01,\n",
       "           7.4103e-01, 2.8710e-01],\n",
       "          [1.9494e-01, 1.6038e-01, 6.9264e-01,  ..., 5.3985e-01,\n",
       "           8.0100e-01, 5.9949e-02],\n",
       "          ...,\n",
       "          [4.7658e-01, 2.1991e-01, 3.4081e-01,  ..., 6.5144e-01,\n",
       "           2.0391e-02, 6.2759e-01],\n",
       "          [5.7824e-01, 6.9779e-01, 1.2356e-01,  ..., 2.3922e-01,\n",
       "           3.2497e-01, 3.0632e-02],\n",
       "          [7.1917e-01, 1.4307e-03, 2.8021e-02,  ..., 4.3231e-01,\n",
       "           1.2766e-01, 5.6003e-01]],\n",
       "\n",
       "         [[8.6087e-01, 5.2176e-01, 5.1975e-01,  ..., 7.4118e-01,\n",
       "           7.8139e-02, 9.2743e-02],\n",
       "          [5.9653e-01, 8.7208e-01, 6.1894e-02,  ..., 1.4432e-01,\n",
       "           5.0603e-01, 2.9318e-02],\n",
       "          [6.2654e-02, 1.2141e-01, 7.1900e-01,  ..., 1.3334e-01,\n",
       "           1.4051e-01, 1.1416e-01],\n",
       "          ...,\n",
       "          [5.9026e-01, 1.0505e-02, 6.2802e-01,  ..., 7.6776e-01,\n",
       "           6.9648e-02, 2.7151e-02],\n",
       "          [1.1348e-01, 9.5824e-01, 2.8078e-01,  ..., 5.8241e-02,\n",
       "           8.0011e-01, 3.1326e-01],\n",
       "          [7.2970e-01, 6.9196e-01, 7.7691e-01,  ..., 2.5614e-01,\n",
       "           2.4235e-01, 9.1875e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.3871e-01, 3.6395e-02, 8.8285e-01,  ..., 5.6818e-01,\n",
       "           4.4738e-03, 4.9813e-02],\n",
       "          [4.9876e-01, 3.1585e-02, 1.3744e-01,  ..., 1.4663e-02,\n",
       "           7.8038e-01, 6.0261e-01],\n",
       "          [4.0260e-01, 1.1990e-01, 1.3057e-01,  ..., 9.0153e-01,\n",
       "           4.7972e-01, 1.8835e-02],\n",
       "          ...,\n",
       "          [5.4609e-02, 4.7731e-01, 3.2031e-02,  ..., 8.0595e-01,\n",
       "           1.9231e-02, 9.3455e-02],\n",
       "          [6.7671e-01, 5.4459e-01, 1.8087e-01,  ..., 4.2944e-01,\n",
       "           4.2346e-01, 3.0886e-01],\n",
       "          [3.4705e-01, 2.0413e-01, 9.1512e-01,  ..., 2.3555e-01,\n",
       "           1.5224e-01, 3.0601e-02]],\n",
       "\n",
       "         [[5.0321e-01, 2.6789e-02, 4.5384e-01,  ..., 6.5637e-02,\n",
       "           6.5098e-01, 3.3804e-01],\n",
       "          [3.8993e-01, 1.2734e-02, 5.3625e-01,  ..., 5.6947e-01,\n",
       "           1.0394e-02, 1.8574e-02],\n",
       "          [2.8732e-01, 3.8142e-01, 5.3821e-01,  ..., 1.9394e-01,\n",
       "           1.8989e-01, 2.4124e-02],\n",
       "          ...,\n",
       "          [2.8607e-02, 7.9672e-01, 1.4034e-02,  ..., 7.1955e-01,\n",
       "           2.8105e-01, 4.2847e-01],\n",
       "          [4.9350e-01, 9.0831e-01, 8.0846e-03,  ..., 4.0167e-01,\n",
       "           1.8455e-01, 4.8317e-01],\n",
       "          [5.7502e-01, 6.6523e-01, 3.6944e-02,  ..., 1.6142e-01,\n",
       "           2.7521e-01, 1.4043e-02]],\n",
       "\n",
       "         [[1.6765e-01, 4.8126e-01, 8.0110e-01,  ..., 4.1054e-01,\n",
       "           2.3862e-01, 5.2115e-01],\n",
       "          [1.6040e-01, 7.3268e-02, 6.6567e-01,  ..., 8.5472e-01,\n",
       "           6.0111e-01, 2.3740e-03],\n",
       "          [7.5587e-02, 1.1194e-01, 9.4734e-02,  ..., 3.2824e-01,\n",
       "           3.2663e-01, 3.0657e-01],\n",
       "          ...,\n",
       "          [6.6845e-01, 4.7560e-01, 4.3174e-01,  ..., 9.4913e-02,\n",
       "           2.1916e-01, 6.4283e-01],\n",
       "          [5.0078e-01, 4.7468e-02, 1.6888e-01,  ..., 4.3032e-01,\n",
       "           1.0972e-02, 2.2092e-02],\n",
       "          [1.0012e-01, 6.6346e-02, 1.0665e-02,  ..., 1.9384e-01,\n",
       "           5.7395e-01, 6.9877e-03]]],\n",
       "\n",
       "\n",
       "        [[[1.9999e-01, 2.2107e-01, 1.8335e-01,  ..., 1.9935e-01,\n",
       "           6.9167e-03, 9.5339e-02],\n",
       "          [9.3049e-02, 3.6432e-01, 3.2806e-01,  ..., 2.3055e-01,\n",
       "           7.9845e-01, 3.2263e-02],\n",
       "          [7.5203e-02, 4.9358e-01, 2.3151e-01,  ..., 6.4569e-01,\n",
       "           4.4704e-01, 5.7870e-01],\n",
       "          ...,\n",
       "          [1.6111e-01, 4.3537e-01, 2.9950e-02,  ..., 1.0725e-02,\n",
       "           7.5409e-01, 6.7643e-02],\n",
       "          [4.7919e-02, 3.4978e-02, 2.7717e-01,  ..., 8.2325e-02,\n",
       "           5.4683e-01, 4.5716e-01],\n",
       "          [1.4350e-01, 9.8800e-01, 8.9757e-01,  ..., 2.3144e-02,\n",
       "           7.0318e-02, 9.1173e-01]],\n",
       "\n",
       "         [[2.7610e-01, 6.0337e-01, 7.9483e-02,  ..., 4.2710e-01,\n",
       "           4.6883e-02, 6.8993e-02],\n",
       "          [1.2458e-01, 5.1049e-01, 6.2309e-02,  ..., 8.4878e-02,\n",
       "           2.8633e-01, 6.2387e-02],\n",
       "          [6.0387e-01, 6.2932e-01, 7.4868e-01,  ..., 8.3677e-01,\n",
       "           7.1397e-02, 8.1212e-01],\n",
       "          ...,\n",
       "          [4.1576e-02, 4.8268e-01, 1.2468e-01,  ..., 1.4193e-01,\n",
       "           2.5876e-01, 2.3560e-01],\n",
       "          [4.7686e-01, 7.8790e-03, 4.8149e-02,  ..., 2.2872e-01,\n",
       "           6.5949e-01, 3.5101e-02],\n",
       "          [1.6185e-01, 3.2293e-02, 3.7551e-01,  ..., 2.3110e-01,\n",
       "           6.9212e-01, 2.1791e-01]],\n",
       "\n",
       "         [[2.0196e-02, 2.3089e-01, 1.6983e-01,  ..., 3.0668e-01,\n",
       "           7.9212e-01, 1.3523e-02],\n",
       "          [1.0495e-01, 6.0776e-01, 7.4936e-01,  ..., 6.9965e-03,\n",
       "           2.3215e-01, 3.5205e-02],\n",
       "          [7.9872e-01, 9.2193e-01, 4.8066e-02,  ..., 3.5206e-01,\n",
       "           4.4825e-02, 6.8448e-01],\n",
       "          ...,\n",
       "          [3.9379e-01, 7.0703e-01, 1.8788e-01,  ..., 7.4348e-02,\n",
       "           1.8446e-01, 4.0482e-04],\n",
       "          [7.2720e-01, 5.6005e-01, 5.3518e-02,  ..., 4.3334e-01,\n",
       "           3.3386e-03, 6.9809e-01],\n",
       "          [1.0654e-01, 7.0540e-02, 8.0085e-01,  ..., 2.6348e-01,\n",
       "           6.8265e-03, 1.2258e-03]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[2.0205e-02, 5.6677e-01, 5.7425e-01,  ..., 1.9059e-01,\n",
       "           4.1814e-02, 2.3361e-02],\n",
       "          [7.5746e-01, 1.8999e-01, 3.7942e-01,  ..., 9.0480e-01,\n",
       "           7.3211e-01, 6.1391e-02],\n",
       "          [1.6120e-01, 7.2422e-02, 1.3616e-01,  ..., 7.0970e-01,\n",
       "           5.2273e-02, 5.3952e-01],\n",
       "          ...,\n",
       "          [2.3043e-01, 7.6287e-02, 4.3541e-01,  ..., 1.2158e-01,\n",
       "           1.3324e-01, 2.5098e-01],\n",
       "          [6.9315e-01, 1.3214e-02, 2.9988e-01,  ..., 1.1837e-03,\n",
       "           9.2349e-01, 1.5982e-01],\n",
       "          [6.5305e-03, 3.1870e-02, 2.2932e-01,  ..., 5.3098e-03,\n",
       "           2.0941e-02, 6.4679e-01]],\n",
       "\n",
       "         [[6.3722e-01, 2.8269e-01, 2.0661e-01,  ..., 1.2847e-01,\n",
       "           3.5275e-02, 1.4586e-02],\n",
       "          [1.5040e-01, 1.8657e-02, 2.5405e-01,  ..., 4.8085e-01,\n",
       "           9.0889e-01, 1.3896e-01],\n",
       "          [4.1730e-01, 2.3820e-01, 4.7929e-01,  ..., 3.1420e-01,\n",
       "           8.8798e-01, 4.0520e-01],\n",
       "          ...,\n",
       "          [2.5399e-01, 5.5968e-01, 5.9778e-01,  ..., 2.7640e-01,\n",
       "           1.4817e-01, 2.8860e-01],\n",
       "          [1.1231e-01, 2.4823e-02, 2.3452e-01,  ..., 2.8051e-01,\n",
       "           5.6781e-01, 1.4477e-02],\n",
       "          [4.3324e-01, 3.9073e-01, 7.0402e-01,  ..., 1.9403e-01,\n",
       "           2.3872e-01, 8.5665e-02]],\n",
       "\n",
       "         [[7.5248e-01, 1.3705e-01, 2.8494e-01,  ..., 3.2774e-03,\n",
       "           2.7872e-01, 2.8775e-01],\n",
       "          [1.9410e-01, 1.0956e-01, 2.3032e-01,  ..., 5.3575e-01,\n",
       "           2.8225e-01, 2.6710e-01],\n",
       "          [4.9524e-03, 4.2585e-01, 8.9494e-02,  ..., 1.2605e-01,\n",
       "           4.7856e-01, 1.2158e-02],\n",
       "          ...,\n",
       "          [1.9751e-01, 4.5798e-02, 6.7423e-02,  ..., 5.7775e-01,\n",
       "           3.9759e-01, 8.8966e-04],\n",
       "          [4.9132e-02, 5.7208e-01, 4.3710e-01,  ..., 1.9055e-01,\n",
       "           4.7018e-01, 8.3238e-01],\n",
       "          [4.5961e-01, 1.1604e-01, 2.5457e-01,  ..., 5.5498e-02,\n",
       "           7.7502e-01, 1.5298e-01]]],\n",
       "\n",
       "\n",
       "        [[[8.2467e-01, 9.1209e-01, 3.4108e-01,  ..., 9.0026e-01,\n",
       "           5.6699e-01, 1.4967e-01],\n",
       "          [6.6629e-02, 2.7732e-01, 5.5399e-02,  ..., 2.3375e-01,\n",
       "           2.7536e-01, 1.5968e-01],\n",
       "          [7.1978e-02, 9.4109e-02, 3.2278e-02,  ..., 2.7967e-03,\n",
       "           4.6676e-01, 2.1299e-02],\n",
       "          ...,\n",
       "          [3.6003e-01, 6.4918e-01, 3.9923e-01,  ..., 5.1868e-03,\n",
       "           1.7450e-01, 3.7846e-01],\n",
       "          [1.1387e-01, 5.2882e-01, 6.2856e-04,  ..., 4.4088e-02,\n",
       "           3.7260e-01, 4.7299e-01],\n",
       "          [8.2708e-01, 4.3588e-01, 1.3989e-01,  ..., 3.7854e-01,\n",
       "           3.7373e-02, 2.1384e-02]],\n",
       "\n",
       "         [[8.2279e-01, 2.1712e-01, 2.2969e-02,  ..., 3.9912e-01,\n",
       "           3.2199e-01, 1.7138e-03],\n",
       "          [1.6841e-01, 1.1589e-02, 8.0146e-02,  ..., 9.5130e-02,\n",
       "           1.9085e-01, 9.1691e-03],\n",
       "          [7.1324e-01, 2.0764e-02, 4.3077e-01,  ..., 2.7227e-01,\n",
       "           5.2644e-01, 1.5342e-01],\n",
       "          ...,\n",
       "          [8.1693e-01, 6.0904e-01, 3.1675e-02,  ..., 2.6339e-02,\n",
       "           3.0865e-01, 2.1777e-01],\n",
       "          [3.9298e-01, 1.3817e-01, 4.5916e-01,  ..., 7.3567e-01,\n",
       "           8.5033e-01, 3.3623e-01],\n",
       "          [6.1699e-01, 5.8552e-01, 2.6312e-01,  ..., 9.5660e-02,\n",
       "           5.8342e-01, 9.6026e-01]],\n",
       "\n",
       "         [[3.0367e-01, 6.4157e-01, 2.1869e-03,  ..., 6.9221e-02,\n",
       "           3.3828e-01, 5.2362e-01],\n",
       "          [9.4440e-02, 4.4536e-01, 8.4515e-01,  ..., 6.5165e-02,\n",
       "           9.6985e-02, 1.5178e-01],\n",
       "          [2.4833e-01, 2.9045e-01, 4.7553e-01,  ..., 1.0718e-01,\n",
       "           3.7763e-01, 8.7261e-01],\n",
       "          ...,\n",
       "          [6.5696e-01, 1.1420e-02, 2.1485e-01,  ..., 4.7091e-03,\n",
       "           5.2680e-02, 9.2110e-01],\n",
       "          [3.5024e-01, 6.4420e-01, 1.6731e-01,  ..., 6.4361e-01,\n",
       "           5.0830e-01, 2.6065e-01],\n",
       "          [3.0669e-02, 1.6653e-01, 5.1434e-01,  ..., 8.7945e-01,\n",
       "           6.8960e-02, 1.7480e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[5.6873e-02, 2.0699e-01, 4.1809e-01,  ..., 9.3431e-01,\n",
       "           2.0553e-01, 4.4492e-01],\n",
       "          [5.0868e-01, 2.8767e-01, 8.2955e-02,  ..., 1.2072e-01,\n",
       "           1.7766e-02, 2.9703e-01],\n",
       "          [3.8897e-01, 5.6918e-01, 1.9205e-01,  ..., 8.4208e-01,\n",
       "           2.0911e-02, 2.9115e-01],\n",
       "          ...,\n",
       "          [4.6382e-01, 8.6383e-03, 7.2274e-02,  ..., 7.8418e-01,\n",
       "           1.2440e-01, 6.7531e-03],\n",
       "          [1.8410e-02, 8.7074e-03, 5.2154e-02,  ..., 3.7916e-02,\n",
       "           8.5170e-03, 2.2078e-01],\n",
       "          [6.8944e-02, 6.8185e-04, 9.4651e-02,  ..., 1.2220e-01,\n",
       "           2.6179e-01, 6.3289e-01]],\n",
       "\n",
       "         [[2.0036e-01, 2.0566e-01, 1.7367e-02,  ..., 6.4768e-01,\n",
       "           9.0534e-01, 3.8147e-01],\n",
       "          [1.7608e-01, 1.1710e-01, 8.6784e-04,  ..., 5.2060e-03,\n",
       "           1.8992e-02, 1.0229e-01],\n",
       "          [7.8671e-03, 7.4598e-01, 5.9364e-01,  ..., 6.7209e-01,\n",
       "           4.1366e-01, 1.8861e-01],\n",
       "          ...,\n",
       "          [5.9636e-02, 4.6081e-03, 2.8690e-01,  ..., 7.4983e-02,\n",
       "           8.5089e-01, 4.4303e-01],\n",
       "          [6.6282e-02, 2.4275e-01, 7.0536e-01,  ..., 2.3906e-01,\n",
       "           5.2732e-03, 5.4874e-01],\n",
       "          [3.4972e-01, 3.5809e-01, 4.7804e-02,  ..., 9.2665e-02,\n",
       "           3.9060e-02, 4.1444e-01]],\n",
       "\n",
       "         [[3.2285e-01, 4.1050e-02, 1.9096e-01,  ..., 7.7168e-01,\n",
       "           8.5512e-01, 7.4066e-01],\n",
       "          [5.7094e-01, 8.2006e-01, 8.1404e-01,  ..., 9.1638e-01,\n",
       "           2.6072e-01, 8.3509e-03],\n",
       "          [6.2823e-01, 8.6610e-02, 7.4049e-01,  ..., 9.0106e-03,\n",
       "           2.1432e-02, 6.1431e-01],\n",
       "          ...,\n",
       "          [4.9277e-02, 6.9162e-01, 3.6133e-03,  ..., 3.8484e-01,\n",
       "           7.4371e-01, 7.4193e-01],\n",
       "          [1.0645e-01, 5.6716e-01, 3.0775e-01,  ..., 2.4756e-01,\n",
       "           1.5887e-01, 1.5223e-01],\n",
       "          [4.2638e-01, 2.6292e-01, 7.0294e-03,  ..., 1.9488e-01,\n",
       "           3.0571e-01, 7.5394e-01]]]], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loss_MSE(proba,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0519, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 128, 128, 32])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(loss_Ce_torch(output,target)*mask[:,1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3654, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((input*mask[:,:1]- target*mask[:,:1])**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 2, 128, 128, 32])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1141)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8830, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4420, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mse_masked*mask).sum()/mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_label = torch.randn(12, 2, 128,128, 32, requires_grad=True)\n",
    "input_unlabel = torch.randn(12, 2, 128,128, 32, requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from monai.utils import LossReduction\n",
    "\n",
    "from monai.networks import one_hot\n",
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "\n",
    "class MSELoss(_Loss):\n",
    "    def __init__(self, include_background: bool= True, to_onehot_y: bool = False, softmax: bool = True,\n",
    "                 reduction = LossReduction.MEAN, sigmoid: bool = False, batch: bool = False,*args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Args follow :py:class:`monai.losses.DiceLoss`.\n",
    "        \"\"\"\n",
    "        super().__init__(reduction=LossReduction(reduction).value)\n",
    "        \n",
    "        self.include_background = include_background\n",
    "        self.to_onehot_y = to_onehot_y\n",
    "        \n",
    "        self.softmax = softmax\n",
    "        self.sigmoid = sigmoid\n",
    "        self.batch = batch\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: the shape should be BNH[WD].\n",
    "            target: the shape should be BNH[WD].\n",
    "            mask: the shape should B1H[WD] or 11H[WD].\n",
    "        \"\"\"\n",
    "        if self.sigmoid:\n",
    "            input = torch.sigmoid(input)\n",
    "\n",
    "        n_pred_ch = input.shape[1]\n",
    "        if self.softmax:\n",
    "            if n_pred_ch == 1:\n",
    "                warnings.warn(\"single channel prediction, `softmax=True` ignored.\")\n",
    "            else:\n",
    "                input = torch.softmax(input, 1)\n",
    "\n",
    "        if self.to_onehot_y:\n",
    "            if n_pred_ch == 1:\n",
    "                warnings.warn(\"single channel prediction, `to_onehot_y=True` ignored.\")\n",
    "            else:\n",
    "                target = one_hot(target, num_classes=n_pred_ch)\n",
    "                \n",
    "        if not self.include_background:\n",
    "            if n_pred_ch == 1:\n",
    "                warnings.warn(\"single channel prediction, `include_background=False` ignored.\")\n",
    "            else:\n",
    "                # if skipping background, removing first channel\n",
    "                target = target[:, 1:]\n",
    "                input = input[:, 1:]\n",
    "                \n",
    "        if target.shape != input.shape:\n",
    "            raise AssertionError(f\"ground truth has different shape ({target.shape}) from input ({input.shape})\")\n",
    "\n",
    "        # reducing only spatial dimensions (not batch nor channels)\n",
    "        reduce_axis: list[int] = torch.arange(2, len(input.shape)).tolist()\n",
    "        if self.batch:\n",
    "            # reducing spatial dimensions and batch\n",
    "            reduce_axis = [0] + reduce_axis\n",
    "\n",
    "        \n",
    "        squared_diff = torch.sum((target-input)**2, axis=1)\n",
    "        \n",
    "        f = squared_diff\n",
    "\n",
    "        if self.reduction == LossReduction.MEAN.value:\n",
    "            f = torch.mean(f)  # the batch and channel average\n",
    "        elif self.reduction == LossReduction.SUM.value:\n",
    "            f = torch.sum(f)  # sum over the batch and channel dims\n",
    "        elif self.reduction == LossReduction.NONE.value:\n",
    "            # If we are not computing voxelwise loss components at least\n",
    "            # make sure a none reduction maintains a broadcastable shape\n",
    "            broadcast_shape = list(f.shape[0:2]) + [1] * (len(input.shape) - 2)\n",
    "            f = f.view(broadcast_shape)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported reduction: {self.reduction}, available options are [\"mean\", \"sum\", \"none\"].')\n",
    "\n",
    "        return f\n",
    "    \n",
    "class MaskedMSELoss(MSELoss):\n",
    "    \"\"\"\n",
    "    Add an additional `masking` process before `DiceLoss`, accept a binary mask ([0, 1]) indicating a region,\n",
    "    `input` and `target` will be masked by the region: region with mask `1` will keep the original value,\n",
    "    region with `0` mask will be converted to `0`. Then feed `input` and `target` to normal `DiceLoss` computation.\n",
    "    This has the effect of ensuring only the masked region contributes to the loss computation and\n",
    "    hence gradient calculation.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Args follow :py:class:`monai.losses.DiceLoss`.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.spatial_weighted = monai.losses.MaskedLoss(loss=super().forward)\n",
    "\n",
    "\n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor, mask: torch.Tensor ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: the shape should be BNH[WD].\n",
    "            target: the shape should be BNH[WD].\n",
    "            mask: the shape should B1H[WD] or 11H[WD].\n",
    "        \"\"\"\n",
    "        return self.spatial_weighted(input=input, target=target, mask=mask)  # type: ignore[no-any-return]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from monai.utils import LossReduction\n",
    "\n",
    "from monai.networks import one_hot\n",
    "class MaskedMSELoss(_Loss):\n",
    "    def __init__(self, include_background: bool= True, to_onehot_y: bool = False, softmax: bool = True,\n",
    "                 reduction = LossReduction.MEAN, sigmoid: bool = False, batch: bool = False,*args: Any, **kwargs: Any) -> None:\n",
    "        \"\"\"\n",
    "        Args follow :py:class:`monai.losses.DiceLoss`.\n",
    "        \"\"\"\n",
    "        super().__init__(reduction=LossReduction(reduction).value)\n",
    "        \n",
    "        self.include_background = include_background\n",
    "        self.to_onehot_y = to_onehot_y\n",
    "        \n",
    "        self.softmax = softmax\n",
    "        self.sigmoid = sigmoid\n",
    "        self.batch = batch\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, input: torch.Tensor, target: torch.Tensor, mask: torch.Tensor ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input: the shape should be BNH[WD].\n",
    "            target: the shape should be BNH[WD].\n",
    "            mask: the shape should B1H[WD] or 11H[WD].\n",
    "        \"\"\"\n",
    "        if self.sigmoid:\n",
    "            input = torch.sigmoid(input)\n",
    "\n",
    "        n_pred_ch = input.shape[1]\n",
    "        if self.softmax:\n",
    "            if n_pred_ch == 1:\n",
    "                warnings.warn(\"single channel prediction, `softmax=True` ignored.\")\n",
    "            else:\n",
    "                input = torch.softmax(input, 1)\n",
    "\n",
    "        if self.to_onehot_y:\n",
    "            if n_pred_ch == 1:\n",
    "                warnings.warn(\"single channel prediction, `to_onehot_y=True` ignored.\")\n",
    "            else:\n",
    "                target = one_hot(target, num_classes=n_pred_ch)\n",
    "                \n",
    "        if not self.include_background:\n",
    "            if n_pred_ch == 1:\n",
    "                warnings.warn(\"single channel prediction, `include_background=False` ignored.\")\n",
    "            else:\n",
    "                # if skipping background, removing first channel\n",
    "                target = target[:, 1:]\n",
    "                input = input[:, 1:]\n",
    "                \n",
    "        if target.shape != input.shape:\n",
    "            raise AssertionError(f\"ground truth has different shape ({target.shape}) from input ({input.shape})\")\n",
    "\n",
    "        # reducing only spatial dimensions (not batch nor channels)\n",
    "        reduce_axis: list[int] = torch.arange(2, len(input.shape)).tolist()\n",
    "        if self.batch:\n",
    "            # reducing spatial dimensions and batch\n",
    "            reduce_axis = [0] + reduce_axis\n",
    "\n",
    "        \n",
    "        squared_diff = (target-input)**2\n",
    "        \n",
    "        f = torch.sum(squared_diff * mask , dim=reduce_axis) / (torch.sum(mask, dim =reduce_axis))\n",
    "\n",
    "        if self.reduction == LossReduction.MEAN.value:\n",
    "            f = torch.mean(f)  # the batch and channel average\n",
    "        elif self.reduction == LossReduction.SUM.value:\n",
    "            f = torch.sum(f)  # sum over the batch and channel dims\n",
    "        elif self.reduction == LossReduction.NONE.value:\n",
    "            # If we are not computing voxelwise loss components at least\n",
    "            # make sure a none reduction maintains a broadcastable shape\n",
    "            broadcast_shape = list(f.shape[0:2]) + [1] * (len(input.shape) - 2)\n",
    "            f = f.view(broadcast_shape)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported reduction: {self.reduction}, available options are [\"mean\", \"sum\", \"none\"].')\n",
    "\n",
    "        return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.clamp(torch.randn_like(input_unlabel)* 0.1, -0.2, 0.2) # Bruit gaussian pas plus gros que 0.2\n",
    "ema_inputs = input_unlabel + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = input_label * 2\n",
    "with torch.no_grad():\n",
    "    ema_output = ema_inputs*2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 8\n",
    "volume_batch_r = input_unlabel.repeat(2, 1, 1, 1, 1)\n",
    "stride = volume_batch_r.shape[0] // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.zeros([stride * T, 2, 128, 128, 32]).cuda()\n",
    "for i in range(T//2):\n",
    "    ema_inputs = volume_batch_r + torch.clamp(torch.randn_like(volume_batch_r) * 0.1, -0.2, 0.2)\n",
    "    with torch.no_grad():\n",
    "        preds[2 * stride * i:2 * stride * (i + 1)] = ema_inputs*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = F.softmax(preds, dim=1)\n",
    "preds = preds.reshape(T, stride, 2, 128, 128, 32)\n",
    "preds = torch.mean(preds, dim=0)  \n",
    "\n",
    "uncertainty = -1.0*torch.sum(preds*torch.log(preds + 1e-6), dim=1, keepdim=True) #(batch, 1, 112,112,80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[0.6499, 0.6815, 0.5827,  ..., 0.6922, 0.6502, 0.6054],\n",
       "           [0.6211, 0.5829, 0.6091,  ..., 0.6366, 0.6520, 0.5951],\n",
       "           [0.6435, 0.6879, 0.6667,  ..., 0.6394, 0.6223, 0.5823],\n",
       "           ...,\n",
       "           [0.5835, 0.5880, 0.6218,  ..., 0.6025, 0.6127, 0.5955],\n",
       "           [0.6376, 0.6862, 0.6176,  ..., 0.6051, 0.6115, 0.5971],\n",
       "           [0.6057, 0.5913, 0.5889,  ..., 0.6385, 0.5895, 0.5871]],\n",
       "\n",
       "          [[0.6874, 0.6602, 0.5958,  ..., 0.6037, 0.6035, 0.6403],\n",
       "           [0.5828, 0.6829, 0.6768,  ..., 0.6013, 0.6910, 0.5859],\n",
       "           [0.5824, 0.6053, 0.6895,  ..., 0.5927, 0.6931, 0.6021],\n",
       "           ...,\n",
       "           [0.5929, 0.6029, 0.6325,  ..., 0.6104, 0.6289, 0.6467],\n",
       "           [0.5915, 0.6677, 0.5933,  ..., 0.5881, 0.6237, 0.6155],\n",
       "           [0.6080, 0.6425, 0.6068,  ..., 0.6919, 0.6713, 0.5970]],\n",
       "\n",
       "          [[0.6140, 0.5840, 0.6587,  ..., 0.6153, 0.6900, 0.6396],\n",
       "           [0.6306, 0.5860, 0.6878,  ..., 0.6296, 0.5848, 0.6070],\n",
       "           [0.6120, 0.6391, 0.5951,  ..., 0.6192, 0.6492, 0.5869],\n",
       "           ...,\n",
       "           [0.6541, 0.6426, 0.6866,  ..., 0.5881, 0.5929, 0.6575],\n",
       "           [0.6899, 0.5847, 0.6022,  ..., 0.5892, 0.6359, 0.6352],\n",
       "           [0.6500, 0.5823, 0.6889,  ..., 0.5919, 0.6634, 0.6876]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.5842, 0.5844, 0.6392,  ..., 0.6799, 0.6852, 0.6293],\n",
       "           [0.6621, 0.6509, 0.5876,  ..., 0.5996, 0.6309, 0.6870],\n",
       "           [0.6613, 0.5835, 0.6569,  ..., 0.6355, 0.6126, 0.6062],\n",
       "           ...,\n",
       "           [0.6136, 0.5858, 0.5876,  ..., 0.6928, 0.5983, 0.5982],\n",
       "           [0.6642, 0.6928, 0.6189,  ..., 0.5862, 0.6466, 0.6854],\n",
       "           [0.6402, 0.5830, 0.6190,  ..., 0.6499, 0.6063, 0.6030]],\n",
       "\n",
       "          [[0.6922, 0.6757, 0.6158,  ..., 0.5989, 0.6846, 0.5824],\n",
       "           [0.6424, 0.5970, 0.6891,  ..., 0.6098, 0.6846, 0.6603],\n",
       "           [0.6195, 0.5836, 0.6121,  ..., 0.6928, 0.6180, 0.6619],\n",
       "           ...,\n",
       "           [0.5834, 0.6724, 0.6931,  ..., 0.5882, 0.6353, 0.5912],\n",
       "           [0.6870, 0.6027, 0.6780,  ..., 0.6019, 0.5933, 0.5830],\n",
       "           [0.6930, 0.6649, 0.6854,  ..., 0.6762, 0.5879, 0.5909]],\n",
       "\n",
       "          [[0.6007, 0.6917, 0.6931,  ..., 0.6284, 0.6412, 0.6828],\n",
       "           [0.5825, 0.6811, 0.6153,  ..., 0.5924, 0.5843, 0.5825],\n",
       "           [0.5872, 0.6339, 0.6465,  ..., 0.5982, 0.6110, 0.6536],\n",
       "           ...,\n",
       "           [0.5898, 0.6197, 0.5844,  ..., 0.6909, 0.6050, 0.5975],\n",
       "           [0.6430, 0.6875, 0.6929,  ..., 0.6216, 0.6603, 0.6067],\n",
       "           [0.6895, 0.6526, 0.5941,  ..., 0.6931, 0.5870, 0.6889]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.5890, 0.6394, 0.6553,  ..., 0.6619, 0.6409, 0.5880],\n",
       "           [0.6400, 0.6728, 0.6757,  ..., 0.6042, 0.6893, 0.5965],\n",
       "           [0.6431, 0.5956, 0.6337,  ..., 0.6142, 0.5944, 0.6704],\n",
       "           ...,\n",
       "           [0.6421, 0.5836, 0.6275,  ..., 0.6930, 0.5843, 0.5938],\n",
       "           [0.6861, 0.6895, 0.5836,  ..., 0.6758, 0.6717, 0.5898],\n",
       "           [0.5840, 0.6300, 0.6264,  ..., 0.6128, 0.6404, 0.6667]],\n",
       "\n",
       "          [[0.6499, 0.5951, 0.5833,  ..., 0.6907, 0.5937, 0.6528],\n",
       "           [0.5864, 0.5887, 0.6083,  ..., 0.6501, 0.5984, 0.5957],\n",
       "           [0.6467, 0.6931, 0.5825,  ..., 0.6674, 0.6895, 0.6714],\n",
       "           ...,\n",
       "           [0.6778, 0.5908, 0.6788,  ..., 0.6415, 0.6142, 0.5970],\n",
       "           [0.6139, 0.5875, 0.6551,  ..., 0.6852, 0.6078, 0.6291],\n",
       "           [0.6287, 0.5853, 0.6898,  ..., 0.5988, 0.6433, 0.6666]],\n",
       "\n",
       "          [[0.6754, 0.6085, 0.6608,  ..., 0.6873, 0.6027, 0.5897],\n",
       "           [0.5876, 0.6211, 0.6602,  ..., 0.6741, 0.6112, 0.5936],\n",
       "           [0.6003, 0.6929, 0.6508,  ..., 0.6640, 0.5950, 0.6549],\n",
       "           ...,\n",
       "           [0.6920, 0.5963, 0.6736,  ..., 0.5878, 0.6284, 0.5833],\n",
       "           [0.5970, 0.6037, 0.6894,  ..., 0.5829, 0.6069, 0.6810],\n",
       "           [0.6055, 0.5947, 0.6498,  ..., 0.6282, 0.5842, 0.6702]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.6835, 0.6644, 0.6080,  ..., 0.6012, 0.6505, 0.6890],\n",
       "           [0.6676, 0.6157, 0.6444,  ..., 0.6717, 0.6514, 0.6176],\n",
       "           [0.5840, 0.5965, 0.5865,  ..., 0.6371, 0.6373, 0.6773],\n",
       "           ...,\n",
       "           [0.6931, 0.5828, 0.6158,  ..., 0.5877, 0.6632, 0.6929],\n",
       "           [0.6217, 0.6670, 0.6331,  ..., 0.6606, 0.5875, 0.6102],\n",
       "           [0.6913, 0.6930, 0.6493,  ..., 0.6260, 0.5907, 0.6429]],\n",
       "\n",
       "          [[0.6930, 0.6607, 0.6718,  ..., 0.6497, 0.5933, 0.6521],\n",
       "           [0.5914, 0.6281, 0.5829,  ..., 0.6315, 0.6830, 0.6930],\n",
       "           [0.6542, 0.6337, 0.5864,  ..., 0.6827, 0.6929, 0.6925],\n",
       "           ...,\n",
       "           [0.6647, 0.6624, 0.6523,  ..., 0.6427, 0.5981, 0.6513],\n",
       "           [0.5867, 0.6797, 0.6879,  ..., 0.5987, 0.6226, 0.6670],\n",
       "           [0.6704, 0.6928, 0.5935,  ..., 0.6893, 0.6499, 0.5843]],\n",
       "\n",
       "          [[0.6929, 0.6107, 0.6929,  ..., 0.5828, 0.6450, 0.6021],\n",
       "           [0.5916, 0.6908, 0.6650,  ..., 0.5986, 0.6664, 0.6845],\n",
       "           [0.6845, 0.6835, 0.6162,  ..., 0.5891, 0.5870, 0.6388],\n",
       "           ...,\n",
       "           [0.6155, 0.5879, 0.6928,  ..., 0.6777, 0.6335, 0.6026],\n",
       "           [0.6144, 0.6868, 0.6866,  ..., 0.6908, 0.5937, 0.6634],\n",
       "           [0.6108, 0.5932, 0.6438,  ..., 0.5877, 0.5904, 0.6863]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.6039, 0.6627, 0.6603,  ..., 0.5848, 0.5942, 0.6685],\n",
       "           [0.6016, 0.6930, 0.6035,  ..., 0.5951, 0.6148, 0.6667],\n",
       "           [0.6027, 0.5825, 0.5904,  ..., 0.6931, 0.6919, 0.6576],\n",
       "           ...,\n",
       "           [0.6420, 0.6045, 0.6429,  ..., 0.6486, 0.6919, 0.6843],\n",
       "           [0.6081, 0.6101, 0.6900,  ..., 0.6565, 0.6684, 0.6554],\n",
       "           [0.5923, 0.6755, 0.5967,  ..., 0.6911, 0.6341, 0.6917]],\n",
       "\n",
       "          [[0.5841, 0.5868, 0.6719,  ..., 0.5825, 0.6551, 0.6885],\n",
       "           [0.6013, 0.5854, 0.5924,  ..., 0.6467, 0.6102, 0.6295],\n",
       "           [0.6865, 0.6732, 0.6000,  ..., 0.6914, 0.5973, 0.6241],\n",
       "           ...,\n",
       "           [0.6306, 0.6656, 0.6018,  ..., 0.6626, 0.6475, 0.5926],\n",
       "           [0.6212, 0.6931, 0.5869,  ..., 0.6447, 0.6554, 0.6706],\n",
       "           [0.6906, 0.6497, 0.5826,  ..., 0.6904, 0.6414, 0.5833]],\n",
       "\n",
       "          [[0.6079, 0.6845, 0.6427,  ..., 0.6784, 0.6568, 0.6926],\n",
       "           [0.5866, 0.5993, 0.5987,  ..., 0.6519, 0.6891, 0.6743],\n",
       "           [0.5856, 0.6469, 0.6069,  ..., 0.6101, 0.6372, 0.6807],\n",
       "           ...,\n",
       "           [0.5826, 0.6918, 0.6172,  ..., 0.6931, 0.5920, 0.6040],\n",
       "           [0.6291, 0.6354, 0.6817,  ..., 0.6581, 0.6663, 0.6087],\n",
       "           [0.6514, 0.6578, 0.6316,  ..., 0.5872, 0.6167, 0.6363]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.6768, 0.6557, 0.5922,  ..., 0.6865, 0.6038, 0.6901],\n",
       "           [0.5874, 0.6214, 0.6569,  ..., 0.6621, 0.6846, 0.5970],\n",
       "           [0.6531, 0.6010, 0.6193,  ..., 0.6080, 0.5976, 0.5829],\n",
       "           ...,\n",
       "           [0.6861, 0.5950, 0.6011,  ..., 0.6920, 0.6286, 0.6884],\n",
       "           [0.6036, 0.5915, 0.6112,  ..., 0.5839, 0.5985, 0.6256],\n",
       "           [0.6316, 0.6024, 0.6298,  ..., 0.6512, 0.6869, 0.6900]],\n",
       "\n",
       "          [[0.6398, 0.6920, 0.6002,  ..., 0.6662, 0.6868, 0.6054],\n",
       "           [0.6760, 0.6685, 0.6532,  ..., 0.5947, 0.6701, 0.6827],\n",
       "           [0.6015, 0.5971, 0.6587,  ..., 0.5991, 0.6420, 0.5824],\n",
       "           ...,\n",
       "           [0.6088, 0.6074, 0.5858,  ..., 0.6487, 0.6920, 0.6779],\n",
       "           [0.6101, 0.6780, 0.6919,  ..., 0.6517, 0.6346, 0.5866],\n",
       "           [0.5907, 0.5827, 0.5876,  ..., 0.5954, 0.6086, 0.6094]],\n",
       "\n",
       "          [[0.5888, 0.6872, 0.6856,  ..., 0.6617, 0.6100, 0.6537],\n",
       "           [0.6916, 0.6644, 0.6537,  ..., 0.6478, 0.6234, 0.6843],\n",
       "           [0.6929, 0.6927, 0.5889,  ..., 0.6717, 0.6620, 0.6807],\n",
       "           ...,\n",
       "           [0.6866, 0.6216, 0.5857,  ..., 0.6615, 0.6460, 0.6264],\n",
       "           [0.6190, 0.6442, 0.6762,  ..., 0.6508, 0.6194, 0.6164],\n",
       "           [0.6625, 0.6924, 0.5925,  ..., 0.6228, 0.6905, 0.5832]]]],\n",
       "\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.5829, 0.6837, 0.6931,  ..., 0.6190, 0.6736, 0.6785],\n",
       "           [0.6281, 0.6651, 0.5867,  ..., 0.6537, 0.6361, 0.5956],\n",
       "           [0.6877, 0.5941, 0.6132,  ..., 0.6127, 0.6742, 0.5928],\n",
       "           ...,\n",
       "           [0.6929, 0.6878, 0.6220,  ..., 0.6909, 0.6797, 0.5878],\n",
       "           [0.6050, 0.6280, 0.5985,  ..., 0.6922, 0.6505, 0.6166],\n",
       "           [0.6143, 0.6460, 0.6713,  ..., 0.6829, 0.6068, 0.5893]],\n",
       "\n",
       "          [[0.5857, 0.5862, 0.6848,  ..., 0.6722, 0.6445, 0.6319],\n",
       "           [0.5861, 0.5887, 0.5877,  ..., 0.6301, 0.6200, 0.5966],\n",
       "           [0.5864, 0.5829, 0.5881,  ..., 0.6220, 0.6456, 0.6789],\n",
       "           ...,\n",
       "           [0.6165, 0.6121, 0.5957,  ..., 0.6914, 0.6550, 0.6891],\n",
       "           [0.6677, 0.6347, 0.5852,  ..., 0.6395, 0.6328, 0.6181],\n",
       "           [0.6564, 0.6774, 0.5843,  ..., 0.6870, 0.6769, 0.6894]],\n",
       "\n",
       "          [[0.5839, 0.6645, 0.6381,  ..., 0.6564, 0.5923, 0.6887],\n",
       "           [0.5973, 0.6166, 0.6072,  ..., 0.5853, 0.6006, 0.5947],\n",
       "           [0.5835, 0.6560, 0.6591,  ..., 0.5835, 0.6330, 0.6924],\n",
       "           ...,\n",
       "           [0.5878, 0.6756, 0.6774,  ..., 0.5908, 0.6930, 0.6377],\n",
       "           [0.6317, 0.6453, 0.6920,  ..., 0.6235, 0.5942, 0.6931],\n",
       "           [0.6647, 0.6468, 0.6660,  ..., 0.6912, 0.5859, 0.5955]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.6132, 0.6928, 0.5902,  ..., 0.6511, 0.5923, 0.6812],\n",
       "           [0.5855, 0.6743, 0.6361,  ..., 0.6470, 0.6453, 0.6545],\n",
       "           [0.6058, 0.6558, 0.6525,  ..., 0.6931, 0.6922, 0.5833],\n",
       "           ...,\n",
       "           [0.6473, 0.6534, 0.6651,  ..., 0.6927, 0.6931, 0.6206],\n",
       "           [0.5879, 0.6752, 0.5942,  ..., 0.6394, 0.6602, 0.5936],\n",
       "           [0.6381, 0.6253, 0.6089,  ..., 0.5833, 0.5873, 0.6724]],\n",
       "\n",
       "          [[0.5987, 0.6197, 0.6128,  ..., 0.6863, 0.6867, 0.6931],\n",
       "           [0.5853, 0.6522, 0.6679,  ..., 0.6926, 0.5830, 0.6013],\n",
       "           [0.6136, 0.6182, 0.6077,  ..., 0.6273, 0.6363, 0.6114],\n",
       "           ...,\n",
       "           [0.6422, 0.5993, 0.6523,  ..., 0.6687, 0.6843, 0.5901],\n",
       "           [0.6771, 0.6661, 0.6830,  ..., 0.6930, 0.6923, 0.6140],\n",
       "           [0.6826, 0.6703, 0.5934,  ..., 0.5879, 0.5906, 0.5928]],\n",
       "\n",
       "          [[0.5898, 0.6902, 0.6115,  ..., 0.6319, 0.6307, 0.5867],\n",
       "           [0.6929, 0.6910, 0.6455,  ..., 0.5889, 0.6931, 0.6688],\n",
       "           [0.5971, 0.6183, 0.6556,  ..., 0.6867, 0.5852, 0.6470],\n",
       "           ...,\n",
       "           [0.6916, 0.5848, 0.6931,  ..., 0.5845, 0.6352, 0.6326],\n",
       "           [0.5985, 0.6126, 0.6342,  ..., 0.5885, 0.6628, 0.6694],\n",
       "           [0.6616, 0.5874, 0.6902,  ..., 0.6914, 0.5824, 0.6429]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.5849, 0.6824, 0.6398,  ..., 0.6925, 0.5851, 0.6193],\n",
       "           [0.6486, 0.6538, 0.6370,  ..., 0.5974, 0.5863, 0.5874],\n",
       "           [0.6644, 0.6273, 0.6897,  ..., 0.6931, 0.6543, 0.6923],\n",
       "           ...,\n",
       "           [0.5937, 0.5910, 0.5931,  ..., 0.5971, 0.6166, 0.6118],\n",
       "           [0.6279, 0.6186, 0.6133,  ..., 0.6931, 0.6806, 0.6222],\n",
       "           [0.5908, 0.6592, 0.5969,  ..., 0.6187, 0.5842, 0.6287]],\n",
       "\n",
       "          [[0.6590, 0.6526, 0.6748,  ..., 0.6496, 0.6021, 0.6438],\n",
       "           [0.6215, 0.6263, 0.5846,  ..., 0.6374, 0.6169, 0.6922],\n",
       "           [0.6508, 0.6902, 0.5964,  ..., 0.6911, 0.5923, 0.6918],\n",
       "           ...,\n",
       "           [0.6745, 0.5923, 0.6928,  ..., 0.6427, 0.6784, 0.5849],\n",
       "           [0.6259, 0.6849, 0.6028,  ..., 0.6384, 0.6668, 0.5961],\n",
       "           [0.6909, 0.6863, 0.6065,  ..., 0.6846, 0.6912, 0.5850]],\n",
       "\n",
       "          [[0.6042, 0.6863, 0.6927,  ..., 0.6074, 0.6581, 0.6274],\n",
       "           [0.5917, 0.6770, 0.5928,  ..., 0.5930, 0.6590, 0.6769],\n",
       "           [0.6879, 0.6040, 0.6930,  ..., 0.5832, 0.5929, 0.6698],\n",
       "           ...,\n",
       "           [0.5860, 0.6788, 0.6102,  ..., 0.6906, 0.6753, 0.6898],\n",
       "           [0.6824, 0.6556, 0.6705,  ..., 0.6693, 0.6064, 0.5991],\n",
       "           [0.6931, 0.6062, 0.6151,  ..., 0.6450, 0.5824, 0.5943]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.6556, 0.5952, 0.6365,  ..., 0.6844, 0.6450, 0.6272],\n",
       "           [0.6318, 0.6071, 0.6326,  ..., 0.5876, 0.5824, 0.6638],\n",
       "           [0.5935, 0.6402, 0.6638,  ..., 0.5952, 0.6104, 0.5866],\n",
       "           ...,\n",
       "           [0.6497, 0.5930, 0.6266,  ..., 0.6771, 0.6919, 0.6688],\n",
       "           [0.6890, 0.6931, 0.6023,  ..., 0.5992, 0.6381, 0.5849],\n",
       "           [0.6040, 0.6803, 0.6530,  ..., 0.5919, 0.5824, 0.5833]],\n",
       "\n",
       "          [[0.6931, 0.6901, 0.6338,  ..., 0.6299, 0.6521, 0.5961],\n",
       "           [0.6111, 0.6535, 0.5951,  ..., 0.5921, 0.6063, 0.5830],\n",
       "           [0.6176, 0.6301, 0.6811,  ..., 0.5972, 0.6444, 0.6907],\n",
       "           ...,\n",
       "           [0.6447, 0.5853, 0.6019,  ..., 0.5823, 0.5927, 0.6265],\n",
       "           [0.5833, 0.6931, 0.6416,  ..., 0.6205, 0.6087, 0.6322],\n",
       "           [0.5922, 0.6825, 0.6066,  ..., 0.6495, 0.6146, 0.6868]],\n",
       "\n",
       "          [[0.6922, 0.5828, 0.6931,  ..., 0.6103, 0.6916, 0.6715],\n",
       "           [0.5906, 0.6153, 0.6785,  ..., 0.5835, 0.6131, 0.6888],\n",
       "           [0.6924, 0.6731, 0.5866,  ..., 0.5825, 0.5851, 0.6004],\n",
       "           ...,\n",
       "           [0.5938, 0.6510, 0.6747,  ..., 0.6111, 0.5869, 0.6621],\n",
       "           [0.6008, 0.5826, 0.6018,  ..., 0.6852, 0.5951, 0.5828],\n",
       "           [0.5841, 0.6186, 0.5947,  ..., 0.6650, 0.5967, 0.5857]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[0.6274, 0.6475, 0.6419,  ..., 0.6171, 0.6832, 0.6931],\n",
       "           [0.5932, 0.6661, 0.6158,  ..., 0.6037, 0.5916, 0.6068],\n",
       "           [0.5941, 0.5830, 0.6043,  ..., 0.6131, 0.6920, 0.6663],\n",
       "           ...,\n",
       "           [0.6825, 0.6196, 0.5828,  ..., 0.5827, 0.5850, 0.6252],\n",
       "           [0.6931, 0.5926, 0.6341,  ..., 0.5853, 0.5835, 0.6694],\n",
       "           [0.6380, 0.6597, 0.6910,  ..., 0.6141, 0.6458, 0.6450]],\n",
       "\n",
       "          [[0.6861, 0.6069, 0.5875,  ..., 0.5886, 0.6140, 0.5981],\n",
       "           [0.5977, 0.6163, 0.6090,  ..., 0.6180, 0.6061, 0.6648],\n",
       "           [0.5940, 0.6324, 0.5955,  ..., 0.6807, 0.5839, 0.6402],\n",
       "           ...,\n",
       "           [0.5912, 0.6921, 0.6807,  ..., 0.6230, 0.6713, 0.5916],\n",
       "           [0.6924, 0.5863, 0.6267,  ..., 0.6440, 0.6282, 0.6889],\n",
       "           [0.6804, 0.5826, 0.6101,  ..., 0.5852, 0.5990, 0.5876]],\n",
       "\n",
       "          [[0.6096, 0.5872, 0.5971,  ..., 0.6134, 0.6671, 0.6554],\n",
       "           [0.6737, 0.6224, 0.6197,  ..., 0.6612, 0.5852, 0.5885],\n",
       "           [0.6902, 0.6093, 0.6055,  ..., 0.6701, 0.5924, 0.6660],\n",
       "           ...,\n",
       "           [0.6060, 0.6927, 0.6691,  ..., 0.6231, 0.5963, 0.6213],\n",
       "           [0.6918, 0.6428, 0.5834,  ..., 0.6162, 0.6356, 0.6278],\n",
       "           [0.6486, 0.5920, 0.5879,  ..., 0.6620, 0.6926, 0.5958]],\n",
       "\n",
       "          ...,\n",
       "\n",
       "          [[0.5887, 0.6036, 0.6746,  ..., 0.5899, 0.6842, 0.5983],\n",
       "           [0.6458, 0.6684, 0.6304,  ..., 0.6302, 0.6551, 0.5961],\n",
       "           [0.6906, 0.6075, 0.6097,  ..., 0.6178, 0.6931, 0.5886],\n",
       "           ...,\n",
       "           [0.6852, 0.6045, 0.5826,  ..., 0.6920, 0.6409, 0.6755],\n",
       "           [0.6315, 0.6635, 0.6456,  ..., 0.6915, 0.5825, 0.5962],\n",
       "           [0.6560, 0.6930, 0.5932,  ..., 0.5846, 0.6036, 0.5855]],\n",
       "\n",
       "          [[0.6597, 0.6924, 0.5874,  ..., 0.6423, 0.5880, 0.6931],\n",
       "           [0.5952, 0.6059, 0.5992,  ..., 0.6462, 0.5832, 0.5925],\n",
       "           [0.6691, 0.5860, 0.5854,  ..., 0.6023, 0.6311, 0.5863],\n",
       "           ...,\n",
       "           [0.6118, 0.6013, 0.6402,  ..., 0.6918, 0.5833, 0.6511],\n",
       "           [0.6163, 0.5826, 0.6881,  ..., 0.6716, 0.6138, 0.6930],\n",
       "           [0.6931, 0.6292, 0.6931,  ..., 0.5874, 0.5824, 0.5828]],\n",
       "\n",
       "          [[0.6220, 0.6911, 0.6292,  ..., 0.6265, 0.6887, 0.6008],\n",
       "           [0.5841, 0.6924, 0.5948,  ..., 0.6917, 0.5854, 0.6713],\n",
       "           [0.6863, 0.6430, 0.6923,  ..., 0.6881, 0.6882, 0.6589],\n",
       "           ...,\n",
       "           [0.5913, 0.6015, 0.6676,  ..., 0.6555, 0.6841, 0.6892],\n",
       "           [0.6796, 0.6902, 0.6323,  ..., 0.6265, 0.6659, 0.6903],\n",
       "           [0.6146, 0.6019, 0.6894,  ..., 0.6686, 0.6566, 0.6483]]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consistency_weight = 0.1 * ramps.sigmoid_rampup(100//150, 40)\n",
    "\n",
    "\n",
    "consistency_dist = consistency_criterion(outputs[labeled_bs:], ema_output) #(batch, 2, 112,112,80)\n",
    "threshold = (0.75+0.25*ramps.sigmoid_rampup(iter_num, max_iterations))*np.log(2)\n",
    "mask = (uncertainty<threshold).float()\n",
    "consistency_dist = torch.sum(mask*consistency_dist)/(2*torch.sum(mask)+1e-16)\n",
    "consistency_loss = consistency_weight * consistency_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_236047/2378503869.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmonai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDiceLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtarget_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mself\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiceLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'B' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from monai.losses import DiceLoss\n",
    "input = torch.rand(B, C, H, W)\n",
    "target_idx = torch.randint(low=0, high=C - 1, size=(B, H, W)).long()\n",
    "target = one_hot(target_idx[:, None, ...], num_classes=C)\n",
    "self = DiceLoss(reduction='none')\n",
    "loss = self(input, target)\n",
    "assert np.broadcast_shapes(loss.shape, input.shape) == input.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic trick to reload library in jupyter: import utils as nash\n",
    "from importlib import reload\n",
    "reload(nash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/FDG-PET-CT-Lesions_nifti/'\n",
    "\n",
    "patients = list(np.load(os.path.join(data_path+'positive_patients.npy')))\n",
    "patients_train, patients_test = train_test_split(patients, test_size = 0.1, random_state=0)\n",
    "\n",
    "all_paths_train= []\n",
    "all_paths_test = []\n",
    "for patient in patients_train:\n",
    "    examens = os.listdir(os.path.join(data_path,patient))\n",
    "    for exam in examens:\n",
    "        all_paths_train.append(os.path.join(os.path.join(data_path,patient), exam))\n",
    "        \n",
    "for patient in patients_test:\n",
    "    examens = os.listdir(os.path.join(data_path,patient))\n",
    "    for exam in examens:\n",
    "        all_paths_test.append(os.path.join(os.path.join(data_path,patient), exam))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 489 subjects\n"
     ]
    }
   ],
   "source": [
    "subjects_train = []\n",
    "for path in all_paths_train:\n",
    "    subject = tio.Subject(\n",
    "        ct=tio.ScalarImage(os.path.join(path, 'CTres.nii.gz') ),\n",
    "        pet=tio.ScalarImage(os.path.join(path, 'SUV.nii.gz') ),\n",
    "        segmentation=tio.LabelMap(os.path.join(path, 'SEG.nii.gz') ),\n",
    "    )\n",
    "    subjects_train.append(subject)\n",
    "dataset_train = tio.SubjectsDataset(subjects_train)\n",
    "print('Dataset size:', len(dataset_train), 'subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 54 subjects\n"
     ]
    }
   ],
   "source": [
    "subjects_test = []\n",
    "for path in all_paths_test:\n",
    "    subject = tio.Subject(\n",
    "        ct=tio.ScalarImage(os.path.join(path, 'CTres.nii.gz') ),\n",
    "        pet=tio.ScalarImage(os.path.join(path, 'SUV.nii.gz') ),\n",
    "        segmentation=tio.LabelMap(os.path.join(path, 'SEG.nii.gz') ),\n",
    "    )\n",
    "    subjects_test.append(subject)\n",
    "dataset_test = tio.SubjectsDataset(subjects_test)\n",
    "print('Dataset size:', len(dataset_test), 'subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labelled set: 48 subjects \t Training unlabelled set: 441 subjects\n",
      "Validation set: 54 subjects\n"
     ]
    }
   ],
   "source": [
    "training_transform = tio.Compose([\n",
    "    tio.ToCanonical(),\n",
    "    tio.OneHot(num_classes=2)])\n",
    "\n",
    "validation_transform = tio.Compose([\n",
    "    tio.ToCanonical(),\n",
    "    tio.Resample(4),\n",
    "    tio.OneHot(num_classes=2)\n",
    "    ])\n",
    "\n",
    "labelled_split_ratio = 0.1\n",
    "n_train = len(dataset_train)\n",
    "nl = int(labelled_split_ratio * n_train)\n",
    "nu = n_train - nl\n",
    "\n",
    "num_split_subjects = nl, nu\n",
    "labelled_subjects, unlabelled_subjects = torch.utils.data.random_split(subjects_train, num_split_subjects)\n",
    "\n",
    "training_labelled_set = tio.SubjectsDataset(\n",
    "    labelled_subjects, transform=training_transform)\n",
    "\n",
    "training_unlabelled_set = tio.SubjectsDataset(\n",
    "    unlabelled_subjects, transform=training_transform)\n",
    "\n",
    "\n",
    "validation_set = tio.SubjectsDataset(\n",
    "    subjects_test, transform=validation_transform)\n",
    "\n",
    "print('Training labelled set:', len(training_labelled_set), 'subjects \\t Training unlabelled set:', len(training_unlabelled_set), 'subjects')\n",
    "print('Validation set:', len(validation_set), 'subjects')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_batch_size = 1\n",
    "validation_batch_size = 2 * training_batch_size\n",
    "\n",
    "patch_size = (128,128,30)\n",
    "samples_per_volume = 1\n",
    "max_queue_length = 1\n",
    "sampler = tio.data.UniformSampler(patch_size)\n",
    "\n",
    "patches_training_labelled_set = tio.Queue(\n",
    "    subjects_dataset=training_labelled_set,\n",
    "    max_length=max_queue_length,\n",
    "    samples_per_volume=samples_per_volume,\n",
    "    sampler=sampler,\n",
    "    num_workers=num_workers,\n",
    "    shuffle_subjects=True,\n",
    "    shuffle_patches=True,\n",
    ")\n",
    "\n",
    "patches_training_unlabelled_set = tio.Queue(\n",
    "    subjects_dataset=training_unlabelled_set,\n",
    "    max_length=max_queue_length,\n",
    "    samples_per_volume=samples_per_volume,\n",
    "    sampler=sampler,\n",
    "    num_workers=num_workers,\n",
    "    shuffle_subjects=True,\n",
    "    shuffle_patches=True,\n",
    ")\n",
    "\n",
    "patches_validation_set = tio.Queue(\n",
    "    subjects_dataset=validation_set,\n",
    "    max_length=max_queue_length,\n",
    "    samples_per_volume=samples_per_volume,\n",
    "    sampler=sampler,\n",
    "    num_workers=num_workers,\n",
    "    shuffle_subjects=False,\n",
    "    shuffle_patches=False,\n",
    ")\n",
    "\n",
    "training_labelled_loader_patches = torch.utils.data.DataLoader(\n",
    "    patches_training_labelled_set, batch_size=training_batch_size)\n",
    "\n",
    "training_unlabelled_loader_patches = torch.utils.data.DataLoader(\n",
    "    patches_training_unlabelled_set, batch_size=training_batch_size)\n",
    "\n",
    "validation_loader_patches = torch.utils.data.DataLoader(\n",
    "    patches_validation_set, batch_size=validation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch = next(iter(training_labelled_loader_patches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'one_batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_296619/2578239744.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'segmentation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'one_batch' is not defined"
     ]
    }
   ],
   "source": [
    "one_batch['segmentation']['data'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import prepare_batch, get_dice_score, get_dice_loss, get_model_and_optimizer, run_epoch, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(enum.Enum):\n",
    "    TRAIN = 'Training'\n",
    "    VALIDATE = 'Validation'\n",
    "\n",
    "CHANNELS_DIMENSION = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "unet = monai.networks.nets.UNet(\n",
    "    spatial_dims=3,\n",
    "    in_channels=2,\n",
    "    out_channels=2,\n",
    "    channels=(4, 8, 16),\n",
    "    strides=(1, 1),\n",
    ")\n",
    "unet = unet.to(device)\n",
    "\n",
    "learning_rate=1e-2\n",
    "optimizer = torch.optim.AdamW(unet.parameters(), lr=learning_rate)\n",
    "loss_function = monai.losses.DiceCELoss(softmax=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018088102340698242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 455,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf8852bfcb84d17b5e93612b2a92c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.339525818824768\n",
      "0.8386653661727905\n",
      "0.6340301632881165\n",
      "0.5822932720184326\n",
      "0.5214874148368835\n",
      "0.5083507895469666\n",
      "0.5064225196838379\n",
      "0.5090855360031128\n",
      "0.50334233045578\n",
      "0.5215180516242981\n",
      "0.5010581612586975\n",
      "0.5000287890434265\n",
      "0.5000192523002625\n",
      "0.5914199352264404\n",
      "0.5000523328781128\n",
      "0.49980300664901733\n",
      "0.500038206577301\n",
      "0.5000171661376953\n",
      "0.49922457337379456\n",
      "0.4952628016471863\n",
      "0.49142563343048096\n",
      "0.4682731330394745\n",
      "0.4998035132884979\n",
      "0.49993032217025757\n",
      "0.3426441252231598\n",
      "0.4995730221271515\n",
      "0.4985935389995575\n",
      "0.49533960223197937\n",
      "0.25100409984588623\n",
      "0.03877878189086914\n",
      "0.011460274457931519\n",
      "0.01946359872817993\n",
      "0.0007806122303009033\n",
      "0.0005221366882324219\n",
      "0.0001512467861175537\n",
      "1.233816146850586e-05\n",
      "5.602836608886719e-06\n",
      "2.0772218704223633e-05\n",
      "0.0\n",
      "5.960464477539063e-08\n",
      "3.2782554626464844e-07\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5039191246032715\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.1045666933059692\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0360136032104492\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "10.723427772521973\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.3185423612594604\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.9636882543563843\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "8.331445693969727\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.5456047058105469\n",
      "0.0\n",
      "84.53878784179688\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "89.6466293334961\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.904725193977356\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "26.899808883666992\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.47677329182624817\n",
      "0.0\n",
      "0.7613092660903931\n",
      "0.0\n",
      "0.0\n",
      "0.4977451264858246\n",
      "0.4998355507850647\n",
      "0.32992619276046753\n",
      "0.5017443299293518\n",
      "5.279759883880615\n",
      "0.8623708486557007\n",
      "5.742029190063477\n",
      "4.304589748382568\n",
      "1.7624526023864746\n",
      "16.47402572631836\n",
      "0.5042043328285217\n",
      "0.5633251070976257\n",
      "0.5079419612884521\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "4.682466983795166\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "for batch_idx, batch in enumerate(tqdm(training_labelled_loader_patches)):\n",
    "    inputs, targets = prepare_batch(batch, device)\n",
    "    logits = unet(inputs)\n",
    "    batch_loss = loss_function(logits, targets)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    print(batch_loss.item())\n",
    "    epoch_losses.append(batch_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02100682258605957,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 455,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af9797f94e84dc6acaf47a23dccd1da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.6248313784599304\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.7487013339996338\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "54.669124603271484\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "8.617166519165039\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "9.288957595825195\n",
      "0.0\n",
      "1.0173509120941162\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "34.76984786987305\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "3.0428013801574707\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "9.69432258605957\n",
      "0.0\n",
      "0.0\n",
      "19.74350929260254\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "86.4998550415039\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.1308643817901611\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5002224445343018\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.4999990165233612\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.38589394092559814\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.036817699670791626\n",
      "0.0\n",
      "0.0\n",
      "0.5063086748123169\n",
      "0.501122236251831\n",
      "0.0\n",
      "0.5265758633613586\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5008740425109863\n",
      "0.0\n",
      "0.0\n",
      "0.5016178488731384\n",
      "31.267986297607422\n",
      "1.8647372722625732\n",
      "0.0\n",
      "0.5009209513664246\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.4981011748313904\n",
      "0.5009559988975525\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5959213972091675\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5000317096710205\n",
      "0.0\n",
      "0.0\n",
      "1.126568078994751\n",
      "0.501762330532074\n",
      "0.0\n",
      "0.0\n",
      "0.8098269104957581\n",
      "0.0\n",
      "0.0\n",
      "13.922237396240234\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5097173452377319\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5107617974281311\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "4.094592094421387\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5012751817703247\n",
      "1.3536425828933716\n",
      "0.0\n",
      "0.4996713101863861\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5477495193481445\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5224961042404175\n",
      "0.0\n",
      "0.5038085579872131\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5030964612960815\n",
      "0.6803056001663208\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.7881393432617188e-07\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.5133772492408752\n",
      "0.0\n",
      "0.0\n",
      "0.5002917051315308\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = []\n",
    "for batch_idx, batch in enumerate(tqdm(training_labelled_loader_patches)):\n",
    "    inputs, targets = prepare_batch(batch, device)\n",
    "    logits = unet(inputs)\n",
    "    batch_loss = loss_function(logits, targets)\n",
    "    batch_loss.backward()\n",
    "    optimizer.step()\n",
    "    print(batch_loss.item())\n",
    "    epoch_losses.append(batch_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efcb02c38b0>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuIAAAHwCAYAAADjFQoyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAABYlAAAWJQFJUiTwAAA8HUlEQVR4nO3debhkVX32/fvXI91ND3QzKmgDCrQSjd0oCL4IaIziGMWE93kcYoJRo6KoMU9EI4+JCRoJk1OCAwajYPBFxRBAaCYBGXqQqUe6m6ahm57nPvN6/6h9mjq7VnXXObWr1lq7vp/rOled2rWrap2qXafuvfZvrW3OOQEAAABor1GhGwAAAAB0IoI4AAAAEABBHAAAAAiAIA4AAAAEQBAHAAAAAiCIAwAAAAEQxAEAAIAACOIAAABAAARxAAAAIACCOAAAABAAQRwAAAAIgCAOAAAABDAmdANawcxWSpoiaVXgpgAAAKDcZkra7pw7erh3LGUQlzRlwoQJ02fNmjU9dEMAAABQXosWLdKePXtGdN+yBvFVs2bNmj5v3rzQ7QAAAECJzZkzR/Pnz181kvtSIw4AAAAEQBAHAAAAAiCIAwAAAAEQxAEAAIAACOIAAABAAARxAAAAIACCOAAAABAAQRwAAAAIgCAOAAAABEAQBwAAAAIgiAMAAAABEMQBAACAAAjiAAAAQAAEcQAAACAAgjgAAAAQAEEcAErCORe6CQCAYSCIA0AJ3LlkvV77z3P1iZ/MJ5ADQCII4gBQAn/+w4e0bnuXfv3IWt22aH3o5gAAGkAQB4CSWb5+Z+gmAAAaQBAHAAAAAiCIAwAAAAEQxAEAAIAACOIAAABAAARxAAAAIACCOAAAABAAQRwAAAAIgCAOAAAABEAQBwAAAAIgiANAyTi50E0AADSAIA4AAAAEQBAHgJIxWegmAAAaQBAHAAAAAiCIAwAAAAEQxAEAAIAACOIAAABAAARxAAAAIACCOAAAABAAQRwAAAAIgCAOAAAABEAQBwAAAAIgiAMAAAABFBbEzeytZnarma0xsz1mtsLM/svMXltn/VPN7CYz25yt/4iZfdrMRhfVJgAAACBWhQRxM/uapF9Lmi3pZkmXS5ov6Z2S7jWz9+XWf6ekuyWdLukGSd+UNE7SpZKuLaJNAAAAQMzGNPsAZna4pM9Jek7SK5xz66tuO1PSXElfkfTjbNkUSVdJ6pd0hnPu4Wz5l7J1zzGzc51zBHIAGAEnF7oJAIAGFNEj/uLscR6oDuGS5Jy7Q9IOSYdULT4nu37tYAjP1u2S9MXs6scKaBcAAAAQrSKC+DJJPZJeY2YHV99gZqdLmizptqrFZ2WXN3se625JuyWdambjC2gbAHQck4VuAgCgAU2XpjjnNpvZ30r6V0lPmNkvJG2SdKykd0j6jaSPVN3l+Oxyqeex+sxspaSXSzpG0qJ9PbeZzatz0wnD+RsAAACAdms6iEuSc+4yM1sl6QeSPlx103JJV+dKVqZml9vqPNzg8mlFtA0AAACIUVGzpnxe0vWSrlalJ3ySpDmSVkj6TzP7ehHPk+ecm+P7kbS4Fc8HAAAAFKXpIG5mZ0j6mqRfOec+45xb4Zzb7ZybL+lPJD0j6bNmdkx2l8Ee76k1DzZ0+dZm2wYAAADEqoge8bdll3fkb3DO7Zb0YPY8r8oWL8kuj8uvb2ZjJB0tqU+V3nQAAACglIoI4oOzmxxS5/bB5T3Z5dzs8s2edU+XNFHSfc657gLaBgAAAESpiCB+T3b5V2b2wuobzOwtkk6T1CXpvmzx9ZI2SjrXzE6qWvcASf+YXf1OAe0CAAAAolXErCnXqzJP+BslLTKzGyStkzRLlbIVk/R/nHObJMk5t93MPpzd704zu1bSZlWmOjw+W35dAe0CAAAAolXEPOIDZna2pI9LOleVAZoTVQnXN0m6wjl3a+4+vzCz10u6UNJ7JB2gylSHn8nW5/zMAAAAKLWi5hHvlXRZ9tPofe6VdHYRzw8AAACkppB5xAEAAAAMD0EcAAAACIAgDgAAAARAEAcAAAACIIgDAAAAARDEAQAAgAAI4gAAAEAABHEAAAAgAII4AAAAEABBHAAAAAiAIA4AJePkQjcBANAAgjgAAAAQAEEcAErGZKGbAABoAEEcAAAACIAgDgAAAARAEAcAAAACIIgDAAAAARDEAQAAgAAI4gAAAEAABHEAAAAgAII4AAAAEABBHAAAAAiAIA4AAAAEQBAHAAAAAiCIAwAAAAEQxAGgZJxc6CYAABpAEAcAAAACIIgDQMmYLHQTAAANIIgDAAAAARDEAQAAgAAI4gCQOOeGDs5ksCYApIEgDgCJy+XwmusAgDgRxAEgceRuAEgTQRwAAAAIgCAOAImrqRGnNgUAkkAQB4DE5WM3ORwA0kAQB4DE1QzWDNMMAMAwEcQBIHH56QrpEQeANBDEASBxtT3iJHEASAFBHABKhh5xAEgDQRwAEkeNOACkiSAOAAAABEAQB4DE1dSEU5sCAEkgiANA4ihNAYA0EcQBIHGc0AcA0kQQB4DE1Zzinj5xAEgCQRwAEkePOACkiSAOAImjRhwA0kQQBwAAAAIgiANA6pi9EACSRBAHgMTlB2cyWBMA0kAQB4DE1fSAk8MBIAkEcQBIHDkcANJEEAeAxNXMI06ROAAkgSAOAIljHnEASBNBHAASxzziAJAmgjgAAAAQAEEcABJXM30hXeIAkASCOACkrqY0hSQOACkgiANA4hisCQBpIogDQOII3gCQJoI4ACSutkacZA4AKSCIA0DimL4QANJEEAeAxFEjDgBpIogDAAAAARDEASBx+Zpwpi8EgDQQxAEgcTU14uRwAEgCQRwASoYcDgBpIIgDQOLoEQeANBHEASBxtTXhJHEASAFBHAASR484AKSJIA4AAAAEQBAHgMRxQh8ASBNBHAASxzziAJAmgjgAJI4ecQBIE0EcABJXM1gzTDMAAMNEEAeA5OVKU0jiAJCEQoO4mb3BzG4ws3Vm1m1mz5rZLWZ2tmfdU83sJjPbbGZ7zOwRM/u0mY0usk0AUHa1PeIkcQBIwZiiHsjMvi7pbyStkfQrSRslHSJpjqQzJN1Ute47Jf1cUpek6yRtlvR2SZdKOk3Se4tqFwCUXU3sJocDQBIKCeJm9mFVQviPJP2Vc64nd/vYqt+nSLpKUr+kM5xzD2fLvyRprqRzzOxc59y1RbQNAAAAiFHTpSlmNl7SVyWtlieES5Jzrrfq6jmq9JRfOxjCs3W6JH0xu/qxZtsFAJ2CwZoAkKYiesT/SJVgfZmkATN7q6QTVSk7edA5d39u/bOyy5s9j3W3pN2STjWz8c657gLaBwCllq8Jz88rDgCIUxFB/NXZZZekBaqE8L3M7G5J5zjnNmSLjs8ul+YfyDnXZ2YrJb1c0jGSFu3ric1sXp2bTmis6QCQPnrEASBNRcyacmh2+Teq/P//fyRNlvQKSbdKOl3Sf1WtPzW73Fbn8QaXTyugbQBQejVBnCQOAEkookd8MMz3SXqHc25Vdv1RM/sTSUskvd7MXuspU2mKc26Ob3nWUz67yOcCgFjVlKYEagcAYHiK6BHfml0uqArhkiTn3G5Jt2RXX5NdDvZ4T5Xf4PKtdW4HAFSp7REnigNACooI4kuyy611bt+SXU7IrX9cfkUzGyPpaFV611cU0DYAAAAgSkUE8dtVORL6MjPzPd7g4M2V2eXc7PLNnnVPlzRR0n3MmAIAI0N/OACkoekg7px7StKNkl4k6VPVt5nZmyT9sSq95YPTFV6vylk3zzWzk6rWPUDSP2ZXv9NsuwCgU9RUopDEASAJRZ3i/uOSXiXpX7N5xBeoUmLyLlXOoHmec26bJDnntmdn4rxe0p1mdq0qp7h/hypTG16vymnvAQANqB2sSRIHgBQUUZoi59waSXMkfVPSS1XpGT9DlZ7y05xzP8+t/wtJr1flBD7vkfRJSb2SPiPpXMdIIwBoGNMXAkCaiuoRV3bCnk9mP42sf6+ks4t6fgDoVDWVKQRxAEhCIT3iAIBw8gcRKU0BgDQQxAEgcfSIA0CaCOIAAABAAARxAEhczWDNMM0AAAwTQRwAkperESeJA0ASCOIAkLja4E0SB4AUEMQBIHEM1gSANBHEASBx1IgDQJoI4gCQuJp5xOkSB4AkEMQBIHE1pSlBWgEAGC6COAAAABAAQRwAEldTI06XOAAkgSAOAIlz+XnEA7UDADA8BHFEo6dvQDcsWKM7Fq9nsBkwHDU94nx+ACAFBHFE47qHVuuC636vD139kOav3hK6OUAyiN0AkCaCOKLxpV8+vvf3C294LGBLgLRQIw4AaSKIA0DiamvESeIAkAKCOAAAABAAQRwAEkdpCgCkiSAOAImrObMmQRwAkkAQB4DE5acrpEYcANJAEAeAxNEjDgBpIogDQOryNeJhWgEAGCaCOAAkrqYUhSQOAEkgiANA4mpmTSGJA0ASCOIAAABAAARxAEgc84gDQJoI4gCQuJpZU4K0AgAwXARxAEhczTzidIkDQBII4gCQOHrEASBNBHEASBw14gCQJoI4ACQvf4p7AEAKCOIAAABAAARxAEhcTSkKtSkAkASCOAAkjsGaAJAmgjgAJI7BmgCQJoI4ACTO1QzWJIkDQAoI4gCQOHrEASBNBHEASBxjNQEgTQRxAEhczSnuA7UDADA8BHEAAAAgAII4AJRMvoccABAngjgAJI7cDQBpIogDQOJqpi8kmANAEgjiAJC4mukLGa4JAEkgiANA4phHHADSRBAHgMTVzCMepBUAgOEiiANA4mrmEadLHACSQBAHAAAAAiCIA0DiKE0BgDQRxAEgdSRxAEgSQRwAElczj3igdgAAhocgDgCJq52+kCgOACkgiANA4qhMAYA0EcQBIHGc0AcA0kQQBwAAAAIgiANA4moHa9IlDgApIIgDQOIoTQGANBHEASBxNYM1CeIAkASCOACkjuQNAEkiiANA4mp7xAnmAJACgjgAJK6mRjxMMwAAw0QQB4DE5XvA6RAHgDQQxAEAAIAACOIAkLjaU9zTJQ4AKSCIA0DimEccANJEEAeAxNX2iAMAUkAQB4DEMVgTANJEEAeA0iGJA0AKCOIAkDhqxAEgTQRxAAAAIACCOAAkLj9dIR3iAJAGgjgAJK62NIUoDgApIIgDQOKYvhAA0kQQB4DEMVgTANJEEAeAxNXUiJPEASAJLQniZvY+M3PZz3l11nmbmd1pZtvMbKeZPWBmH2xFewCgzGp6xMM0AwAwTIUHcTM7StI3Je3cxzqfkHSjpBMl/VjSVZJeIOlqM/tG0W0CgI5CEgeAJBQaxM3MJP1Q0iZJ362zzkxJ35C0WdJJzrmPO+cukPQKSU9K+qyZvbbIdgEAAACxKbpH/HxJZ0n6kKRdddb5C0njJX3TObdqcKFzboukf8qufrTgdgFAaeVrwukQB4A0FBbEzWyWpIslXe6cu3sfq56VXd7sue1/cusAAPaDecQBIE1jingQMxsj6RpJqyV9YT+rH59dLs3f4Jxba2a7JB1pZhOdc7v387zz6tx0wn7aAAClwTziAJCmQoK4pL+X9CpJr3PO7dnPulOzy211bt8maVK23j6DOACAecQBIFVNB3EzO1mVXvBLnHP3N9+kxjnn5tRp0zxJs9vZFgAIpWYecfrEASAJTdWIZyUp/6FKmcmXGrzbYE/41Dq376/HHABQhR5xAEhTs4M1D5R0nKRZkrqqTuLjJH05W+eqbNll2fUl2eVx+QczsyNUKUtZs7/6cABABTXiAJCmZktTuiV9v85ts1WpG/+tKuF7sGxlrqTTJL25atmgt1StAwAAAJRWU0E8G5hZ7xT2F6kSxH/knPte1U0/lPR5SZ8wsx8OziVuZgfp+RlXvCcDAgB4cI57AEhSUbOmNMw5t9LM/kbSFZIeNrPrJPVIOkfSkQow6BMAUlZbmkISB4AUtD2IS5Jz7kozWyXpc5I+oEqt+hOSvuic+1GINgFAqhisCQBpalkQd85dJOmifdx+o6QbW/X8ANApaqcvBACkoLBT3APN4JTcwMhxinsASBNBHFHg0DowckxfCABpIogjCgO55J2/DgAAUDYEcUSBHj1g5DiiBABpIogjCtS4AiPHdIUAkCaCOKKQL0UhhwPD4Pm8sDMLAPEjiCNKRAigcb7PCzkcAOJHEEcUGKwJjJyv95tPEADEjyCOKDDYDBg53+eF0hQAiB9BHFGonTWFEAE0ylua0vZWAACGiyCOKNSUpgwEaggAAECbEMQRBY6iAyPnL01pfzsAAMNDEEccmEccGDFfKRflXQAQP4I4olA7a0qghqAwdy/doKvuXqFte3pDN6X06BEHgDSNCd0AQGKwZtms3LhLH/jBg5KkFRt36Z/f/QeBWwQAQHzoEUcU8qUo9Oal7Uf3rdr7+08fXB2uIR3CO484nyEAiB5BHFHIl6JQmgI0zj99IR8iAIgdQRxRqA0NhAgAAFBuBHFEIX8YnR5xoHEM1gSANBHEEYXaU9yTIoBG+acvBADEjiCOKOSDBCECaJy/R5xPEQDEjiCOKNQM1qQ2BWiYf7AmACB2BHFEoWb6wkDtAFJEjTgApIkgjijUhAZCBDAMviTe/lYAAIaHII4o1M6aQooAGuXtESeJA0D0COKIAoM1AQBApyGIIwq1Z9YkigONokYcANJEEEcUagZrEiKAhjGPOACkiSCOKDBWExg55hEHgDQRxBGF2h5xQgTQKOYRR94tj6/T1feu1O6evtBNAbAPY0I3AJB8p7gP0w4gRdSIo9r81Vv0kWvmSZK27O7VBX90XOAWAaiHHnFEgdIUYOT8NeJ8ijrVN25Zsvf3y29fFrAlAPaHII4o5GdJYdYUYBioTQGAJBHEEQVKUwAAQKchiCMKvh5wBmwCjaFDHADSRBBHFBhsBoycb6eVzw8AxI8gjmiRI4DG+HvE+QQBQOwI4oiCrzSFAZtAYziihGpmoVsAoFEEcUSBIAGMHDXiAJAmgjiiwKF1YOT8NeJ8fgAgdgRxRME/a0qAhgAAALQJQRxRoDQFGDnvESU+PwAQPYI4ouA7jM5gTaBBfFQAIEkEcUSBwWbAyPnGU7AfCwDxI4gjCv7SFJIE0Ajv54ddWQCIHkEcUfDPIx6gIUCCGGMBAGkiiCMK3tBAkAAa4i1NCdAOxMHEGX2AVBDEEQV/kCBKAI2gtAsA0kQQRxR8mYHSlPIgFAIAUIsgjijQo1cu+feOt7K1qOwCgDQRxBEFBmuWS/69Y0741mKwJgCkiSCOKPh79EgSqerPpcD8dRSNPnEASBFBHFHwlqGQI5JFaUp70SMOAGkiiCMKDNYsl4GB3HVSYUvRH45qxuyFQDII4ogC0xeWSz54s1PVWr4jSuz7AED8COKIAofWy4XBmgAA7B9BHFHw9ZgS3tJVUyM+UGdFFILBzgCQJoI4osCh9XJh1pT24ogSAKSJII4o+HrECRLpojSlvbw94rzkABA9gjgiwWDNMqkdrMl72UreI0p8fgAgegRxRIFD6+XCPOLh8ZoDQPwI4ogCgzXLhXnE24uXFwDSRBBHFPzziCNVzCPeXt7PD685AESPII4oUJpSLjVBnCQOAEANgjii4Ctd8A1AQxqYNaW9vDuyHFMCgOgRxBEtYkS6KE1pL44oAUCaCOKIgq/HlF7UdNEj3l6MsQCANBHEEQV69MqldvpC3sxW8n9+eM0BIHYEcUSBIF4ulKa0l/fMmm1vBWLFYGkgXgRxRIHSlHJhHvE2Y0cWVfoH8jvCbAxArAjiiAJfE+XSn/vizwcDFMs/QwqveafKB+/85xFAPAjiiAM9eqXCKe6BcPJHpPj8AfEiiCMKlKaUC7OmtBdjLFCtpkecI1JAtAjiiAIH1suFwZrtxecH1WpKw9grA6JFEEcUmH6tXOgRby/fZ4WXvHPlP39uwL8egPAI4oiCvzQlQENQCOYRby9vjzivecfKv/f0iAPxIogjCv6vCb48UpWvSe2nR66lvEeU2t8MRILpC4F0NB3EzWyGmZ1nZjeY2XIz22Nm28zst2b2l2bmfQ4zO9XMbjKzzdl9HjGzT5vZ6GbbhPT4eu/oEU8XpSnt5e8Rb3szEImazx//TIFojSngMd4r6TuS1kq6Q9JqSYdJerek70l6i5m911UlLTN7p6SfS+qSdJ2kzZLeLulSSadlj4kOwqwP5ZLfsSKIA+2TD97kcCBeRQTxpZLeIem/nXt+SIiZfUHSg5Leo0oo/3m2fIqkqyT1SzrDOfdwtvxLkuZKOsfMznXOXVtA25AI/2Azvj1SlQ/evJUt5vv8UJzSsTihD5COpktTnHNznXM3VofwbPk6Sd/Nrp5RddM5kg6RdO1gCM/W75L0xezqx5ptF9Li67GhFyddlKa0l/fV5SXvWPngTWkKEK8iesT3pTe77KtadlZ2ebNn/bsl7ZZ0qpmNd8517+vBzWxenZtOGFYrEZx/HmS+PFLFPOLtxWBNVMtvD+wIA/Fq2awpZjZG0geyq9Wh+/jscmn+Ps65PkkrVdlBOKZVbUN8vGUofHckq6ZGlSQuSVq3rUvv//4D+sg1D2tXd9/+79Ag304r2atz1c5axMYAxKqVPeIXSzpR0k3OuVuqlk/NLrfVud/g8mn7ewLn3Bzf8qynfHZjzUQMfKGB7450UZri94UbHtU9yzZKkq64fZn+7uxZhTyufz+W17xTcUQKSEdLesTN7HxJn5W0WNL7W/EcKBdvjx5BIlkEAb+5i9fv/f3Xj6wt7HGZdQjVKE0B0lF4EDezT0i6XNITks50zm3OrTLY4z1VfoPLtxbdNsSLwZrlQhBoL/8YC3QqTugDpKPQIG5mn5Z0paTHVAnh6zyrLckuj/Pcf4yko1UZ3LmiyLYhbv4ePb48UlU7fSHvZR6vCVqlZvpCejWAaBUWxM3sb1U5Ic9CVUL4+jqrzs0u3+y57XRJEyXdt78ZU1Au/tIUpIrSlPZiHn5UYx5/IB2FBPHsZDwXS5on6Q3OuY37WP16SRslnWtmJ1U9xgGS/jG7+p0i2oV00CNeLv0D+eu8l3lm1tLH5xXvXPmPG58/IF5Nz5piZh+U9BVVzpR5j6TzPV8wq5xzV0uSc267mX1YlUB+p5ldq8op7t+hytSG16ty2nt0EH+PXoCGoBCc4r69vC8vL3nHokYcSEcR0xcenV2OlvTpOuvcJenqwSvOuV+Y2eslXSjpPZIOkLRc0mckXeHoCu04zPpQLhwaby9mHUK12tIwtgUgVk0HcefcRZIuGsH97pV0drPPj3Lwz5rCl0eqmEd8/4rsb2BHFtXyJ9DKl4oBiEfLzqwJDAeDNcuFwZrt5a1M4TXvWOwIA+kgiCMKvqBGhVK6mEd8/1o9WBOdq2ZHmD1hIFoEccSBwZqlwjzi7eUd7BygHYgDR6SAdBDEEQUmfSiX/KwN1Ki2lr80hU9Qp6qZvpBtAYgWQRxR8JUuUM6QLkpT9q/QoOwr7Sru0ZEYpi8E0kEQRxSY9aFcKE1pLwZrYpDvs0aNOBAvgjiiQGlKuVCjun9FDtb07+jwonci31k0+fwB8SKIIwq+Q6f0oqaL6dPaix5xDPKFbk5xD8SLII44UJpSGt5D47yXLeUt7Wp/MxABxtsAaSGIIwp8eZSH99A4SbwGR3zQCvwvBdJCEEcUGKxZHr7MTRBoLe+ZaXnJOxKlKUBaCOKIAkPNysPfIxegIZErdrCmZxmfoI7kC93slAHxIogjChxOLQ/f28Z72VocUcIgX8kTPeJAvAjiiAKzr5UHM+DEgVe8M/mnL2RrAGJFEEe0OLSeJkpTGlPkzonvsdj56UyM0QDSQhBHFAhv5TEwULuMQ+OtxauLQfwvBdJCEEcUqHEtD0pT2o/PDwb5Pn9l2BHu7R/QY89sYypUlA5BHFFgsGZ50CPXmCJnTQEGlbE0xTmnP/u3+/W2K3+rz//8kdDNAQpFEEcUGKtZHmUMArHzziPOJ6gj+XqMU+9FXrNlj+av3ipJun7emrCNAQpGEEcU/LOmpP3l0ak4xX1jih2s2dgylJ+3NCXxbaGn3zPwBCgJgjiiQHgrD9/7Ro14a7Efi0H+E/qkvTEk3nxgnwjiiIK/R4//vinqL+lgsWblywOKfEX8Z9ZEJ+qEU9zz3YAyIYgjCv4aV6TIW6PKm1mzg1JsOCpfLyhGpoyDpfN/U9l2LNDZCOKIgn+AX/vbgeZxinu/fHgoMkzQI45BZZyBqqdvaI14b+pF70AVgjiiQGlKeTCPuF9fLnjnrwNF8J1QK/VZU/KflV7fHwkkiiCOKPhP0R2gIWhaGQ+NF6G/v4U94g0vRNn5Z01Je2Poy82a0kePOEqEII4o+OcR559tiphH3C8fhvoK7NXz7sjy+elI3h3hxPeE89MX9jKdIUqEII4o0CNeHkxF6ZcP3kUeXWf6QgzyHWlJ/fOX7wEniKNMCOKIAoM1y8N3GDz1Hrki5ANSsT3inmWFPTpS4p2+MPG9svxnhdIUlAlBHFGgNKU8vIPFEg8CRciHhwFX3A4KR5QwqJyzptAjjvIiiCMKBInyYLCmn69koKieSnZkMcg7j3/iH8B8jzjTF6JMCOKIAtMXlgfvpV9Lzzjqfc2LeWikxVsalvi2kO8Bp0ccZUIQRxS8Z9ZM/MujU5Xx0HgRvD3iqSckRMf3UUt9O8v3gBc5vgIIjSCOKPjritvfDjSP0hQ/3wCzok7q4y9NQScq4wm1amdNSfvvAaoRxBEFb484USJJZTyhSBFa2SPuDVq85h2plWMRQqE0BWVGEEcUyBHl4cuWqffIFcF3OL2oQ+z0iGOQvzSl/e0oUj54M30hymRM6AYAEuGtTPyzNgRoSARWbdylv/7P+Zo6Yaw+esaxNbcX1yPe2DKUn2+bSv1/ab4UJX+mTSBlBHFEwleagn0Z7CUaOzquA1uc4v55n7puoZ5Yu12StLunr+b2woK4d7BzZ77mnc5bGpb4II0+esRRYnF9g6Nj0aM3PCs37tKpF8/VqRfP1VObdoVuzhCc4v55v3966/O/r9lWc3tLe8QLeWSkpoyDpXtbeFZaIDSCOKLAlHfD89mfLdSGHd3asKNbf/Nfj4RuzhCUGTWuqFlTgEFlPCKVrxHv6SOIozwI4ohCjIPNYj4b3fzVW/f+/tBTm8M1xMN74prEg0CrFFea4lnGS96RfNtU6kG8pjQl4v/NwHARxBGF2EpT7lq6QSd99Ta9//sPJF9f2W5lPDTeKoXVulKagkwZa8TzgzWZvhBlQhBHFGI7CcUHf/CgNu/q0T3LNuqGBc8Ea0eKfO8bpSl+DNZE0WLr1ChC7Tziif9BQBWCOKIVy7/aZc/tCN2EpPjPkhrLuxmXokp2eHkxqJUnjgolf+QoX6oCpIwgjih4yxkS//JoFwvdgBz/exmgIQnob+UJffj4dKQyntmWM2uizAjiiALTr5VHGWdtaJWiasS95UB8gjpSbGV+RchPX0hpCsqEII4olLGusVMxFWXjmDUFRfNtUqmXpvT25WdNoUcc5UEQRxQIbyMX26vErCmNYxo2FM0/fWGAhhQoH7zpEUeZEMQRBf6tlgelKY1r5WBNXvHO5D+zbdpbA9MXoswI4ogDZ2MsDU5x37j+Fvbs8fHpTGXcEWawJsqMII4oxFTOkD+0m/qXWLuVcbBYqxRRmlLvtWWwZmfqjOkL0/57gGoEcUTBf4r7MP9se3IDg7r76H0ZDuYRr2gk/BQRkOq9tB34kkNxdWoUpZcacZQYQRxRiOnLo6u3f5/XsW/MI16R36HzKWL2ByIJqpXxnAyUpqDMCOKIQkzTF3b15YM4//SHgxlwKrr79r8DV8TrUrc0pQNfc5SzRrymNKUT9+xRWgRxRKFelWsI3b350pS4e8TjO7Omb1naQWAkGuoRL+AQe71H6MCXHKpTI574ttCT6wHv6Uv8DwKqEMQRBe9MG4E6PWLvEe/LfSkNuLgOPcdUZhRSI2MLUh9Eh/j4/5emvZ3RI44yI4gjCv55kMN8eeSDd2w14l2egJfvMQqJHvGKRo6kFDNrSp3lTT8yUuT7V5D65y/f+cCsKSgTgjii4AvdwWrE84M1I5s1pduzY5AvpwnJ1yOXeA4YkUaOpBQya0q9wq4OfM3hD92pH3npyQXvmDoegGYRxBEF/5R37W+HVFtS4Au+IflKHmKqY/cdBk+9R24kGgkLLZ2+kD7xjlTGHeF8KUq+hxxIGUEcUYhpHvF8j3hs84j7g3g8bfQdNU69R24kGjlK0crXJfXwhZHp9/WIJ74x9Pblpy9M++8BqhHEEYWYenFin0fc1/sdUxtjei9DokYcIZRxjEZv7o9iHnGUCUEcUfDPIx7H9IUxhVzJ39MaU48484hXNDZrShEn9OHUmnietzQs8SNSNYM1C/h79vT0644l67Wjq7fpxwKaMSZ0AwCpzmDNAO2QansyYwq5UgI14iXskRuJxs6s2XmvC1qrbNOH9g+4mvYX0SP+0R/P011LN+iVR07VLz5+msxiOyMDOgU94oiCP7y1vx2Sf/rCmM5S6AvdMc2aUrYgMFKN7MAV0VNJaQqq+TJqymM0fKG72ekLBwac7lq6QZL0+zXbtGU3veIIhyCOKPjriuMYrDng4hocFHtpSkxlRiG1rUa83vLOe8mh8pWG+YJ4sz3iO7r6hlzfvocgjnAI4oiCf9aUMGIv/Yi9fd5TbCfcIzdS7Zo1pd5ODtMXdibvmTUTDuK+3u9mg/jWPT256wRxhEMQRxRi6kX1Dc6M6TT3vvbF1CNeZGnKc9u79K07lmvB6i1Ntqr9GplHnB5xFM07fWE8/x6GrdczoLnZ0pR8KcqW3T111gRaj8GaiEJMU951RT49oLdHPKIdhSIHa154w6O6bdF6TR4/Rvf93VmafMDYJlvXPu3rEa+zvOlHRop8m1TKpWG+ssBmz6y5NRe8t1EjjoDoEUcUYpppw9f7HVPph3ewZkTtK3Kn6rZF6yVJO7r7dP+Tm5ppVts1ViNewA4Usxeiim8AcMon9PGdRbPZI0nbcqUo+WAOtBNBHFHwTl8YySnupbhKU2I/s2ZRg8XydaCp1Zk3No94889TrxacGvHO5P38JfbZqebrEe8fcE39TVt2DQ3ezJqCkAjiiIK3Rrz9zZBUrwY7nh7n2GdNKeroxsad3UOub9qVVq9VI/OIF3FCH6Cab+cu4Rxed2Cmr3a8UfnBmfkecqCdCOKIAoM1G+efRzyeHQX/mf2G/zjrtw8N4ht2dNdZM06hT3FPh3hn6oRZU/a1vBFbGayJiBDEMSIPrdqsi371uB57ZlshjxfTYE1fj3PsgzW7ouoRLyYI5IP3+uSCeJsGaw5zOcrNP2tKultDvYGZzUxhmK8JzwdzoJ2YNQXD1t3Xr49eM0+bdvXorqUbNPezr2/69MBxzSMe9/SA0feIF1Sakg/eG3Z0jbRJQTQya0oxPeJ1asQT7gXFyPk2qe6+AXX19uuAsaPb36Am+QZrSs2dZC1fmsI84giJHnEM24oNu/bW667cuEsbdzZ/WC+ms8H5ylCi6hGPvka8mHnE1+eCd2qlKY1MsVbIKe7rLSeHd6R6/zdXbtzV5pYUo97OajMzDuUHZ/pmTbntief0nu/cp2vuXzXi5wEaQRDHsC19bseQ68ty10fCXyPe9MOOiH8e8XiCri903/fkpmjmwq07r/Uw39B8j3h6pSlha8SrF//soaf1Z/92v2574rmmnw9xq7dz9+SGnW1uSWN29/TpgusW6uM/me8NxHVLU/pG/tnZtp/SlO6+fl3ws4Wa99QWXXTjEzUDx4EiBQ3iZnakmf3AzJ41s24zW2Vml5nZQSHbhX1b9tzQf+j5YD4SMc0j7u9xjqdH3Nc7v3rzbp19xT3a1d0XoEVD1Xvfhps58z3gG3d2JzUNW9tO6FNv+sJs8eZdPbrwF4/qgZWb9fmfP5J0vTD2r/rz96LpE/f+/uT6OHvE//N3q3XDgmf034+s1T/dtGjv8k07u3XH4vXaXqdspMhZU7Z39Q75XNyzdKN2dFX+l/YPOC1YvXXEzwXsT7AgbmbHSpon6UOSHpR0qaQVkj4l6X4zmxGqbTFzzumXC5/R7YvC9WwtW5/rEV9fRE9LPOEgxR5xSXpm6x79JoIez/pBvLke8d5+l1QtZyPlQkX0iO/P/U9u2ltPu3lXjx5/tpgB1ohTdQfySw89cO/vzfaIr960W/cs21D4zvB1Dz+99/efPbxG67Z1qau3X+/+zn360NUP6VPXLvTeb6SDNfsHXM10hc5pSOC/8ZFnh9y+8Okt3sfaurunbg37/sxd/JzO+9FD+ve7nxzR/VEeIXvEvy3pUEnnO+fe5Zz7P865s1QJ5MdL+mrAtkXru3et0KeuXai//NHD+nXun0W75HvE89dHIqrSFO/0hfH0iO+rd/7e5Rvb2BK/et9Lw+2J3bC9dnBmvm48Zm2bR7xuaUrlhvueHLpN3Ls8rTOUtsNjz2zTmy+7W+f96GHtrHNUqau3X5/86QK98V/v0oMrN4/4ubp6+/XLhc9o9abddddZu22P3nrFPTruwv/RnH/4jS65dYnO+Jc7dNI//kaPrNk6ZN0HVmzSGy65U5/4yXx19fYPKQF76WGT9/7+q98/qz++9G49uqZ2R6yrt19/9R8P68Qv36JX/t9bdcmtS4bc/tSmXXrblffo/d9/UP/w30+M8C+vWLetS3/63fv17m/fq5Ubd2nNlqGvw1X3rNDti9brqX28PtLz0xdu2dWjP/u3+3XGv9xR89rMe2qzXve1ufrf3/vd3vC9o6vX+90yuJO/p6e/pkPjW3c8qbMuuXPI/9dv3LJEf/iV3+h9339gyGe9t39AH7nmYb3s72/WZbct1Z98+16981v36unNlb/HOafPX/97/cXVD+u2Rev1TzctrvmMFu3bdy7Xq75yqy7+n8WSpJ88sFqnf/0OXX7bspY+LxoTJIhnveFvkrRK0rdyN39Z0i5J7zezSW1uWtR2dffpazcv3nv9y798vO2H6rt6+7Vq09BDnEvX72h6hoZYBms65/yDNSMqTdlXT+udSzcEny2j/iwew3uMDZ66zJQGbDZSztTS6QuzG+5/cmjwbvWXfmp6+gZ0/k8XaPG6Hbpt0XO69DdLvev9+90rdOPvn9Xy9Tv1yZ/OH1EZWE/fgP739x7Qp65dqLOvuEfL1/vL+i761eN6/Nnt6ukf0KZdPbpy7nKt2rRbG3f26PPXP19etLO7T5/86QI9uWGXfv3IWl1++zI9/uz2vY/z8hdMGfK4S57bofOvXVCzk/jtO5br1iee087uPm3b06sr5y7XfVWh8/Lbl2l7Vqrxw3tX7Q2VI3HhDY/qwVWbNX/1Vv3xpXfX/L+95ndP6cq5+w+Igz3iX79lsR5YuVmrNu3WBdct3Lu8q7dfn7p2odZs2aN7l2/a+77Wm6pwsD79B/eu1O6e2s/uig27dP5PF2jbnl4tWrtd37pzuSTpdys266cPrt673k8eWK1bHn9Ou3v6ddlty7Rg9Vb9/umt+rv/71E553TjI2v1s4fXDHnsH967ar9/70gtfHqrvn7zEm3Z3avv3vWkvnfPCn3pl49p9ebduvS2pbpn2YaWPTcaYyG+tM3sPElXSfp359xHPLffokpQf6Nz7vYRPP682bNnz543b17zjW1Qd1+/LrhuYUufY8OObj20aughstOPO0QHjm/flFS7e/p155LaD+6bXnaYxowe+RSGv3niuZrpqGZMGqeTj5k+4scciYEB6ebH19Usnzljol6W+1IL5bfLNu79UvQ564RDdcDYcAe7HlmzTWu27KlZPpxtpK/f6VZPmc0fHjVNL5h2QNNtbIe5i9fvt6TpoIlj9dpjm6vC6+od0NzF62uWv3jGRM06fErN9jxuzCi9cdahTT1nmWzc0aMHVw3t4X7LiYcrPyPr7YvWD9kJHsm2+Nz2bs176vn/4S+cNkGvPGqq+gec1m3vlpzThh3denbbvo/8nPaSGZo6Yaye3dqlhU9v9a4zY9I43f35M/XyL99Sc9trZk7XwZPH7b1+26L1NeH8BVMP0B++aJqck2594rkhO40nHD5Zxxwy/H6y7t4B3e7ZVkfilGOma9qEcTXb98lHT9eMA8d5vy/fcuLh2tHVp996jhwOviZ3LN6gPfs4AvqKI6dqZ3efVmx4vkNq6oSxOu0llc/xvv4/n3n8IXr0mW01M42ZSW9+ee02V4THntmu1fvYcTp8ygGa/eJpxT9xQK84cpo++vpj2/qcc+bM0fz58+c75+YM976hgvi/SPqcpM855y7x3P5NSR+X9NfOue/s43HqJe0TZs+ePbGdQXxPT79m/f3NbXs+AEC5HXnQBO9ObSO+/p5X6E9ffZTecvk9WrR2+/7vIGnaxLHa2dXXlrELeX/5uqP1/d+ubPvz1nPC4ZM1fdI43fdka0q5Dj5wnI48aGLdHSmM3B+97DBd9YGT2vqczQTxUN1mU7PLeqOGBpdPa31T0nPg+DGaNC78iRn+4IVT97/SMJ1w+GRNmzi28McdidkvmqYxo1rQRVGQF06boMOnVHrkjq+qBY3F2NGm2S+a1vTjvKqAxwjpJYceqIOqtuljDpmkgw8c35LnevXMg1Rvk41xG4nJuDGjNG70vr8Sx4wyjR/Tnq/N6ZPG6ecfO1V/fupMSdKkcaPrPvf4MaOG/K96zczpOmfOkZK093L8mFEat4+2jxszSj/60Gt04VtneW8fO9oK+988bvTQ1/rQyeP1qTe+VP/r5BcNWW/mjIlDrp98tP8I6bjR9f+28ft4X096sX+CtrGjTV9554n6f19TaU+99/2gOq+Hmfa+VhPHjfY+/xff+jJ94exZdT+vRZt8wNDzN04ez/kcY5H0O1FvzyPrKZ/dzraMHW361v9q/VOOHW06+egZ6u7r17yntozoRClFmHnwRL3siCla8PRWrd1azAC6A8aO0mkvOVjdvQO6f8XGuoP+2mHqhLE65Zjp2rSrRw/nDm/GYOxo02kvOVj9zmnpuh165VHT9NSmXVqyLp65gl898yBNnzRO96/YpO17Rjat4sEHjtOrZ07Xqk27tGht89Nktlt+mx4zapRe99KD1dM/oPuWbyp0KsGDJo7VKcfM0LrtXTXTrb14RuXzOn/1Fj23PZ06+3YZP2aUTjl2hrbs6tGjz2yrO55h1hGTdcTUCfrdik3eOuJGTBo/Wqe95GAtWbdj74BEM+3dOdu4s1ujTJr9ooN06JQD9OW3v0xvf+ULdNT0CRo3epQeWLl570BFqRL0TjlmhtZu26NFa3dowrhROuWYGRqVJby/OG2mZr9omg6ZPF7TJo7T757c5B1n8vIXTNHMgyfplUdN0yuOnKZ1ufKYWUdM1gumTdD9KzZpd/fIx8yMHiXNefF0jTLtLR15zdHTNeWAsfrKO16ut/7BEdq6u7dSWnXEFD20arM27ezRcYcdqJcceqAWPr1Vz1Z955hVykWmTBirB1ZsHlJiM/i+bt7Zo8eeHfq+HnnQBL3iyKlatHZHzcmOTjhiso49pDLjzNEHT9LEcaNr/vbB74jFVe/joGMOmaTjD5us+au36OiDKyU8D63asvfz/uIZE3Vi1pF16wWv15J1rf3fNuPAcTr56Omav3qL1m3r1vRJlesL1xT3/R2Tw6a0pqOjVZIuTdnH47e9RhwAAACdJ8XSlMG5kY6rc/tLs0v/8HUAAAAgcaGC+B3Z5ZvMbEgbzGyypNMk7Zb0u3Y3DAAAAGiHIEHcOfekpFslzVSlBKXa/5U0SdI1zrk4z8kLAAAANCnkYM2/lnSfpCvM7A2SFkk6WdKZqpSkXBiwbQAAAEBLBTvrR9YrfpKkq1UJ4J+VdKykyyWd4pzjPMwAAAAoraDTFzrnnpb0oZBtAAAAAEIIdx5sAAAAoIMRxAEAAIAACOIAAABAAARxAAAAIACCOAAAABAAQRwAAAAIgCAOAAAABEAQBwAAAAIgiAMAAAABEMQBAACAAMw5F7oNhTOzTRMmTJg+a9as0E0BAABAiS1atEh79uzZ7JybMdz7ljWIr5Q0RdKqNj/1Cdnl4jY/L+LHtgEftgv4sF3Ah+0iXjMlbXfOHT3cO5YyiIdiZvMkyTk3J3RbEBe2DfiwXcCH7QI+bBflRI04AAAAEABBHAAAAAiAIA4AAAAEQBAHAAAAAiCIAwAAAAEwawoAAAAQAD3iAAAAQAAEcQAAACAAgjgAAAAQAEEcAAAACIAgDgAAAARAEAcAAAACIIgDAAAAARDEC2BmR5rZD8zsWTPrNrNVZnaZmR0Uum1onpmdY2ZXmtk9ZrbdzJyZ/Xg/9znVzG4ys81mtsfMHjGzT5vZ6H3c521mdqeZbTOznWb2gJl9sPi/CEUwsxlmdp6Z3WBmy7P3eZuZ/dbM/tLMvP9f2TbKz8y+Zma3m9nT2Xu82cwWmNmXzWxGnfuwXXQgM3tf9p3izOy8OusM+302sw+a2YPZ+tuy+7+tNX8FmsEJfZpkZsdKuk/SoZJ+KWmxpNdIOlPSEkmnOec2hWshmmVmCyW9UtJOSWsknSDpP51z76uz/jsl/VxSl6TrJG2W9HZJx0u63jn3Xs99PiHpSkmbsvv0SDpH0pGSLnHOfa7YvwrNMrOPSvqOpLWS7pC0WtJhkt4taaoq28B7XdU/WbaNzmBmPZLmS3pC0npJkySdIukkSc9KOsU593TV+mwXHcjMjpL0qKTRkg6U9GHn3Pdy6wz7fTazb0j6rCrfV9dLGifpXEnTJX3SOffNVv1NGAHnHD9N/Ei6RZJTZeOuXv6v2fLvhm4jP02/x2dKeqkkk3RG9r7+uM66U1T54u2WdFLV8gNU2WFzks7N3WemKl/AmyTNrFp+kKTl2X1eG/p14KfmvT5LlbA0Krf8cFVCuZP0HraNzvuRdECd5V/N3rNvs1109k/2fXKbpCcl/Uv2np3X7Pss6dRs+XJJB+Uea1P2eDNb9XfxM/wfSlOakPWGv0nSKknfyt38ZUm7JL3fzCa1uWkokHPuDufcMpf9N9uPcyQdIula59zDVY/RJemL2dWP5e7zF5LGS/qmc25V1X22SPqn7OpHR9h8tIhzbq5z7kbn3EBu+TpJ382unlF1E9tGh8jeU5+fZZcvrVrGdtGZzldlZ/5DqmQFn5G8z4PXv5qtN3ifVarklPHZcyISBPHmnJld3ur5Mt4h6V5JE1U5JInOcFZ2ebPntrsl7ZZ0qpmNb/A+/5NbB2nozS77qpaxbeDt2eUjVcvYLjqMmc2SdLGky51zd+9j1ZG8z2wbiSGIN+f47HJpnduXZZfHtaEtiEPdbcI51ydppaQxko5p8D5rVektOdLMJhbbVLSCmY2R9IHsavWXIdtGhzGzz5nZRWZ2qZndI+kfVAnhF1etxnbRQbL/D9eoUr72hf2sPqz3OTv6/kJJO7Pb88gkERoTugGJm5pdbqtz++Dyaa1vCiIxkm2ikftMytbb3Uzj0BYXSzpR0k3OuVuqlrNtdJ7PqTKAd9DNkv7cObehahnbRWf5e0mvkvQ659ye/aw73PeZTJIgesQBoCBmdr4qsxUslvT+wM1BYM65w51zpsoA3ner0qu9wMxmh20ZQjCzk1XpBb/EOXd/6PYgDgTx5gzuXU6tc/vg8q2tbwoiMZJtotH71OvlQASyacYuV2XKujOdc5tzq7BtdCjn3HPOuRtUGdw/Q9J/VN3MdtEBspKU/1ClzORLDd5tuO8zmSRBBPHmLMku69VbDY6Mr1dDjvKpu01k/4iPVmUA34oG73OEKoce1zjnOMQcKTP7tCpz/T6mSghf51mNbaPDOeeeUmVH7eVmdnC2mO2iMxyoyvs1S1JX1Ul8nCqzrEnSVdmyy7Lrw3qfnXO7JD0j6cDs9jwySYQI4s25I7t8U/4semY2WdJpqtRt/a7dDUMwc7PLN3tuO12VWXTuc851N3ift+TWQWTM7G8lXSppoSohfH2dVdk2IEkvyC77s0u2i87QLen7dX4WZOv8Nrs+WLYykveZbSM1oScyT/1HnNCno37U2Al9Nmh4J+c4WpycI8kfVQ4xO0kPS5q+n3XZNjrgR5Xey6me5aP0/Al97mW74KfqfbtI/hP6DPt9Fif0Se6HU9w3yXOK+0WSTlZljvGlkk51nOI+aWb2Lknvyq4eLumPVTlMfE+2bKOrOs1wtv71qvzDu1aV01W/Q9npqiX9qct98Mzsk5KuEKerToaZfVDS1ar0bF4pfz3uKufc1VX3eZfYNkotK1P6Z1V6N1eq8r4dJun1qgzWXCfpDc65J6ru8y6xXXQsM7tIlfIU3ynuh/0+m9klkj6joae4/zNVxidwivvYhN4TKMOPpKMk/VDSWlU+JE9JukxVe6P8pPuj53sr6v2s8tznNEk3SdoiaY+kRyVdIGn0Pp7n7ZLukrRDlflhH5L0wdB/Pz8j3i6cpDvZNjrrR5WpK7+pSqnSRlXqu7dl79lFqnPkhO2ic39Up0e8mfdZ0p9n6+3K7neXpLeF/lv5qf2hRxwAAAAIgMGaAAAAQAAEcQAAACAAgjgAAAAQAEEcAAAACIAgDgAAAARAEAcAAAACIIgDAAAAARDEAQAAgAAI4gAAAEAABHEAAAAgAII4AAAAEABBHAAAAAiAIA4AAAAEQBAHAAAAAiCIAwAAAAEQxAEAAIAACOIAAABAAP8/MxCKP+ehglcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 369
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils' from '/home/hschmutz/Documents/autoPEt/utils.py'>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_losses_val = []\n",
    "for batch_idx, batch in enumerate(tqdm(training_loader_patches)):\n",
    "    inputs, targets = prepare_batch(one_batch, device)\n",
    "    logits = unet(inputs)\n",
    "    batch_losses = get_dice_loss(probabilities, targets)\n",
    "    batch_loss = batch_losses.mean()\n",
    "    epoch_losses_val.append(batch_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
